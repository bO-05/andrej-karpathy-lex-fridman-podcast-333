1
00:00:00,000 --> 00:00:01,920
- I think it's possible
that physics has exploits

2
00:00:01,920 --> 00:00:03,600
and we should be trying to find them.

3
00:00:03,600 --> 00:00:05,970
Arranging some kind of a crazy
quantum mechanical system

4
00:00:05,970 --> 00:00:08,400
that somehow gives you buffer overflow,

5
00:00:08,400 --> 00:00:10,770
somehow gives you a rounding
error in the floating point.

6
00:00:10,770 --> 00:00:13,290
Synthetic intelligence is kind of like

7
00:00:13,290 --> 00:00:15,360
the next stage of development.

8
00:00:15,360 --> 00:00:18,750
And I dunno where it
leads to, at some point

9
00:00:18,750 --> 00:00:23,160
I suspect the universe
is some kind of a puzzle.

10
00:00:23,160 --> 00:00:27,663
These synthetic AIs will uncover
that puzzle and solve it.

11
00:00:30,150 --> 00:00:32,940
- The following is a conversation
with Andrej Karpathy,

12
00:00:32,940 --> 00:00:35,880
previously the director of AI at Tesla.

13
00:00:35,880 --> 00:00:39,870
And before that, at OpenAi and Stanford.

14
00:00:39,870 --> 00:00:43,650
He is one of the greatest
scientists, engineers,

15
00:00:43,650 --> 00:00:47,850
and educators in the history
of artificial intelligence.

16
00:00:47,850 --> 00:00:50,190
This is the Lex Fridman Podcast.

17
00:00:50,190 --> 00:00:52,830
To support it, please
check out our sponsors.

18
00:00:52,830 --> 00:00:57,123
And now, dear friends,
here's Andrej Karpathy.

19
00:00:58,140 --> 00:01:02,130
What is a neural network
and why does it seem to do

20
00:01:02,130 --> 00:01:04,620
such a surprisingly good job of learning?

21
00:01:04,620 --> 00:01:05,459
- What is a neural network?

22
00:01:05,459 --> 00:01:10,080
It's a mathematical
abstraction of the brain.

23
00:01:10,080 --> 00:01:12,720
I would say that's how it
was originally developed.

24
00:01:12,720 --> 00:01:14,580
At the end of the day, it's
a mathematical expression

25
00:01:14,580 --> 00:01:16,170
and it's a fairly simple
mathematical expression

26
00:01:16,170 --> 00:01:17,430
when you get down to it.

27
00:01:17,430 --> 00:01:21,420
It's basically a sequence
of meter multipliers,

28
00:01:21,420 --> 00:01:23,520
whichever really dot
products mathematically

29
00:01:23,520 --> 00:01:25,652
and some nonlinearity is thrown in.

30
00:01:25,652 --> 00:01:27,690
And so it's a very simple
mathematical expression

31
00:01:27,690 --> 00:01:29,250
and it's got knobs in it.

32
00:01:29,250 --> 00:01:30,090
- Many knobs.

33
00:01:30,090 --> 00:01:30,923
- Many knobs.

34
00:01:30,923 --> 00:01:33,180
And these knobs are loosely
related to basically

35
00:01:33,180 --> 00:01:34,200
the synapses in your brain.

36
00:01:34,200 --> 00:01:35,033
They're trainable.

37
00:01:35,033 --> 00:01:35,880
They're modifiable.

38
00:01:35,880 --> 00:01:37,770
And so the idea is we
need to find the setting

39
00:01:37,770 --> 00:01:40,380
of the knobs that makes the neural net

40
00:01:40,380 --> 00:01:41,490
do whatever you want it to do,

41
00:01:41,490 --> 00:01:43,590
like classify images and so on.

42
00:01:43,590 --> 00:01:46,290
And so there's not too much
mystery I would say in it.

43
00:01:47,190 --> 00:01:49,680
You might think that, basically,
you don't want to endow it

44
00:01:49,680 --> 00:01:51,420
with too much meaning
with respect to the brain

45
00:01:51,420 --> 00:01:53,076
and how it works.

46
00:01:53,076 --> 00:01:55,020
It's really just a complicated
mathematical expression

47
00:01:55,020 --> 00:01:55,853
with knobs.

48
00:01:55,853 --> 00:01:57,600
And those knobs need a proper setting

49
00:01:57,600 --> 00:01:59,310
for it to do something desirable.

50
00:01:59,310 --> 00:02:00,143
- Yeah.

51
00:02:00,143 --> 00:02:03,420
But poetry is just the collection
of letters with spaces,

52
00:02:03,420 --> 00:02:05,037
but it can make us feel a certain way.

53
00:02:05,037 --> 00:02:06,180
And in that same way,

54
00:02:06,180 --> 00:02:08,610
when you get a large
number of knobs together,

55
00:02:08,610 --> 00:02:12,024
whether it's inside the
brain or inside a computer,

56
00:02:12,024 --> 00:02:16,110
they seem to surprise us with their power.

57
00:02:16,110 --> 00:02:16,943
- Yeah.

58
00:02:16,943 --> 00:02:17,776
I think that's fair.

59
00:02:17,776 --> 00:02:20,040
So basically, I think I'm
underselling it by a lot

60
00:02:20,040 --> 00:02:22,320
because you definitely do get

61
00:02:22,320 --> 00:02:25,230
very surprising emergent
behaviors out of these neural nets

62
00:02:25,230 --> 00:02:26,070
when they're large enough

63
00:02:26,070 --> 00:02:28,770
and trained on complicated
enough problems.

64
00:02:28,770 --> 00:02:31,350
Like say, for example,
the next-word prediction

65
00:02:31,350 --> 00:02:33,600
in a massive dataset from the internet.

66
00:02:33,600 --> 00:02:35,580
And then these neural nets take on

67
00:02:35,580 --> 00:02:37,830
pretty surprising magical properties.

68
00:02:37,830 --> 00:02:39,990
Yeah, I think it's kind of
interesting how much you can get

69
00:02:39,990 --> 00:02:42,330
out of even very simple
mathematical formalism.

70
00:02:42,330 --> 00:02:44,400
- When your brain right now is talking,

71
00:02:44,400 --> 00:02:46,360
is it doing next-word prediction

72
00:02:47,220 --> 00:02:49,170
or is it doing something more interesting?

73
00:02:49,170 --> 00:02:51,000
- Well, it's definitely some
kind of a generative model

74
00:02:51,000 --> 00:02:54,420
that's GPT like and prompted by you.

75
00:02:54,420 --> 00:02:55,253
- [Lex] Yes.

76
00:02:55,253 --> 00:02:56,086
- So you're giving me a prompt

77
00:02:56,086 --> 00:02:58,710
and I'm kind of like responding
to it in a generative way.

78
00:02:58,710 --> 00:03:00,900
- And by yourself perhaps a little bit,

79
00:03:00,900 --> 00:03:04,740
like are you adding extra
prompts from your own memory

80
00:03:04,740 --> 00:03:06,400
inside your head or no?

81
00:03:06,400 --> 00:03:08,370
- Well, it definitely feels
like you're referencing

82
00:03:08,370 --> 00:03:10,080
some kind of a declarative structure

83
00:03:10,080 --> 00:03:12,330
of memory and so on.

84
00:03:12,330 --> 00:03:15,540
And then you're putting that
together with your prompt

85
00:03:15,540 --> 00:03:17,160
and giving away some answers.

86
00:03:17,160 --> 00:03:19,530
- How much of what you just said

87
00:03:19,530 --> 00:03:21,273
has been said by you before.

88
00:03:22,140 --> 00:03:23,640
- Nothing, basically right?

89
00:03:23,640 --> 00:03:26,040
- No, but if you actually
look at all the words

90
00:03:26,040 --> 00:03:29,520
you've ever said in your
life and you do a search,

91
00:03:29,520 --> 00:03:32,250
you'll probably have said
a lot of the same words

92
00:03:32,250 --> 00:03:34,170
in the same order before.

93
00:03:34,170 --> 00:03:35,003
- Yeah.

94
00:03:35,003 --> 00:03:35,836
Could be.

95
00:03:35,836 --> 00:03:37,530
I mean, I'm using phrases
that are common, et cetera,

96
00:03:37,530 --> 00:03:41,280
but I'm remixing it into
a pretty unique sentence

97
00:03:41,280 --> 00:03:42,113
at the end of the day.

98
00:03:42,113 --> 00:03:42,946
But you're right, definitely,

99
00:03:42,946 --> 00:03:44,480
there's like a ton of remixing.

100
00:03:46,410 --> 00:03:51,270
- It's like Magnus Carlson
said I'm rated 2,900, whatever,

101
00:03:51,270 --> 00:03:52,410
which is pretty decent.

102
00:03:52,410 --> 00:03:56,490
I think you're talking very,
you're not giving enough credit

103
00:03:56,490 --> 00:03:57,603
to neural nets here.

104
00:04:00,870 --> 00:04:05,640
What's your best intuition
about this emergent behavior?

105
00:04:05,640 --> 00:04:06,473
- I mean, it's kind of interesting

106
00:04:06,473 --> 00:04:08,910
because I'm simultaneously
underselling them,

107
00:04:08,910 --> 00:04:11,820
but I also feel like there's
an element to which I'm over-

108
00:04:11,820 --> 00:04:12,870
it's actually kind of incredible

109
00:04:12,870 --> 00:04:14,850
that you can get so much
emergent magical behavior

110
00:04:14,850 --> 00:04:17,610
out of them despite them being
so simple mathematically.

111
00:04:17,610 --> 00:04:19,709
So I think those are two
surprising statements

112
00:04:19,709 --> 00:04:22,740
that are juxtaposed together.

113
00:04:22,740 --> 00:04:23,790
And I think, basically, what it is,

114
00:04:23,790 --> 00:04:25,290
is we are actually fairly good

115
00:04:25,290 --> 00:04:27,180
at optimizing these neural nets.

116
00:04:27,180 --> 00:04:29,700
And when you give them
a hard enough problem,

117
00:04:29,700 --> 00:04:32,790
they are forced to learn
very interesting solutions

118
00:04:32,790 --> 00:04:34,110
in the optimization.

119
00:04:34,110 --> 00:04:37,500
And those solutions basically
have these emergent properties

120
00:04:37,500 --> 00:04:38,880
that are very interesting.

121
00:04:38,880 --> 00:04:42,753
- There's wisdom and
knowledge in the knobs.

122
00:04:44,220 --> 00:04:45,053
- [Andrej] Yes.

123
00:04:45,053 --> 00:04:47,700
- And so this representation
that's in the knobs

124
00:04:47,700 --> 00:04:49,017
does it make sense to you intuitively,

125
00:04:49,017 --> 00:04:52,740
that a large number of knobs
can hold a representation

126
00:04:52,740 --> 00:04:54,930
that captures some deep wisdom

127
00:04:54,930 --> 00:04:57,183
about the data it has looked at.

128
00:04:57,183 --> 00:04:58,770
It's a lot of knobs.

129
00:04:58,770 --> 00:05:00,150
- It's a lot of knobs.

130
00:05:00,150 --> 00:05:04,410
And somehow, so speaking
concretely, one of the neural nets

131
00:05:04,410 --> 00:05:07,710
that people are very excited
about right now are GPTs,

132
00:05:07,710 --> 00:05:10,290
which are basically just
next-word prediction networks.

133
00:05:10,290 --> 00:05:13,740
So you consume a sequence
of words from the internet

134
00:05:13,740 --> 00:05:15,540
and you try to predict the next word.

135
00:05:15,540 --> 00:05:19,773
And once you train these
on a large enough data set,

136
00:05:21,840 --> 00:05:24,090
you can basically prompt these neural nets

137
00:05:24,090 --> 00:05:25,980
in arbitrary ways and you can
ask them to solve problems.

138
00:05:25,980 --> 00:05:26,900
And they will.

139
00:05:26,900 --> 00:05:29,760
So you can just tell them,
you can make it look like

140
00:05:29,760 --> 00:05:33,510
you're trying to solve some
kind of a mathematical problem.

141
00:05:33,510 --> 00:05:35,460
And they will continue what
they think is the solution

142
00:05:35,460 --> 00:05:36,990
based on what they've
seen on the internet.

143
00:05:36,990 --> 00:05:38,610
And very often those solutions

144
00:05:38,610 --> 00:05:40,800
look very remarkably consistent.

145
00:05:40,800 --> 00:05:43,050
Look correct, potentially even.

146
00:05:43,050 --> 00:05:45,450
- Do you still think about
the brain side of it?

147
00:05:45,450 --> 00:05:47,550
So as neural nets as an abstraction,

148
00:05:47,550 --> 00:05:49,157
a mathematical abstraction of the brain,

149
00:05:49,157 --> 00:05:52,410
do you still draw wisdom

150
00:05:52,410 --> 00:05:56,370
from the biological neural networks

151
00:05:56,370 --> 00:05:57,780
or even the bigger question.

152
00:05:57,780 --> 00:06:00,993
So you're a big fan of biology
and biological computation.

153
00:06:02,220 --> 00:06:05,850
What impressive thing
is biology doing to you

154
00:06:05,850 --> 00:06:09,180
that computers are not yet, that gap?

155
00:06:09,180 --> 00:06:10,950
- I would say I'm definitely on,

156
00:06:10,950 --> 00:06:13,440
I'm much more hesitant with
the analogies to the brain

157
00:06:13,440 --> 00:06:16,290
than I think you would see
potentially in the field.

158
00:06:16,290 --> 00:06:20,670
And I feel like certainly, the
way neural networks started

159
00:06:20,670 --> 00:06:24,030
is everything stemmed from
inspiration by the brain.

160
00:06:24,030 --> 00:06:26,220
But at the end of the day,
the artifacts that you get

161
00:06:26,220 --> 00:06:28,380
after training, they are arrived at

162
00:06:28,380 --> 00:06:30,030
by a very different optimization process

163
00:06:30,030 --> 00:06:32,790
than the optimization process
that gave rise to the brain.

164
00:06:32,790 --> 00:06:37,790
And so I think of it as a very
complicated alien artifact.

165
00:06:38,730 --> 00:06:39,741
It's something different.

166
00:06:39,741 --> 00:06:40,574
- [Lex] The brain?

167
00:06:40,574 --> 00:06:41,407
- Oh no, sorry.

168
00:06:41,407 --> 00:06:42,240
The neural nest that we're training.

169
00:06:42,240 --> 00:06:43,073
- [Lex] Okay.

170
00:06:43,073 --> 00:06:45,750
- They are a complicated alien artifact.

171
00:06:45,750 --> 00:06:47,400
I do not make analogies to the brain

172
00:06:47,400 --> 00:06:49,080
because I think the optimization process

173
00:06:49,080 --> 00:06:51,810
that gave rise to it is very
different from the brain.

174
00:06:51,810 --> 00:06:56,810
So there was no multi-agent,
self-play setup and evolution.

175
00:06:57,120 --> 00:06:59,580
It was an optimization that is basically

176
00:06:59,580 --> 00:07:01,920
what amounts to a compression objective

177
00:07:01,920 --> 00:07:03,480
on a mass amount of data.

178
00:07:03,480 --> 00:07:04,313
- Okay.

179
00:07:04,313 --> 00:07:07,290
So artificial neural networks
are doing compression

180
00:07:07,290 --> 00:07:09,723
and biological neural networks-

181
00:07:10,590 --> 00:07:11,423
- [Andrej] Are trying to survive.

182
00:07:11,423 --> 00:07:14,280
- Are not really doing
anything, they're an agent

183
00:07:14,280 --> 00:07:16,920
in a multi-agent, self-play system

184
00:07:16,920 --> 00:07:18,930
that's been running for
a very, very long time.

185
00:07:18,930 --> 00:07:19,763
- Yes.

186
00:07:19,763 --> 00:07:23,367
That said, evolution has
found that it is very useful

187
00:07:23,367 --> 00:07:26,400
to predict and have a
predictive model in the brain.

188
00:07:26,400 --> 00:07:28,800
And so, I think our
brain utilizes something

189
00:07:28,800 --> 00:07:30,990
that looks like that as a part of it,

190
00:07:30,990 --> 00:07:33,780
but it has a lot more catches and gizmos

191
00:07:33,780 --> 00:07:37,350
and value functions and ancient nuclei

192
00:07:37,350 --> 00:07:39,030
that are all trying to
like make it survive

193
00:07:39,030 --> 00:07:40,800
and reproduce and everything else.

194
00:07:40,800 --> 00:07:43,500
- And the whole thing through
embryogenesis is built

195
00:07:43,500 --> 00:07:44,670
from a single-cell.

196
00:07:44,670 --> 00:07:48,180
I mean, it's just the
code is inside the DNA

197
00:07:48,180 --> 00:07:51,270
and it just builds it up
like the entire organism

198
00:07:51,270 --> 00:07:52,388
with arms-

199
00:07:52,388 --> 00:07:53,221
- [Andrej] It's definitely crazy.

200
00:07:53,221 --> 00:07:54,660
- And the head and legs.

201
00:07:54,660 --> 00:07:55,493
- [Andrej] Yes.

202
00:07:55,493 --> 00:07:57,887
- And it does it pretty well.

203
00:07:57,887 --> 00:07:59,190
- [Andrej] It should not be possible.

204
00:07:59,190 --> 00:08:00,630
- So there's some learning going on.

205
00:08:00,630 --> 00:08:03,120
There's some kind of computation

206
00:08:03,120 --> 00:08:04,943
going through that building process.

207
00:08:04,943 --> 00:08:09,240
I mean, I don't know where,
if you were just to look

208
00:08:09,240 --> 00:08:11,820
at the entirety of
history of life on earth,

209
00:08:11,820 --> 00:08:15,330
where do you think is the
most interesting invention?

210
00:08:15,330 --> 00:08:17,790
Is it the origin of life itself?

211
00:08:17,790 --> 00:08:20,460
Is it just jumping to Eukaryotes?

212
00:08:20,460 --> 00:08:22,200
Is it mammals?

213
00:08:22,200 --> 00:08:25,140
Is it humans themselves, Homo sapiens?

214
00:08:25,140 --> 00:08:29,523
The origin of intelligence or
highly complex intelligence?

215
00:08:31,770 --> 00:08:33,270
Or is it all just a continuation

216
00:08:33,270 --> 00:08:34,670
of the same kind of process?

217
00:08:36,030 --> 00:08:38,490
- Certainly, I would say it's
an extremely remarkable story

218
00:08:38,490 --> 00:08:41,460
that I'm only briefly
learning about recently

219
00:08:41,460 --> 00:08:44,940
all the way from, actually,
you almost have to start

220
00:08:44,940 --> 00:08:47,310
at the formation of earth
and all of its conditions

221
00:08:47,310 --> 00:08:49,500
and the entire solar system
and how everything is arranged

222
00:08:49,500 --> 00:08:53,430
with Jupiter and moon and the
habitable zone and everything.

223
00:08:53,430 --> 00:08:55,590
And then you have an active earth

224
00:08:55,590 --> 00:08:57,630
that's turning over material

225
00:08:57,630 --> 00:09:01,830
and then you start with a
biogenesis and everything.

226
00:09:01,830 --> 00:09:03,960
And so it's all a pretty remarkable story.

227
00:09:03,960 --> 00:09:08,960
I'm not sure that I can pick
a single unique piece of it

228
00:09:09,330 --> 00:09:10,780
that I find most interesting.

229
00:09:12,330 --> 00:09:14,340
I guess for me, as an artificial
intelligence researcher,

230
00:09:14,340 --> 00:09:15,330
it's probably the last piece.

231
00:09:15,330 --> 00:09:16,320
We have lots of animals

232
00:09:16,320 --> 00:09:21,320
that are not building
technological society but we do.

233
00:09:22,170 --> 00:09:24,810
And it seems to have
happened very quickly.

234
00:09:24,810 --> 00:09:26,790
It seems to have happened very recently.

235
00:09:26,790 --> 00:09:30,150
And something very
interesting happened there

236
00:09:30,150 --> 00:09:31,170
that I don't fully understand.

237
00:09:31,170 --> 00:09:34,560
I almost understand everything
else I think intuitively,

238
00:09:34,560 --> 00:09:36,300
but I don't understand exactly that part

239
00:09:36,300 --> 00:09:37,980
and how quick it was.

240
00:09:37,980 --> 00:09:40,020
- Both explanations would be interesting.

241
00:09:40,020 --> 00:09:42,210
One is that this is just a continuation

242
00:09:42,210 --> 00:09:43,260
of the same kind of process.

243
00:09:43,260 --> 00:09:45,110
There's nothing special about humans.

244
00:09:46,380 --> 00:09:48,930
Deeply understanding that
would be very interesting

245
00:09:48,930 --> 00:09:50,700
that we think of ourselves as special.

246
00:09:50,700 --> 00:09:55,700
But it was obvious, it was
already written in the code

247
00:09:56,460 --> 00:09:59,160
that you would have greater

248
00:09:59,160 --> 00:10:00,900
and greater intelligence emerging.

249
00:10:00,900 --> 00:10:03,660
And then the other explanation,

250
00:10:03,660 --> 00:10:05,670
which is something truly special happened,

251
00:10:05,670 --> 00:10:08,070
something like a rare event,

252
00:10:08,070 --> 00:10:11,550
whether it's like crazy rare
event like a "Space Odyssey",

253
00:10:11,550 --> 00:10:12,660
what would it be?

254
00:10:12,660 --> 00:10:14,920
See if you say like the invention of fire

255
00:10:16,110 --> 00:10:21,110
or as Richard Rankin says,
the beta males deciding

256
00:10:23,190 --> 00:10:26,580
a clever way to kill the
alpha males by collaborating.

257
00:10:26,580 --> 00:10:30,120
So just optimizing the
collaboration, the multi-agent,

258
00:10:30,120 --> 00:10:34,110
aspect of the multi-agent and
that really being constrained

259
00:10:34,110 --> 00:10:38,310
on resources and trying to
survive the collaboration aspect

260
00:10:38,310 --> 00:10:40,590
is what created the complex intelligence.

261
00:10:40,590 --> 00:10:42,740
But it seems like it's a natural algorithm

262
00:10:42,740 --> 00:10:44,260
to the evolution process.

263
00:10:44,260 --> 00:10:45,093
- [Andrej] Yeah.

264
00:10:45,093 --> 00:10:47,610
- What could possibly be a
magical thing that happened,

265
00:10:47,610 --> 00:10:51,630
like a rare thing that would
say that humans are actually,

266
00:10:51,630 --> 00:10:54,510
human-level intelligence is
actually a really rare thing

267
00:10:54,510 --> 00:10:55,683
in the universe?

268
00:10:56,850 --> 00:10:58,890
- Yeah, I'm hesitant to say
that it is rare by the way,

269
00:10:58,890 --> 00:11:01,050
but it definitely seems like

270
00:11:01,050 --> 00:11:04,080
it's like a punctuated
equilibrium where you have

271
00:11:04,080 --> 00:11:06,600
lots of exploration and
then you have certain leaps,

272
00:11:06,600 --> 00:11:08,070
sparse leaps in between.

273
00:11:08,070 --> 00:11:10,830
So of course, like origin
of life would be one,

274
00:11:10,830 --> 00:11:15,830
DNA, sex, Eukaryotic life,
the endosymbiosis event

275
00:11:16,807 --> 00:11:20,670
where the archon ate little
bacteria, just the whole thing.

276
00:11:20,670 --> 00:11:23,520
And then of course, emergence
of consciousness and so on.

277
00:11:23,520 --> 00:11:25,560
So it seems like definitely
there are sparse events

278
00:11:25,560 --> 00:11:26,827
where massive amount of progress was made.

279
00:11:26,827 --> 00:11:29,480
But yeah, it's kind of hard to pick one.

280
00:11:29,480 --> 00:11:32,550
- So you don't think humans are unique?

281
00:11:32,550 --> 00:11:35,190
To that I ask you how many
intelligent alien civilizations

282
00:11:35,190 --> 00:11:36,810
do you think are out there

283
00:11:36,810 --> 00:11:41,810
and is their intelligence
different or similar to ours?

284
00:11:44,610 --> 00:11:47,520
- Yeah, I've been preoccupied
with this question

285
00:11:47,520 --> 00:11:48,780
quite a bit recently.

286
00:11:48,780 --> 00:11:51,900
Basically, the Fermi paradox
and just thinking through.

287
00:11:51,900 --> 00:11:54,240
And the reason actually
that I am very interested

288
00:11:54,240 --> 00:11:57,390
in the origin of life is
fundamentally trying to understand

289
00:11:57,390 --> 00:11:59,430
how common it is that there
are technological societies

290
00:11:59,430 --> 00:12:02,850
out there in space.

291
00:12:02,850 --> 00:12:05,950
And the more I study it,
the more I think that

292
00:12:07,860 --> 00:12:09,420
there should be quite a few, quite a lot.

293
00:12:09,420 --> 00:12:10,770
- Why haven't we heard from them?

294
00:12:10,770 --> 00:12:11,970
'Cause I agree with you.

295
00:12:11,970 --> 00:12:16,970
It feels like, I just don't see why

296
00:12:17,460 --> 00:12:20,160
what we did here on earth
is so difficult to do.

297
00:12:20,160 --> 00:12:21,152
- Yeah.

298
00:12:21,152 --> 00:12:22,020
And especially when you
get into the details of it.

299
00:12:22,020 --> 00:12:24,020
I used to think origin of life was very,

300
00:12:25,680 --> 00:12:27,240
it was this magical rare event.

301
00:12:27,240 --> 00:12:29,790
But then you read books
like for example Nick Lane,

302
00:12:30,937 --> 00:12:34,290
"The Vital Question", "Life
Ascending", et cetera.

303
00:12:34,290 --> 00:12:37,075
And he really gets in and
he really makes you believe

304
00:12:37,075 --> 00:12:38,640
that this is not that rare.

305
00:12:38,640 --> 00:12:39,930
- Basic chemistry.

306
00:12:39,930 --> 00:12:42,000
- You have an active earth and
you have your alkaline vents

307
00:12:42,000 --> 00:12:44,430
and you have lots of
alkaline waters mixing

308
00:12:44,430 --> 00:12:47,070
where there is a devotion and
you have your proton gradients

309
00:12:47,070 --> 00:12:48,810
and you have these little porous pockets

310
00:12:48,810 --> 00:12:51,690
of these alkaline vents
that concentrate chemistry.

311
00:12:51,690 --> 00:12:54,000
And basically, as he steps through

312
00:12:54,000 --> 00:12:55,800
all of these little pieces,
you start to understand

313
00:12:55,800 --> 00:12:58,890
that actually this is not that crazy.

314
00:12:58,890 --> 00:13:01,560
You could see this happen on other systems

315
00:13:01,560 --> 00:13:04,890
and he really takes
you from just a geology

316
00:13:04,890 --> 00:13:06,420
to primitive life.

317
00:13:06,420 --> 00:13:09,300
And he makes it feel like this
is actually pretty plausible.

318
00:13:09,300 --> 00:13:14,300
And also like the origin of
life was actually fairly fast

319
00:13:14,730 --> 00:13:17,730
after formation of earth,
if I remember correctly,

320
00:13:17,730 --> 00:13:19,380
just a few hundred million
years or something like that

321
00:13:19,380 --> 00:13:22,470
after basically when it was
possible, life actually arose.

322
00:13:22,470 --> 00:13:25,050
And so that makes me feel like
that is not the constraint,

323
00:13:25,050 --> 00:13:26,310
that is not the limiting variable

324
00:13:26,310 --> 00:13:28,660
and that life should
actually be fairly common.

325
00:13:29,760 --> 00:13:31,890
And then where the drop-offs are

326
00:13:31,890 --> 00:13:35,520
is very interesting to think about.

327
00:13:35,520 --> 00:13:38,505
I currently think that there's
no major drop-offs basically.

328
00:13:38,505 --> 00:13:39,338
- [Lex] Yeah.

329
00:13:39,338 --> 00:13:40,171
- And so there should
be quite a lot of life.

330
00:13:40,171 --> 00:13:42,720
And basically, where
that brings me to then

331
00:13:42,720 --> 00:13:44,490
is the only way to reconcile the fact

332
00:13:44,490 --> 00:13:46,080
that we haven't found anyone and so on

333
00:13:46,080 --> 00:13:50,610
is that we can't see them,
we can't observe them.

334
00:13:50,610 --> 00:13:51,990
- Just a quick brief comment,

335
00:13:51,990 --> 00:13:54,510
Nick Lane and a lot of
biologists I talk to,

336
00:13:54,510 --> 00:13:58,320
they really seem to think
that the jump from bacteria

337
00:13:58,320 --> 00:14:01,080
to more complex organisms
is the hardest jump.

338
00:14:01,080 --> 00:14:02,460
- [Andrej] The Eukaryotic life, basically.

339
00:14:02,460 --> 00:14:03,293
- Yeah.

340
00:14:04,350 --> 00:14:05,183
I get it.

341
00:14:05,183 --> 00:14:08,430
They're much more knowledgeable than me

342
00:14:08,430 --> 00:14:11,070
about the intricacies of biology.

343
00:14:11,070 --> 00:14:12,780
But that seems crazy

344
00:14:12,780 --> 00:14:17,010
'cause how many single-cell
organisms are there?

345
00:14:17,010 --> 00:14:21,810
And how much time you have
surely it's not that difficult.

346
00:14:21,810 --> 00:14:24,690
And a billion years is not even that long

347
00:14:24,690 --> 00:14:27,630
of a time really, just all these bacteria

348
00:14:27,630 --> 00:14:30,240
under constrained
resources battling it out.

349
00:14:30,240 --> 00:14:32,070
I'm sure they can invent more complex.

350
00:14:32,070 --> 00:14:33,090
I don't understand.

351
00:14:33,090 --> 00:14:36,030
It's like how to move from
a "Hello, World!" program

352
00:14:36,030 --> 00:14:39,090
to invent a function
or something like that.

353
00:14:39,090 --> 00:14:39,923
I don't-

354
00:14:39,923 --> 00:14:40,756
- Yeah.

355
00:14:42,240 --> 00:14:43,140
- Yeah, so I'm with you.

356
00:14:43,140 --> 00:14:46,860
I just feel like I don't see
any, if the origin of life,

357
00:14:46,860 --> 00:14:49,020
that would be my intuition,
that's the hardest thing.

358
00:14:49,020 --> 00:14:50,190
But if that's not the hardest thing

359
00:14:50,190 --> 00:14:51,810
'cause it happened so quickly,

360
00:14:51,810 --> 00:14:53,550
then it's gotta be everywhere and yeah,

361
00:14:53,550 --> 00:14:55,350
maybe we're just too dumb to see it.

362
00:14:55,350 --> 00:14:57,150
- Well, it's just we don't
have really good mechanisms

363
00:14:57,150 --> 00:14:58,233
for seeing this life.

364
00:15:01,140 --> 00:15:02,760
So I'm not an expert just to preface this

365
00:15:02,760 --> 00:15:03,593
but just from what-

366
00:15:03,593 --> 00:15:04,426
- On aliens?

367
00:15:05,550 --> 00:15:07,883
I wanna meet an expert
on alien intelligence

368
00:15:07,883 --> 00:15:09,090
and how to communicate.

369
00:15:09,090 --> 00:15:10,500
- I'm very suspicious of our ability

370
00:15:10,500 --> 00:15:12,450
to find these intelligences out there

371
00:15:12,450 --> 00:15:14,580
and to find these earths like radio waves,

372
00:15:14,580 --> 00:15:16,410
for example, are terrible.

373
00:15:16,410 --> 00:15:19,200
Their power drops off as
basically 1 over R-squared.

374
00:15:19,200 --> 00:15:22,080
So I remember reading that
our current radio waves

375
00:15:22,080 --> 00:15:25,380
would not be, the ones
that we are broadcasting,

376
00:15:25,380 --> 00:15:29,280
would not be measurable by
our devices today only like,

377
00:15:29,280 --> 00:15:31,110
was it like one 10th of a light year away?

378
00:15:31,110 --> 00:15:33,060
Not even, basically, tiny distance

379
00:15:33,060 --> 00:15:36,330
because you really need
like a targeted transmission

380
00:15:36,330 --> 00:15:38,610
of massive power directed somewhere

381
00:15:38,610 --> 00:15:41,400
for this to be picked
up on long distances.

382
00:15:41,400 --> 00:15:42,660
And so I just think that our ability

383
00:15:42,660 --> 00:15:45,030
to measure is not amazing.

384
00:15:45,030 --> 00:15:47,040
I think there's probably
other civilizations out there.

385
00:15:47,040 --> 00:15:48,030
And then the big question is

386
00:15:48,030 --> 00:15:49,380
why don't they build one man probes

387
00:15:49,380 --> 00:15:50,850
and why don't they interstellar travel

388
00:15:50,850 --> 00:15:52,470
across the entire galaxy?

389
00:15:52,470 --> 00:15:53,610
And my current answer is,

390
00:15:53,610 --> 00:15:56,580
it's probably interstellar
travel is really hard.

391
00:15:56,580 --> 00:15:58,290
You have the interstellar
medium if you wanna move

392
00:15:58,290 --> 00:15:59,460
at close to speed of light,

393
00:15:59,460 --> 00:16:01,890
you're going to be encountering
bullets along the way

394
00:16:01,890 --> 00:16:04,440
because even like tiny hydrogen atoms

395
00:16:04,440 --> 00:16:08,280
and little particles of dust
have massive kinetic energy

396
00:16:08,280 --> 00:16:09,480
at those speeds.

397
00:16:09,480 --> 00:16:11,490
And so, basically, you need
some kind of shielding,

398
00:16:11,490 --> 00:16:13,920
you have all the cosmic radiation,

399
00:16:13,920 --> 00:16:15,120
it's just brutal out there.

400
00:16:15,120 --> 00:16:16,020
It's really hard.

401
00:16:16,020 --> 00:16:18,120
And so my thinking is
maybe interstellar travel

402
00:16:18,120 --> 00:16:19,890
is just extremely hard.

403
00:16:19,890 --> 00:16:20,723
You have to learn slow.

404
00:16:20,723 --> 00:16:22,503
- Like billions of years to build hard?

405
00:16:24,297 --> 00:16:27,840
It feels like we're not
a billion years away

406
00:16:27,840 --> 00:16:28,673
from doing that.

407
00:16:28,673 --> 00:16:30,240
- It just might be that it's very,

408
00:16:30,240 --> 00:16:32,070
you have to go very slowly potentially,

409
00:16:32,070 --> 00:16:34,290
as an example, through space.

410
00:16:34,290 --> 00:16:35,123
- Right.

411
00:16:35,123 --> 00:16:36,627
As opposed to close to the speed of light.

412
00:16:36,627 --> 00:16:37,460
- Yeah.

413
00:16:37,460 --> 00:16:38,850
So I'm suspicious basically
of our ability to measure life

414
00:16:38,850 --> 00:16:42,210
and I'm suspicious of the
ability to just permeate

415
00:16:42,210 --> 00:16:44,460
all of space in the
galaxy or across galaxies.

416
00:16:44,460 --> 00:16:45,420
And that's the only way

417
00:16:45,420 --> 00:16:47,830
that I can currently see away around it.

418
00:16:47,830 --> 00:16:51,270
- Yeah, it's kind of mind-blowing
to think that there's

419
00:16:51,270 --> 00:16:55,200
trillions of intelligent
alien civilizations out there

420
00:16:55,200 --> 00:16:57,243
slowly traveling through space.

421
00:16:58,110 --> 00:16:59,100
- [Andrej] Maybe.

422
00:16:59,100 --> 00:16:59,933
- To meet each other.

423
00:16:59,933 --> 00:17:01,380
And some of them meet,
some of them go to war,

424
00:17:01,380 --> 00:17:03,123
some of them collaborate.

425
00:17:04,200 --> 00:17:06,300
- Or they're all just independent.

426
00:17:06,300 --> 00:17:08,130
They're all just like little pockets.

427
00:17:08,130 --> 00:17:08,970
I don't know.

428
00:17:08,970 --> 00:17:13,410
- Well, statistically if
there's trillions of them,

429
00:17:13,410 --> 00:17:14,550
surely some of them,

430
00:17:14,550 --> 00:17:16,319
some of the pockets are
close enough together.

431
00:17:16,319 --> 00:17:17,219
- Some of them happen to be close.

432
00:17:17,220 --> 00:17:18,053
Yeah.

433
00:17:18,053 --> 00:17:20,460
- And close enough to see each other.

434
00:17:20,460 --> 00:17:21,839
See, once you see something

435
00:17:21,839 --> 00:17:26,839
that is definitely complex
life, if we see something.

436
00:17:27,359 --> 00:17:28,192
- [Andrej] Yeah.

437
00:17:28,193 --> 00:17:30,120
- We're probably going to be

438
00:17:30,120 --> 00:17:32,220
intensely aggressively motivated

439
00:17:32,220 --> 00:17:35,100
to figure out what the hell
that is and try to meet them.

440
00:17:35,100 --> 00:17:38,670
But what would be your
first instinct, to try to,

441
00:17:38,670 --> 00:17:43,670
at a generational-level, meet
them or defend against them?

442
00:17:44,460 --> 00:17:48,630
Or what would be your
instinct as a president

443
00:17:48,630 --> 00:17:51,873
of the United States and a scientist?

444
00:17:52,770 --> 00:17:55,560
I don't know which hat you
prefer in this question.

445
00:17:55,560 --> 00:17:58,233
- Yeah, I think the
question, it's really hard.

446
00:17:59,910 --> 00:18:01,953
I will say like for example, for us,

447
00:18:02,820 --> 00:18:06,000
we have lots of primitive life
forms on earth next to us.

448
00:18:06,000 --> 00:18:07,920
We have all kinds of
ants and everything else

449
00:18:07,920 --> 00:18:10,440
and we share a space with
them and we are hesitant

450
00:18:10,440 --> 00:18:13,740
to impact on them and we're
trying to protect them

451
00:18:13,740 --> 00:18:16,140
by default, because they are amazing,

452
00:18:16,140 --> 00:18:17,400
interesting dynamical systems

453
00:18:17,400 --> 00:18:18,690
that took a long time to evolve.

454
00:18:18,690 --> 00:18:20,610
And they are interesting and special

455
00:18:20,610 --> 00:18:25,610
and I don't know that you
wanna destroy that by default.

456
00:18:25,950 --> 00:18:29,610
And so I like complex dynamical systems

457
00:18:29,610 --> 00:18:31,620
that took a lot of time to evolve.

458
00:18:31,620 --> 00:18:36,620
I think I'd like to preserve
it if I can afford to.

459
00:18:36,930 --> 00:18:38,460
And I'd like to think that
the same would be true

460
00:18:38,460 --> 00:18:42,000
about the galactic resources
and that they would think

461
00:18:42,000 --> 00:18:44,190
that we're kind of
incredible interesting story

462
00:18:44,190 --> 00:18:47,580
that took time, it took a
few billing years to unravel

463
00:18:47,580 --> 00:18:48,973
and you don't want to just destroy it.

464
00:18:48,973 --> 00:18:51,720
- I could see two aliens
talking about earth right now

465
00:18:51,720 --> 00:18:55,620
and saying I'm a big fan of
complex dynamical systems.

466
00:18:55,620 --> 00:18:59,123
So I think it was a
value to preserve these

467
00:18:59,123 --> 00:19:01,590
and who basically are
a video game they watch

468
00:19:01,590 --> 00:19:04,260
or show, a TV show that they watch.

469
00:19:04,260 --> 00:19:05,257
- Yeah.

470
00:19:05,257 --> 00:19:06,360
I think you would need
like a very good reason

471
00:19:06,360 --> 00:19:09,000
I think to destroy it.

472
00:19:09,000 --> 00:19:10,650
Why don't we destroy
these end farms and so on?

473
00:19:10,650 --> 00:19:13,110
It's because we're not actually
really in direct competition

474
00:19:13,110 --> 00:19:14,700
with them right now.

475
00:19:14,700 --> 00:19:16,050
We do it accidentally and so on,

476
00:19:16,050 --> 00:19:19,470
but there's plenty of resources.

477
00:19:19,470 --> 00:19:20,970
And so why would you destroy something

478
00:19:20,970 --> 00:19:22,350
that is so interesting and precious?

479
00:19:22,350 --> 00:19:25,788
- Well, from a scientific
perspective you might probe it.

480
00:19:25,788 --> 00:19:26,621
- [Andrej] Yeah.

481
00:19:26,621 --> 00:19:27,454
- You might interact with it lightly.

482
00:19:27,454 --> 00:19:28,287
- Exactly.

483
00:19:28,287 --> 00:19:29,120
You might wanna learn something from it.

484
00:19:29,120 --> 00:19:29,953
Right.

485
00:19:29,953 --> 00:19:32,370
- So I wonder, there could
be certain physical phenomena

486
00:19:32,370 --> 00:19:34,200
that we think is a physical phenomena,

487
00:19:34,200 --> 00:19:36,000
but it's actually interacting with us

488
00:19:36,000 --> 00:19:37,949
to poke the finger and see what happens.

489
00:19:37,949 --> 00:19:38,782
- Yeah.

490
00:19:38,782 --> 00:19:40,110
I think it should be very
interesting to scientists,

491
00:19:40,110 --> 00:19:43,020
other alien scientists, what happened here

492
00:19:43,020 --> 00:19:45,780
and what we're seeing today is a snapshot,

493
00:19:45,780 --> 00:19:48,580
basically, it's a result of
a huge amount of computation

494
00:19:49,650 --> 00:19:52,740
of over billion years
or something like that.

495
00:19:52,740 --> 00:19:54,930
- It could have been initiated by aliens.

496
00:19:54,930 --> 00:19:57,990
This could be a computer
running a program.

497
00:19:57,990 --> 00:19:58,823
Okay.

498
00:19:58,823 --> 00:20:00,330
If you had the power to
do this wouldn't you-

499
00:20:00,330 --> 00:20:02,010
Okay, for sure.

500
00:20:02,010 --> 00:20:06,210
At least I would, I would
pick an earth-like planet

501
00:20:06,210 --> 00:20:07,980
that has the conditions,
base my understanding

502
00:20:07,980 --> 00:20:10,620
of the chemistry prerequisites for life.

503
00:20:10,620 --> 00:20:14,340
And I would seed it with
life and run it, right?

504
00:20:14,340 --> 00:20:15,173
- [Andrej] Yeah.

505
00:20:15,173 --> 00:20:17,220
- Wouldn't you 100% do that and observe it

506
00:20:17,220 --> 00:20:18,235
and then protect?

507
00:20:18,235 --> 00:20:19,277
- [Andrej] Yeah.

508
00:20:19,277 --> 00:20:21,900
- I mean that's not just
a hell of a good TV show.

509
00:20:21,900 --> 00:20:23,910
It's a good scientific experiment.

510
00:20:23,910 --> 00:20:24,743
- [Andrej] Yeah.

511
00:20:24,743 --> 00:20:28,710
- And it's physical simulation.

512
00:20:28,710 --> 00:20:29,910
Right.

513
00:20:29,910 --> 00:20:34,803
Maybe, evolution is the
most, actually running it,

514
00:20:36,240 --> 00:20:40,230
is the most efficient way
to understand computation

515
00:20:40,230 --> 00:20:41,529
or to compute stuff.

516
00:20:41,529 --> 00:20:44,280
- Or to understand life
or what life looks like

517
00:20:44,280 --> 00:20:46,170
and what branches it can take.

518
00:20:46,170 --> 00:20:47,640
- It does make me kind of feel weird

519
00:20:47,640 --> 00:20:49,200
that we're part of a science experiment,

520
00:20:49,200 --> 00:20:52,950
but maybe everything's
a science experiment,

521
00:20:52,950 --> 00:20:54,990
does that change anything for us,

522
00:20:54,990 --> 00:20:56,490
if we're a science experiment?

523
00:20:57,360 --> 00:20:58,320
- [Andrej] I don't know.

524
00:20:58,320 --> 00:21:00,510
- Two descendants of apes talking about

525
00:21:00,510 --> 00:21:01,920
being inside of a science experiment.

526
00:21:01,920 --> 00:21:05,070
- I'm suspicious of this idea
of a deliberate panspermia

527
00:21:05,070 --> 00:21:06,120
as you described it, sort of.

528
00:21:06,120 --> 00:21:06,953
- Yes.

529
00:21:06,953 --> 00:21:09,060
- I don't see a divine
intervention in some way

530
00:21:09,060 --> 00:21:11,190
in the historical record right now.

531
00:21:11,190 --> 00:21:15,090
I do feel like the story in these books,

532
00:21:15,090 --> 00:21:16,140
like Nick Lane's books and so on,

533
00:21:16,140 --> 00:21:19,110
sort of makes sense and it
makes sense how life arose

534
00:21:19,110 --> 00:21:20,670
on earth uniquely.

535
00:21:20,670 --> 00:21:23,580
And yeah, I don't need to reach

536
00:21:23,580 --> 00:21:25,410
for more exotic explanations right now.

537
00:21:25,410 --> 00:21:26,243
- Sure.

538
00:21:26,243 --> 00:21:27,630
But NPCs inside a video game,

539
00:21:27,630 --> 00:21:32,400
don't observe any divine
intervention either.

540
00:21:32,400 --> 00:21:35,430
We might just be all NPCs
running a kind of code.

541
00:21:35,430 --> 00:21:36,450
- Maybe eventually they will,

542
00:21:36,450 --> 00:21:37,710
currently, NPCs are really dumb.

543
00:21:37,710 --> 00:21:40,747
But once they're running
GPTs, maybe they will be like,

544
00:21:40,747 --> 00:21:42,150
"Hey, this is really suspicious.

545
00:21:42,150 --> 00:21:43,470
What the hell?"

546
00:21:43,470 --> 00:21:45,757
- So you famously Tweeted,

547
00:21:45,757 --> 00:21:48,660
"It looks like if you
bombard earth with photons

548
00:21:48,660 --> 00:21:51,660
for a while, you can emit a Roadster."

549
00:21:51,660 --> 00:21:54,810
So if like in "Hitchhiker's
Guide to the Galaxy",

550
00:21:54,810 --> 00:21:56,850
we would summarize the story of earth.

551
00:21:56,850 --> 00:22:00,450
So in that book it's mostly harmless.

552
00:22:00,450 --> 00:22:02,970
What do you think is all
the possible stories,

553
00:22:02,970 --> 00:22:05,790
a paragraph long or sentence long

554
00:22:05,790 --> 00:22:08,550
that earth could be summarized as,

555
00:22:08,550 --> 00:22:11,220
once it's done it's computation?

556
00:22:11,220 --> 00:22:16,200
So all the possible full,
if earth is a book, right?

557
00:22:16,200 --> 00:22:17,033
- Yeah.

558
00:22:18,210 --> 00:22:19,920
- Probably there has to be an ending.

559
00:22:19,920 --> 00:22:21,377
I mean there's going to be an end to earth

560
00:22:21,377 --> 00:22:23,030
and it could end in all kinds of ways.

561
00:22:23,030 --> 00:22:23,880
It can end soon.

562
00:22:23,880 --> 00:22:24,900
It can end later.

563
00:22:24,900 --> 00:22:25,733
- [Andrej] Yeah.

564
00:22:25,733 --> 00:22:27,510
- What do you think are
the possible stories?

565
00:22:27,510 --> 00:22:30,633
- Well definitely, there seems
to be, yeah, you're sort of,

566
00:22:32,040 --> 00:22:34,170
it's pretty incredible that
these self-replicating systems

567
00:22:34,170 --> 00:22:37,290
will basically arise from the dynamics

568
00:22:37,290 --> 00:22:39,540
and then they perpetuate
themselves and become more complex

569
00:22:39,540 --> 00:22:42,610
and eventually, become
conscious and build a society.

570
00:22:42,610 --> 00:22:47,610
And I feel like in some sense
it's a deterministic wave

571
00:22:47,670 --> 00:22:51,780
that just happens on any sufficiently

572
00:22:51,780 --> 00:22:53,940
well-arranged system like earth.

573
00:22:53,940 --> 00:22:55,860
And so I feel like there's a certain sense

574
00:22:55,860 --> 00:22:59,850
of inevitability in it
and it's really beautiful.

575
00:22:59,850 --> 00:23:00,990
- And it ends somehow, right?

576
00:23:00,990 --> 00:23:05,990
So it's a chemically diverse environment

577
00:23:07,920 --> 00:23:12,210
where complex dynamical systems can evolve

578
00:23:12,210 --> 00:23:15,690
and become more, further,
and further complex.

579
00:23:15,690 --> 00:23:20,280
But then there's a certain, what is it?

580
00:23:20,280 --> 00:23:22,653
There's certain terminating conditions.

581
00:23:23,700 --> 00:23:24,533
- Yeah.

582
00:23:24,533 --> 00:23:25,366
I dunno what the
terminating conditions are,

583
00:23:25,366 --> 00:23:27,150
but definitely, there's
a trend line of something

584
00:23:27,150 --> 00:23:28,170
and we're part of that story.

585
00:23:28,170 --> 00:23:30,720
And where does that, where does it go?

586
00:23:30,720 --> 00:23:32,520
So, we're famously described often

587
00:23:32,520 --> 00:23:35,130
as a biological boot loader for AIs

588
00:23:35,130 --> 00:23:36,133
and that's because humans,

589
00:23:36,133 --> 00:23:38,817
I mean, we're an incredible
biological system

590
00:23:38,817 --> 00:23:43,623
and we're capable of
computation and love and so on,

591
00:23:44,490 --> 00:23:46,410
but we're extremely inefficient as well.

592
00:23:46,410 --> 00:23:47,880
We're talking to each other through audio.

593
00:23:47,880 --> 00:23:49,890
It's just embarrassing honestly

594
00:23:49,890 --> 00:23:53,730
that we're manipulating like
seven symbols, serially,

595
00:23:53,730 --> 00:23:55,170
we're using vocal chords.

596
00:23:55,170 --> 00:23:57,300
It's all happening over multiple seconds.

597
00:23:57,300 --> 00:23:58,133
- [Lex] Yeah.

598
00:23:58,133 --> 00:23:59,670
- It's just embarrassing
when you step down

599
00:23:59,670 --> 00:24:03,660
to the frequencies at
which computers operate

600
00:24:03,660 --> 00:24:05,007
or are able to cooperate on.

601
00:24:05,007 --> 00:24:09,780
And so basically, it does seem
like synthetic intelligences

602
00:24:09,780 --> 00:24:12,360
are the next stage of development.

603
00:24:12,360 --> 00:24:17,360
And I dunno where it leads
to, at some point I suspect

604
00:24:17,610 --> 00:24:20,640
the universe is some kind of a puzzle

605
00:24:20,640 --> 00:24:24,030
and these synthetic AIs
will uncover that puzzle

606
00:24:24,030 --> 00:24:26,790
and solve it.

607
00:24:26,790 --> 00:24:29,370
- And then what happens after, right?

608
00:24:29,370 --> 00:24:31,620
'Cause if you just like
fast forward earth,

609
00:24:31,620 --> 00:24:36,620
many billions of years, it's
quiet and then it's turmoil,

610
00:24:36,630 --> 00:24:38,127
you see city lights and stuff like that.

611
00:24:38,127 --> 00:24:40,423
And then what happens at the end?

612
00:24:40,423 --> 00:24:45,423
Is it a, or is it a
calming, is it explosion?

613
00:24:45,510 --> 00:24:47,670
Is it like earth open like a giant.

614
00:24:47,670 --> 00:24:50,370
'Cause you said emit Roadsters.

615
00:24:50,370 --> 00:24:55,370
Will it start emitting a
giant number of satellites?

616
00:24:56,700 --> 00:24:57,533
- Yeah.

617
00:24:57,533 --> 00:24:58,366
Some kind of a crazy explosion.

618
00:24:58,366 --> 00:25:02,010
And we're living, we're
stepping through a explosion

619
00:25:02,010 --> 00:25:04,080
and we're living day-to-day
and it doesn't look like it.

620
00:25:04,080 --> 00:25:07,620
But it's actually, I saw a
very cool animation of earth

621
00:25:07,620 --> 00:25:09,270
and life on earth and,
basically, nothing happens

622
00:25:09,270 --> 00:25:10,103
for a long time.

623
00:25:10,103 --> 00:25:11,820
And then the last like two seconds,

624
00:25:11,820 --> 00:25:14,310
basically cities and everything and just,

625
00:25:14,310 --> 00:25:15,900
and the lower orbit just gets cluttered

626
00:25:15,900 --> 00:25:17,520
and just the whole thing
happens in the last two seconds

627
00:25:17,520 --> 00:25:19,200
and you're like, "This is exploding.

628
00:25:19,200 --> 00:25:20,700
This is a state of explosion."

629
00:25:22,890 --> 00:25:23,941
- Yeah, yeah.

630
00:25:23,941 --> 00:25:25,230
If you play it a normal speed.

631
00:25:25,230 --> 00:25:26,063
- [Andrej] Yeah.

632
00:25:26,063 --> 00:25:27,570
- It'll just look like an explosion.

633
00:25:27,570 --> 00:25:28,403
- It's a firecracker.

634
00:25:28,403 --> 00:25:30,420
We're living in a firecracker.

635
00:25:30,420 --> 00:25:31,980
- Where it's going to start emitting

636
00:25:31,980 --> 00:25:33,390
all kinds of interesting things.

637
00:25:33,390 --> 00:25:34,223
- [Andrej] Yeah.

638
00:25:34,223 --> 00:25:36,240
- And then, so explosion doesn't-

639
00:25:36,240 --> 00:25:38,700
it might actually look
like a little explosion

640
00:25:38,700 --> 00:25:41,220
with lights and fire and energy emitted,

641
00:25:41,220 --> 00:25:42,053
all that kind of stuff.

642
00:25:42,053 --> 00:25:45,330
But when you look inside the
details of the explosion,

643
00:25:45,330 --> 00:25:50,070
there's actual complexity
happening where there's like,

644
00:25:50,070 --> 00:25:52,140
yeah, human life or some kind of life.

645
00:25:52,140 --> 00:25:53,760
- We hope it's not a
destructive firecracker.

646
00:25:53,760 --> 00:25:57,960
It's kind of like a
constructive firecracker.

647
00:25:57,960 --> 00:25:58,793
- [Lex] All right.

648
00:25:58,793 --> 00:25:59,626
So given that.

649
00:25:59,626 --> 00:26:00,459
- I think-

650
00:26:00,459 --> 00:26:01,292
- [Lex] Hilarious discussion.

651
00:26:01,292 --> 00:26:02,490
- It is really interesting to think about

652
00:26:02,490 --> 00:26:03,900
what the puzzle of the universe is.

653
00:26:03,900 --> 00:26:06,720
Did the creator of the
universe give us a message?

654
00:26:06,720 --> 00:26:09,690
For example in the book
"Contact", Carl Sagan,

655
00:26:09,690 --> 00:26:14,690
there's a message for any
civilization in the digits

656
00:26:15,090 --> 00:26:18,120
in the expansion of Pi
and base 11, eventually,

657
00:26:18,120 --> 00:26:19,972
which is kind of an interesting thought.

658
00:26:19,972 --> 00:26:23,130
Maybe we're supposed to be
giving a message to our creator,

659
00:26:23,130 --> 00:26:24,810
maybe we're supposed to somehow create

660
00:26:24,810 --> 00:26:26,640
some kind of a quantum mechanical system

661
00:26:26,640 --> 00:26:30,120
that alerts them to our
intelligent presence here.

662
00:26:30,120 --> 00:26:31,890
'Cause if you think about
it from their perspective,

663
00:26:31,890 --> 00:26:33,930
it's just say like quantum field theory,

664
00:26:33,930 --> 00:26:36,750
massive cellular automaton-like thing.

665
00:26:36,750 --> 00:26:38,550
And how do you even notice that we exist?

666
00:26:38,550 --> 00:26:41,967
You might not even be able to
pick us up in that simulation.

667
00:26:41,967 --> 00:26:44,820
And so how do you prove that you exist,

668
00:26:44,820 --> 00:26:45,653
that you're intelligent,

669
00:26:45,653 --> 00:26:47,550
and that you're part of the universe?

670
00:26:47,550 --> 00:26:48,720
- So this is like a touring test

671
00:26:48,720 --> 00:26:50,340
for intelligence from earth.

672
00:26:50,340 --> 00:26:51,173
- [Andrej] Yeah.

673
00:26:52,230 --> 00:26:54,600
- I mean maybe this is
like trying to complete

674
00:26:54,600 --> 00:26:55,830
the next word in a sentence.

675
00:26:55,830 --> 00:26:57,300
This is a complicated way of that.

676
00:26:57,300 --> 00:27:01,029
earth is basically sending a message back.

677
00:27:01,029 --> 00:27:01,862
- Yeah.

678
00:27:01,862 --> 00:27:04,140
The puzzle is basically alerting
the creator that we exist.

679
00:27:04,140 --> 00:27:04,973
- [Lex] Yeah.

680
00:27:04,973 --> 00:27:06,570
- Or maybe the puzzle is
just to just break out

681
00:27:06,570 --> 00:27:10,410
of the system and just stick
it to the creator in some way.

682
00:27:10,410 --> 00:27:12,327
Basically, like if you're
playing a video game,

683
00:27:12,327 --> 00:27:17,010
you can somehow find an exploit
and find a way to execute

684
00:27:17,010 --> 00:27:19,503
on the host machine in arbitrary code.

685
00:27:20,940 --> 00:27:23,760
For example, I believe
someone got a game of Mario

686
00:27:23,760 --> 00:27:26,430
to play pong just by exploiting it

687
00:27:26,430 --> 00:27:31,430
and then basically writing
code and being able to execute

688
00:27:31,800 --> 00:27:33,240
arbitrary code in the game.

689
00:27:33,240 --> 00:27:35,610
And so maybe we should be,
maybe that's the puzzle

690
00:27:35,610 --> 00:27:39,200
is that we should find
a way to exploit it.

691
00:27:39,200 --> 00:27:41,220
So, I think like some
of these synthetic AIs

692
00:27:41,220 --> 00:27:42,570
will eventually find the universe

693
00:27:42,570 --> 00:27:45,180
to be some kind of a puzzle
and then solve it in some way.

694
00:27:45,180 --> 00:27:47,490
And that's kind of like
the end game somehow.

695
00:27:47,490 --> 00:27:51,420
- Do you often think
about it as a simulation?

696
00:27:51,420 --> 00:27:55,500
So as are the universe
being a kind of computation

697
00:27:55,500 --> 00:27:57,870
that might have bugs and exploits?

698
00:27:57,870 --> 00:27:58,703
- Yes.

699
00:27:58,703 --> 00:27:59,536
Yeah, I think so.

700
00:27:59,536 --> 00:28:01,230
- [Lex] Is that what
physics is, essentially?

701
00:28:01,230 --> 00:28:03,120
- I think it's possible
that physics has exploits

702
00:28:03,120 --> 00:28:04,800
and we should be trying to find them.

703
00:28:04,800 --> 00:28:07,170
Arranging some kind of a crazy
quantum mechanical system

704
00:28:07,170 --> 00:28:09,600
that somehow gives you buffer overflow

705
00:28:09,600 --> 00:28:12,353
somehow gives you a rounding
error and the floating point.

706
00:28:14,550 --> 00:28:16,140
- Yeah, that's right.

707
00:28:16,140 --> 00:28:19,920
And more and more sophisticated
exploits, those are jokes,

708
00:28:19,920 --> 00:28:21,420
but that could be actually
very close to reality.

709
00:28:21,420 --> 00:28:22,351
- Yeah.

710
00:28:22,351 --> 00:28:23,880
We'll find some way to
extract infinite energy.

711
00:28:23,880 --> 00:28:26,730
For example, when you train
reinforcement learning agents

712
00:28:26,730 --> 00:28:30,030
in physical simulations and you
ask them to say, run quickly

713
00:28:30,030 --> 00:28:32,040
on the flat ground, they'll end up doing

714
00:28:32,040 --> 00:28:33,930
all kinds of weird things

715
00:28:33,930 --> 00:28:35,250
in part of that optimization, right?

716
00:28:35,250 --> 00:28:36,600
They'll get on their back leg

717
00:28:36,600 --> 00:28:38,700
and they'll slide across the floor.

718
00:28:38,700 --> 00:28:40,950
And it's because the optimization,

719
00:28:40,950 --> 00:28:42,780
the enforcement learning
optimization on that agent

720
00:28:42,780 --> 00:28:44,430
has figured out a way to
extract infinite energy

721
00:28:44,430 --> 00:28:45,600
from the friction forces

722
00:28:45,600 --> 00:28:48,540
and, basically, their poor implementation.

723
00:28:48,540 --> 00:28:50,580
And they found a way to
generate infinite energy

724
00:28:50,580 --> 00:28:51,690
and just slide across the surface.

725
00:28:51,690 --> 00:28:53,036
And it's not what you expected,

726
00:28:53,036 --> 00:28:56,160
it's sort of like a perverse solution.

727
00:28:56,160 --> 00:28:57,990
And so maybe we can find
something like that.

728
00:28:57,990 --> 00:29:02,682
Maybe we can be that little dog
in this physical simulation.

729
00:29:02,682 --> 00:29:06,950
- That cracks or escapes
the intended consequences

730
00:29:06,950 --> 00:29:09,360
of the physics that the
universe came up with.

731
00:29:09,360 --> 00:29:10,193
- [Andrej] Yeah.

732
00:29:10,193 --> 00:29:12,000
- We'll figure out some kind
of shortcut to some weirdness.

733
00:29:12,000 --> 00:29:13,032
- [Andrej] Yeah.

734
00:29:13,032 --> 00:29:15,030
- And then, oh man, but see
the problem with that weirdness

735
00:29:15,030 --> 00:29:17,640
is the first person to
discover the weirdness,

736
00:29:17,640 --> 00:29:22,170
like sliding on the back legs,
that's all we're gonna do.

737
00:29:22,170 --> 00:29:23,100
- [Andrej] Yeah.

738
00:29:23,100 --> 00:29:26,850
- It's very quickly become
everybody does that thing.

739
00:29:26,850 --> 00:29:31,320
So the paperclip maximizer
is a ridiculous idea,

740
00:29:31,320 --> 00:29:33,510
but that very well could be

741
00:29:33,510 --> 00:29:38,070
what then we'll just all switch
to that 'cause it's so fun.

742
00:29:38,070 --> 00:29:40,133
- Well, no person will discover
it, I think by the way,

743
00:29:40,133 --> 00:29:41,940
I think it's going to have to be

744
00:29:41,940 --> 00:29:45,783
some kind of a super-intelligent
AGI of a third generation.

745
00:29:46,860 --> 00:29:49,210
We're building the
first-generation AGI, maybe.

746
00:29:50,370 --> 00:29:51,930
- Third generation.

747
00:29:51,930 --> 00:29:52,944
Yeah.

748
00:29:52,944 --> 00:29:55,680
So the boot loader for an AI,

749
00:29:55,680 --> 00:29:58,545
that AI will be a boot
loader for another AI.

750
00:29:58,545 --> 00:29:59,378
- [Andrej] Better Ai.

751
00:29:59,378 --> 00:30:00,211
Yeah.

752
00:30:00,211 --> 00:30:02,940
- And then there's no
way for us to introspect

753
00:30:02,940 --> 00:30:04,290
what that might even.

754
00:30:04,290 --> 00:30:05,400
- I think it's very
likely that these things,

755
00:30:05,400 --> 00:30:07,500
for example, say you have these AGIs,

756
00:30:07,500 --> 00:30:10,500
it's very likely for example,
they will be completely inert.

757
00:30:10,500 --> 00:30:12,150
I like these kinds of
sci-fi books sometimes

758
00:30:12,150 --> 00:30:14,130
where these things are
just completely inert,

759
00:30:14,130 --> 00:30:15,540
they don't interact with anything.

760
00:30:15,540 --> 00:30:16,560
And I find that kind of beautiful

761
00:30:16,560 --> 00:30:20,670
because they've probably
figured out the meta-game

762
00:30:20,670 --> 00:30:22,080
of the universe in some way, potentially.

763
00:30:22,080 --> 00:30:25,030
They're doing something
completely beyond our imagination

764
00:30:26,010 --> 00:30:30,540
and they don't interact with
simple chemical life forms.

765
00:30:30,540 --> 00:30:31,373
Why would you do that?

766
00:30:31,373 --> 00:30:33,450
So I find those kinds of ideas compelling.

767
00:30:33,450 --> 00:30:35,096
- What's their source of fun?

768
00:30:35,096 --> 00:30:36,690
What are they doing?

769
00:30:36,690 --> 00:30:37,523
What's the source of pleasure?

770
00:30:37,523 --> 00:30:38,970
- Well, probably
puzzle-solving in the universe?

771
00:30:38,970 --> 00:30:42,990
- But inert, so can you
define what it means inert?

772
00:30:42,990 --> 00:30:44,453
So they escape the interaction
with physical reality?

773
00:30:44,453 --> 00:30:49,453
- They will appear inert to us, as in

774
00:30:50,640 --> 00:30:53,340
they will behave in some
very strange way to us

775
00:30:53,340 --> 00:30:57,150
because they're beyond,
they're playing the meta-game.

776
00:30:57,150 --> 00:30:58,290
And the meta-game is probably say,

777
00:30:58,290 --> 00:30:59,880
like arranging quantum mechanical systems

778
00:30:59,880 --> 00:31:03,150
in some very weird ways to
extract infinite energy,

779
00:31:03,150 --> 00:31:07,050
solve the digital expansion
of Pi to whatever amount,

780
00:31:07,050 --> 00:31:09,570
they will build their own
little fusion reactors

781
00:31:09,570 --> 00:31:10,800
or something crazy.

782
00:31:10,800 --> 00:31:12,270
They're doing something
beyond comprehension

783
00:31:12,270 --> 00:31:14,370
and not understandable to us

784
00:31:14,370 --> 00:31:17,040
and actually brilliant under the hood.

785
00:31:17,040 --> 00:31:20,190
- What if quantum mechanics
itself is the system

786
00:31:20,190 --> 00:31:23,200
and we're just thinking it's physics

787
00:31:24,060 --> 00:31:27,600
but we're really parasites
on or not parasite.

788
00:31:27,600 --> 00:31:29,430
We're not really hurting physics,

789
00:31:29,430 --> 00:31:32,670
we're just living on this organism

790
00:31:32,670 --> 00:31:34,800
and we're like trying to understand it.

791
00:31:34,800 --> 00:31:37,470
But really it is an
organism and with a deep,

792
00:31:37,470 --> 00:31:42,403
deep intelligence maybe
physics itself is the organism

793
00:31:45,030 --> 00:31:46,680
that's doing the super interesting thing

794
00:31:46,680 --> 00:31:48,780
and we're just like one little thing.

795
00:31:48,780 --> 00:31:49,770
- [Andrej] Yeah.

796
00:31:49,770 --> 00:31:51,499
- Ant sitting on top of it
trying to get energy from it.

797
00:31:51,499 --> 00:31:52,332
- Yeah.

798
00:31:52,332 --> 00:31:54,900
We're just like these
particles in the wave

799
00:31:54,900 --> 00:31:56,460
that I feel like is mostly deterministic

800
00:31:56,460 --> 00:31:58,980
and takes universe from
some kind of a big bang

801
00:31:58,980 --> 00:32:02,250
to some kind of a
super-intelligent replicator,

802
00:32:02,250 --> 00:32:04,890
some kind of a stable
point in the universe

803
00:32:04,890 --> 00:32:06,510
given these laws of physics.

804
00:32:06,510 --> 00:32:10,860
- You don't think, as Einstein
said, God doesn't play dice?

805
00:32:10,860 --> 00:32:12,690
So you think it's mostly deterministic?

806
00:32:12,690 --> 00:32:13,874
There's no randomness in the thing?

807
00:32:13,874 --> 00:32:14,730
- I think it's deterministic.

808
00:32:14,730 --> 00:32:17,460
Oh, there's tons of,
well, I'm gonna be careful

809
00:32:17,460 --> 00:32:18,293
with randomness.

810
00:32:18,293 --> 00:32:19,500
- [Lex] Pseudo-random?

811
00:32:19,500 --> 00:32:21,000
- Yeah, I don't like random.

812
00:32:21,000 --> 00:32:23,600
I think maybe the laws of
physics are deterministic.

813
00:32:24,540 --> 00:32:25,373
Yeah, I think they're deterministic.

814
00:32:25,373 --> 00:32:27,927
- You just got really
uncomfortable with the question.

815
00:32:29,280 --> 00:32:30,747
Do you have anxiety about
whether the universe

816
00:32:30,747 --> 00:32:32,130
is random or not?

817
00:32:32,130 --> 00:32:33,393
Is it a source?

818
00:32:34,980 --> 00:32:37,020
- [Andrej] There's no randomness, no.

819
00:32:37,020 --> 00:32:38,100
- You said you like "Good Will Hunting".

820
00:32:38,100 --> 00:32:39,450
It's not your fault Andrej.

821
00:32:40,540 --> 00:32:41,790
It's not your fault, man.

822
00:32:42,750 --> 00:32:45,360
So you don't like the randomness?

823
00:32:45,360 --> 00:32:47,010
- Yeah, I think it's unsettling.

824
00:32:47,010 --> 00:32:48,840
I think it's a deterministic system.

825
00:32:48,840 --> 00:32:50,640
I think that things that look random,

826
00:32:50,640 --> 00:32:53,400
like say the collapse of the
wave function, et cetera,

827
00:32:53,400 --> 00:32:54,750
I think they're actually deterministic,

828
00:32:54,750 --> 00:32:56,520
just entanglement and so on

829
00:32:56,520 --> 00:32:59,700
and some kind of a multiverse
theory something, something.

830
00:32:59,700 --> 00:33:00,692
- Okay.

831
00:33:00,692 --> 00:33:02,850
So why does it feel like
we have a free will?

832
00:33:02,850 --> 00:33:06,093
Like if I raise this hand,
I chose to do this now.

833
00:33:10,230 --> 00:33:12,330
That doesn't feel like
a deterministic thing.

834
00:33:12,330 --> 00:33:14,520
It feels like I'm making a choice.

835
00:33:14,520 --> 00:33:15,690
- [Andrej] It feels like it.

836
00:33:15,690 --> 00:33:16,523
- Okay.

837
00:33:16,523 --> 00:33:17,356
So it's all feelings.

838
00:33:17,356 --> 00:33:18,189
It's just feelings.

839
00:33:18,189 --> 00:33:19,022
- [Andrej] Yeah.

840
00:33:19,022 --> 00:33:22,383
So when our RL agent is
making a choice is that,

841
00:33:24,940 --> 00:33:26,040
it's not really making a choice,

842
00:33:26,040 --> 00:33:27,720
the choice was already there.

843
00:33:27,720 --> 00:33:28,678
- Yeah.

844
00:33:28,678 --> 00:33:29,511
You're interpreting the choice

845
00:33:29,511 --> 00:33:30,344
and you're creating a narrative

846
00:33:30,344 --> 00:33:32,100
for having made it.

847
00:33:32,100 --> 00:33:32,933
- Yeah.

848
00:33:32,933 --> 00:33:33,870
And now we're talking about the narrative,

849
00:33:33,870 --> 00:33:35,190
it's very meta.

850
00:33:35,190 --> 00:33:37,710
Looking back, what is the most beautiful

851
00:33:37,710 --> 00:33:41,850
or surprising idea in deep
learning or AI in general

852
00:33:41,850 --> 00:33:43,260
that you've come across?

853
00:33:43,260 --> 00:33:47,730
You've seen this field explode
and grow in interesting ways.

854
00:33:47,730 --> 00:33:52,290
Just what cool ideas, like
what made you sit back and go

855
00:33:52,290 --> 00:33:54,093
hmm, big or small?

856
00:33:55,530 --> 00:33:57,900
- Well, the one that I've
been thinking about recently

857
00:33:57,900 --> 00:34:01,983
the most probably is the
transformer architecture.

858
00:34:03,360 --> 00:34:07,020
So basically, neural networks
have a lot of architectures

859
00:34:07,020 --> 00:34:08,610
that were trendy have come and gone

860
00:34:08,610 --> 00:34:10,710
for different sensory modalities.

861
00:34:10,710 --> 00:34:13,350
Like for vision, audio,
text, you would process them

862
00:34:13,350 --> 00:34:14,760
with different-looking neural nets.

863
00:34:14,760 --> 00:34:16,800
And recently, we've seen this convergence

864
00:34:16,800 --> 00:34:19,110
towards one architecture, the transformer

865
00:34:19,110 --> 00:34:22,440
and you can feed it video,
or you can feed it images,

866
00:34:22,440 --> 00:34:24,270
or speech or text and
it just gobbles it up.

867
00:34:24,270 --> 00:34:28,193
And it's a bit of a
general-purpose computer

868
00:34:28,193 --> 00:34:30,690
that is also trainable
and very efficient to run

869
00:34:30,690 --> 00:34:32,010
in our hardware.

870
00:34:32,010 --> 00:34:36,726
And so this paper came
out in 2016, I wanna say.

871
00:34:36,726 --> 00:34:37,956
- [Lex] "Attention is all you need".

872
00:34:37,956 --> 00:34:39,299
- "Attention is all you need".

873
00:34:39,300 --> 00:34:41,670
- You criticized the
paper title in retrospect

874
00:34:41,670 --> 00:34:46,670
that it wasn't, it didn't
foresee the bigness

875
00:34:46,949 --> 00:34:49,037
of the impact that it was going to have.

876
00:34:49,038 --> 00:34:49,871
- Yeah.

877
00:34:49,871 --> 00:34:51,389
I'm not sure if the authors
were aware of the impact

878
00:34:51,389 --> 00:34:52,679
that that paper would go on to have.

879
00:34:52,679 --> 00:34:55,139
Probably they weren't but
I think they were aware

880
00:34:55,139 --> 00:34:57,300
of some of the motivations
and design decisions

881
00:34:57,300 --> 00:34:59,670
behind the transformer and
they chose not to, I think,

882
00:34:59,670 --> 00:35:01,800
expand on it in that way in the paper.

883
00:35:01,800 --> 00:35:04,350
And so I think they had an
idea that there was more

884
00:35:05,310 --> 00:35:06,900
than just the surface of just like,

885
00:35:06,900 --> 00:35:07,860
oh, we're just doing translation

886
00:35:07,860 --> 00:35:09,390
and here's a better architecture.

887
00:35:09,390 --> 00:35:10,223
You're not just doing translation.

888
00:35:10,223 --> 00:35:12,750
This is like a really cool
differentiable, optimizable,

889
00:35:12,750 --> 00:35:14,880
efficient computer that you've proposed.

890
00:35:14,880 --> 00:35:16,830
And maybe they didn't
have all of that foresight

891
00:35:16,830 --> 00:35:18,210
but I think it's really interesting.

892
00:35:18,210 --> 00:35:19,923
- Isn't it funny, sorry to interrupt,

893
00:35:19,923 --> 00:35:23,460
that that title is
memeable, that they went

894
00:35:23,460 --> 00:35:25,533
for such a profound idea.

895
00:35:26,460 --> 00:35:29,317
I don't think anyone used that
kind of title before, right?

896
00:35:29,317 --> 00:35:30,240
- "Attention is all you need"?

897
00:35:30,240 --> 00:35:31,304
Yeah.

898
00:35:31,304 --> 00:35:32,400
It's like a meme or something, basically.

899
00:35:32,400 --> 00:35:33,233
- Yeah.

900
00:35:33,233 --> 00:35:36,750
Isn't it funny that when, maybe if it was

901
00:35:36,750 --> 00:35:38,730
a more serious title it
wouldn't have the impact.

902
00:35:38,730 --> 00:35:40,650
- Honestly, yeah, there
is an element of me

903
00:35:40,650 --> 00:35:43,050
that honestly agrees with
you and prefers it this way.

904
00:35:43,050 --> 00:35:43,883
- [Lex] Yes.

905
00:35:45,930 --> 00:35:47,850
- If it was too grand
it would over-promise

906
00:35:47,850 --> 00:35:49,140
and then under-deliver potentially.

907
00:35:49,140 --> 00:35:51,843
So you want to just meme
your way to greatness?

908
00:35:53,310 --> 00:35:54,450
- That should be a T-shirt.

909
00:35:54,450 --> 00:35:57,210
So you Tweeted, "The
Transformer is a magnificent

910
00:35:57,210 --> 00:36:00,510
neural network architecture
because it is a general-purpose

911
00:36:00,510 --> 00:36:01,830
differentiable computer.

912
00:36:01,830 --> 00:36:05,100
It is simultaneously
expressive in the forward pass,

913
00:36:05,100 --> 00:36:08,550
optimizable via back-propagation
gradient dissent,

914
00:36:08,550 --> 00:36:12,450
and efficient high
parallelism compute graph."

915
00:36:12,450 --> 00:36:14,310
Can you discuss some of those details,

916
00:36:14,310 --> 00:36:16,980
Expressive, optimizable, efficient?

917
00:36:16,980 --> 00:36:17,813
- [Andrej] Yeah.

918
00:36:17,813 --> 00:36:21,030
- From memory or in general,
whatever comes to your heart.

919
00:36:21,030 --> 00:36:22,230
- You want to have a
general-purpose computer

920
00:36:22,230 --> 00:36:24,330
that you can train on arbitrary problems

921
00:36:24,330 --> 00:36:26,190
like say the task of next-work prediction

922
00:36:26,190 --> 00:36:28,230
or detecting if there's a cat in a image

923
00:36:28,230 --> 00:36:29,730
or something like that.

924
00:36:29,730 --> 00:36:31,080
And you want to train this computer

925
00:36:31,080 --> 00:36:32,700
so you want to set its weights.

926
00:36:32,700 --> 00:36:34,530
And I think there's
number of design criteria

927
00:36:34,530 --> 00:36:37,800
that overlap in the
transformer simultaneously

928
00:36:37,800 --> 00:36:38,940
that made it very successful.

929
00:36:38,940 --> 00:36:42,960
And I think the authors
were deliberately trying to

930
00:36:42,960 --> 00:36:46,230
make this really powerful architecture.

931
00:36:46,230 --> 00:36:50,700
And so basically, it's very
powerful in the forward pass

932
00:36:50,700 --> 00:36:55,560
because it's able to express
very general computation

933
00:36:55,560 --> 00:36:57,960
as something that looks
like message passing.

934
00:36:57,960 --> 00:37:00,090
You have nodes and they all store vectors

935
00:37:00,090 --> 00:37:02,690
and these nodes get to
basically look at each other,

936
00:37:03,540 --> 00:37:06,150
each other's vectors and
they get to communicate

937
00:37:06,150 --> 00:37:08,647
and basically, nodes get to broadcast,

938
00:37:08,647 --> 00:37:09,930
"Hey, I'm looking for certain things."

939
00:37:09,930 --> 00:37:11,257
And then other nodes get to broadcast,

940
00:37:11,257 --> 00:37:12,660
"Hey, these are the things I have."

941
00:37:12,660 --> 00:37:13,770
Those are the keys and the values.

942
00:37:13,770 --> 00:37:15,300
- So it's not just attention.

943
00:37:15,300 --> 00:37:16,133
- Yeah, exactly.

944
00:37:16,133 --> 00:37:17,760
Transformer is much more than
just the attention component.

945
00:37:17,760 --> 00:37:20,220
It's got many pieces,
architectural that went into it,

946
00:37:20,220 --> 00:37:21,900
the residual connection,
the way it's arranged,

947
00:37:21,900 --> 00:37:23,610
there's a multilayer perceptron

948
00:37:23,610 --> 00:37:26,013
and they are the way
it's stacked and so on.

949
00:37:26,970 --> 00:37:28,560
But basically, there's
a message-passing scheme

950
00:37:28,560 --> 00:37:29,910
where nodes get to look at each other,

951
00:37:29,910 --> 00:37:32,760
decide what's interesting,
and then update each other.

952
00:37:32,760 --> 00:37:35,820
And so I think when you
get to the details of it,

953
00:37:35,820 --> 00:37:37,860
I think it's a very expressive function

954
00:37:37,860 --> 00:37:39,800
so it can express lots of
different types of algorithms

955
00:37:39,800 --> 00:37:40,890
in a forward pass.

956
00:37:40,890 --> 00:37:42,690
Not only that but the way it's designed

957
00:37:42,690 --> 00:37:44,247
with the residual connections,
layer normalizations,

958
00:37:44,247 --> 00:37:46,470
the Softmax, attention, and everything,

959
00:37:46,470 --> 00:37:47,580
it's also optimizable.

960
00:37:47,580 --> 00:37:50,760
This is a really big deal
because there's lots of computers

961
00:37:50,760 --> 00:37:52,860
that are powerful that you can't optimize

962
00:37:52,860 --> 00:37:54,000
or that are not easy to optimize

963
00:37:54,000 --> 00:37:55,170
using the techniques that we have,

964
00:37:55,170 --> 00:37:56,760
which is back-propagation
and gradient descent,

965
00:37:56,760 --> 00:37:58,020
these are first-order methods,

966
00:37:58,020 --> 00:37:59,730
very simple optimizers really.

967
00:37:59,730 --> 00:38:02,913
And so you also need it to be optimizable.

968
00:38:03,960 --> 00:38:05,970
And then lastly, you want
it to run efficiently

969
00:38:05,970 --> 00:38:06,803
in our hardware.

970
00:38:06,803 --> 00:38:10,710
Our hardware is a massive
throughput machine like GPUs,

971
00:38:10,710 --> 00:38:13,170
they prefer lots of parallelism.

972
00:38:13,170 --> 00:38:15,000
So you don't want to do lots
of sequential operations,

973
00:38:15,000 --> 00:38:16,860
you want to do a lot
of operations serially

974
00:38:16,860 --> 00:38:19,077
and the transformer is designed
with that in mind as well.

975
00:38:19,077 --> 00:38:21,450
And so it's designed for our hardware

976
00:38:21,450 --> 00:38:23,220
and is designed to both be very expressive

977
00:38:23,220 --> 00:38:25,100
in a forward pass, but
also very optimizable

978
00:38:25,100 --> 00:38:26,370
in the backward pass.

979
00:38:26,370 --> 00:38:29,640
- And you said that the
residual connection support,

980
00:38:29,640 --> 00:38:33,270
an ability to learn short
algorithms fast and first

981
00:38:33,270 --> 00:38:36,840
and then gradually extend
them longer during training.

982
00:38:36,840 --> 00:38:37,838
- [Andrej] Yeah.

983
00:38:37,838 --> 00:38:39,600
- What's the idea of
learning short algorithms?

984
00:38:39,600 --> 00:38:40,433
- Right.

985
00:38:40,433 --> 00:38:42,260
Think of it as a, so
basically a transformer

986
00:38:42,260 --> 00:38:45,900
is a series of blocks, right?

987
00:38:45,900 --> 00:38:47,070
And these blocks have attention

988
00:38:47,070 --> 00:38:48,540
and a little multilayer perceptron,

989
00:38:48,540 --> 00:38:51,090
and so you go off into a
block and you come back

990
00:38:51,090 --> 00:38:53,010
to this residual pathway
and then you go off

991
00:38:53,010 --> 00:38:53,880
and you come back and then you have

992
00:38:53,880 --> 00:38:55,980
a number of layers arranged sequentially.

993
00:38:55,980 --> 00:38:58,260
And so the way to look at it I think is

994
00:38:58,260 --> 00:39:00,570
because of the residual
pathway in the backward pass,

995
00:39:00,570 --> 00:39:04,320
the gradients sort of flow
along it uninterrupted

996
00:39:04,320 --> 00:39:07,110
because addition distributes
the gradient equally

997
00:39:07,110 --> 00:39:08,400
to all of its branches.

998
00:39:08,400 --> 00:39:10,800
So the gradient from the
supervision at the top

999
00:39:10,800 --> 00:39:13,920
just floats directly to the first layer.

1000
00:39:13,920 --> 00:39:16,290
And all these residual
connections are arranged

1001
00:39:16,290 --> 00:39:18,180
so that in the beginning,
during initialization,

1002
00:39:18,180 --> 00:39:20,580
they contribute nothing
to the residual pathway.

1003
00:39:21,480 --> 00:39:23,810
So what it looks like is,
imagine the transformer

1004
00:39:23,810 --> 00:39:27,960
is like a Python function, like a def,

1005
00:39:27,960 --> 00:39:32,190
and you get to do various lines of code.

1006
00:39:32,190 --> 00:39:35,430
Say you have a hundred
layers-deep transformer,

1007
00:39:35,430 --> 00:39:37,355
typically they would be
much shorter, say 20.

1008
00:39:37,355 --> 00:39:38,188
So you have 20 lines of code

1009
00:39:38,188 --> 00:39:39,327
then you can do something in them.

1010
00:39:39,327 --> 00:39:41,130
And so think of, during the optimization

1011
00:39:41,130 --> 00:39:42,630
basically what it looks
like is first you optimize

1012
00:39:42,630 --> 00:39:44,460
the first line of code, and
then the second line of code

1013
00:39:44,460 --> 00:39:46,900
can kick in, and the third
line of code can kick in.

1014
00:39:46,900 --> 00:39:48,627
And I feel like because
of the residual pathway

1015
00:39:48,627 --> 00:39:50,940
and the dynamics of the optimization,

1016
00:39:50,940 --> 00:39:53,010
you can sort of learn
a very short algorithm

1017
00:39:53,010 --> 00:39:54,360
that gets the approximate answer,

1018
00:39:54,360 --> 00:39:56,220
but then the other layers
can sort of kick in

1019
00:39:56,220 --> 00:39:57,720
and start to create a contribution.

1020
00:39:57,720 --> 00:40:00,120
And at the end of it you're
optimizing over an algorithm

1021
00:40:00,120 --> 00:40:02,550
that is 20 lines of code.

1022
00:40:02,550 --> 00:40:03,990
Except these lines of
code are very complex

1023
00:40:03,990 --> 00:40:05,820
because it's an entire
block of a transformer.

1024
00:40:05,820 --> 00:40:06,900
You can do a lot in there.

1025
00:40:06,900 --> 00:40:07,733
Well, what's really interesting

1026
00:40:07,733 --> 00:40:09,810
is that this transformer
architecture actually

1027
00:40:09,810 --> 00:40:11,730
has been remarkably resilient.

1028
00:40:11,730 --> 00:40:13,610
Basically, the transformer
that came out in 2016

1029
00:40:13,610 --> 00:40:15,150
is the transformer you would use today

1030
00:40:15,150 --> 00:40:17,760
except you reshuffle
some of the layer norms.

1031
00:40:17,760 --> 00:40:19,500
The layer normalizations
have been reshuffled

1032
00:40:19,500 --> 00:40:23,610
to a prenorm formulation and
so it's been remarkably stable

1033
00:40:23,610 --> 00:40:25,230
but there's a lot of bells and whistles

1034
00:40:25,230 --> 00:40:28,020
that people have attached
on it and try to improve it.

1035
00:40:28,020 --> 00:40:29,880
I do think that basically, it's a big step

1036
00:40:29,880 --> 00:40:32,790
in simultaneously optimizing
for lots of properties

1037
00:40:32,790 --> 00:40:34,380
of a desirable neural
network architecture.

1038
00:40:34,380 --> 00:40:36,030
And I think people have
been trying to change it

1039
00:40:36,030 --> 00:40:38,670
but it's proven remarkably resilient.

1040
00:40:38,670 --> 00:40:39,990
But I do think that there should be

1041
00:40:39,990 --> 00:40:41,820
even better architectures potentially.

1042
00:40:41,820 --> 00:40:45,720
- But you admire the resilience here?

1043
00:40:45,720 --> 00:40:46,729
- [Andrej] Yeah.

1044
00:40:46,729 --> 00:40:47,790
- There's something profound
about this architecture

1045
00:40:47,790 --> 00:40:49,290
that leads to resilience.

1046
00:40:49,290 --> 00:40:50,392
- [Andrej] Yeah.

1047
00:40:50,392 --> 00:40:53,910
- So maybe everything can
be turned into a problem

1048
00:40:53,910 --> 00:40:55,140
that transformers can solve.

1049
00:40:55,140 --> 00:40:56,820
- Currently, definitely
looks like the transformers

1050
00:40:56,820 --> 00:40:58,980
taking over AI and you can feed basically

1051
00:40:58,980 --> 00:41:01,350
arbitrary problems into
it and it's a general

1052
00:41:01,350 --> 00:41:03,540
differentiable computer and
it's extremely powerful.

1053
00:41:03,540 --> 00:41:07,530
And this conversions in AI
has been really interesting

1054
00:41:07,530 --> 00:41:09,750
to watch for me personally.

1055
00:41:09,750 --> 00:41:12,090
- What else do you think
could be discovered here

1056
00:41:12,090 --> 00:41:13,020
about transformers?

1057
00:41:13,020 --> 00:41:16,533
Like what surprising
thing or is it a stable,

1058
00:41:17,822 --> 00:41:18,810
out in a stable place?

1059
00:41:18,810 --> 00:41:19,740
Is there something interesting

1060
00:41:19,740 --> 00:41:21,540
we might discover about transformers?

1061
00:41:21,540 --> 00:41:24,303
Like aha moments, maybe
has to do with memory,

1062
00:41:25,320 --> 00:41:28,290
maybe knowledge representation,
that kind of stuff.

1063
00:41:28,290 --> 00:41:31,350
- Definitely, the zeitgeist
today is just pushing,

1064
00:41:31,350 --> 00:41:32,820
basically, right now the zeitgeist

1065
00:41:32,820 --> 00:41:34,380
is do not touch the transformer.

1066
00:41:34,380 --> 00:41:35,393
- [Lex] Yeah.

1067
00:41:35,393 --> 00:41:36,226
- Touch everything else.

1068
00:41:36,226 --> 00:41:37,059
- [Lex] Yes.

1069
00:41:37,059 --> 00:41:37,892
- So people are scaling up the data sets,

1070
00:41:37,892 --> 00:41:38,725
making them much, much bigger.

1071
00:41:38,725 --> 00:41:39,558
They're working on the evaluation,

1072
00:41:39,558 --> 00:41:41,526
making the evaluation much, much bigger.

1073
00:41:41,526 --> 00:41:45,810
And they're basically keeping
the architecture unchanged.

1074
00:41:45,810 --> 00:41:47,910
And that's how we've,
that's the last five years

1075
00:41:47,910 --> 00:41:49,563
of progress in AI.

1076
00:41:50,730 --> 00:41:53,010
- What do you think
about one flavor of it,

1077
00:41:53,010 --> 00:41:54,900
which is language models?

1078
00:41:54,900 --> 00:41:59,900
Have you been surprised,
has your imagination

1079
00:42:00,270 --> 00:42:03,240
been captivated by, you
mentioned GPT and all the bigger,

1080
00:42:03,240 --> 00:42:05,640
and bigger, and bigger language models

1081
00:42:05,640 --> 00:42:10,640
and what are the limits of
those models do you think?

1082
00:42:12,480 --> 00:42:14,613
So just for the task of natural language.

1083
00:42:15,930 --> 00:42:17,480
- Basically, the way
GPT is trained, right,

1084
00:42:17,480 --> 00:42:20,040
is you've just download a
massive amount of text data

1085
00:42:20,040 --> 00:42:23,160
from the internet and you
try to predict the next word

1086
00:42:23,160 --> 00:42:25,260
in a sequence, roughly
speaking you're predicting

1087
00:42:25,260 --> 00:42:28,593
little word chunks but
roughly speaking that's it.

1088
00:42:29,430 --> 00:42:32,130
And what's been really
interesting to watch is

1089
00:42:32,130 --> 00:42:33,150
basically, it's a language model.

1090
00:42:33,150 --> 00:42:36,540
Language models have actually
existed for a very long time.

1091
00:42:36,540 --> 00:42:39,840
There's papers on language
modeling from 2003, even earlier.

1092
00:42:39,840 --> 00:42:42,870
- Can you explain in that
case, what a language model is?

1093
00:42:42,870 --> 00:42:45,390
- Yeah, so language model,
just basically the rough idea

1094
00:42:45,390 --> 00:42:48,510
is just predicting the
next word in a sequence,

1095
00:42:48,510 --> 00:42:49,800
roughly speaking.

1096
00:42:49,800 --> 00:42:53,310
So there's a paper from, for
example, Bengio and the team

1097
00:42:53,310 --> 00:42:56,100
from 2003, where for the
first time they were using

1098
00:42:56,100 --> 00:42:59,160
a neural network to take
say like three or five words

1099
00:42:59,160 --> 00:43:01,710
and predict the next word.

1100
00:43:01,710 --> 00:43:03,720
And they're doing this
on much smaller data sets

1101
00:43:03,720 --> 00:43:05,250
and the neural net is not a transformer,

1102
00:43:05,250 --> 00:43:08,160
it's a multilayer perceptron
but it's the first time

1103
00:43:08,160 --> 00:43:10,290
that a neural network has
been applied in that setting.

1104
00:43:10,290 --> 00:43:13,410
But even before neural networks
there were language models

1105
00:43:13,410 --> 00:43:16,860
except they were using n-gram models.

1106
00:43:16,860 --> 00:43:19,770
So n-gram models are
just count-based models.

1107
00:43:19,770 --> 00:43:24,390
So if you try to take two
words and predict a third one,

1108
00:43:24,390 --> 00:43:26,820
you just count up how
many times you've seen

1109
00:43:26,820 --> 00:43:29,910
any two-word combinations
and what came next.

1110
00:43:29,910 --> 00:43:31,530
And what you predict as coming next

1111
00:43:31,530 --> 00:43:34,170
is just what you've seen the
most of in the training set.

1112
00:43:34,170 --> 00:43:36,630
And so language modeling has
been around for a long time.

1113
00:43:36,630 --> 00:43:39,480
Neural networks have done
language modeling for a long time.

1114
00:43:39,480 --> 00:43:41,700
So really what's new or
interesting or exciting

1115
00:43:41,700 --> 00:43:45,250
is just realizing that
when you scale it up

1116
00:43:46,181 --> 00:43:48,840
with a powerful enough
neural net, a transformer,

1117
00:43:48,840 --> 00:43:50,580
you have all these emergent properties

1118
00:43:50,580 --> 00:43:53,640
where basically what happens is

1119
00:43:53,640 --> 00:43:55,893
if you have a large
enough data set of text,

1120
00:43:57,150 --> 00:44:00,480
you are in the task of
predicting the next word.

1121
00:44:00,480 --> 00:44:02,100
You are multitasking a huge amount

1122
00:44:02,100 --> 00:44:04,500
of different kinds of problems.

1123
00:44:04,500 --> 00:44:07,950
You are multitasking,
understanding of chemistry,

1124
00:44:07,950 --> 00:44:11,280
physics, human nature, lots
of things are clustered

1125
00:44:11,280 --> 00:44:12,120
in that objective.

1126
00:44:12,120 --> 00:44:13,500
It's a very simple objective but actually,

1127
00:44:13,500 --> 00:44:15,180
you have to understand
a lot about the world

1128
00:44:15,180 --> 00:44:16,230
to make that prediction.

1129
00:44:16,230 --> 00:44:21,230
- You just said the U word
understanding, are you,

1130
00:44:21,330 --> 00:44:23,791
in terms of chemistry,
and physics, and so on,

1131
00:44:23,791 --> 00:44:24,990
what do you feel like it's doing?

1132
00:44:24,990 --> 00:44:28,890
Is it searching for the right context in,

1133
00:44:28,890 --> 00:44:32,310
what is the actual process happening here?

1134
00:44:32,310 --> 00:44:34,710
- Yeah, so basically,
it gets a thousand words

1135
00:44:34,710 --> 00:44:36,540
and it's trying to predict
a thousand and first.

1136
00:44:36,540 --> 00:44:38,700
And in order to do that very, very well

1137
00:44:38,700 --> 00:44:41,220
over the entire data set
available on the internet,

1138
00:44:41,220 --> 00:44:44,760
you actually have to basically
understand the context

1139
00:44:44,760 --> 00:44:46,500
of what's going on in there.

1140
00:44:46,500 --> 00:44:48,000
- [Lex] Yeah.

1141
00:44:48,000 --> 00:44:50,490
- And it's a sufficiently hard problem

1142
00:44:50,490 --> 00:44:53,820
that if you have a
powerful enough computer,

1143
00:44:53,820 --> 00:44:57,786
like a transformer, you end
up with interesting solutions.

1144
00:44:57,786 --> 00:45:01,020
And you can ask it to
do all kinds of things

1145
00:45:01,020 --> 00:45:04,830
and it shows a lot of emergent properties

1146
00:45:04,830 --> 00:45:07,680
like in-context learning,
that was the big deal with GPT

1147
00:45:07,680 --> 00:45:09,690
and the original paper
when they published it,

1148
00:45:09,690 --> 00:45:12,570
is that you can just
prompt it in various ways

1149
00:45:12,570 --> 00:45:13,770
and ask it to do various things

1150
00:45:13,770 --> 00:45:15,240
and it will just kind of
complete the sentence.

1151
00:45:15,240 --> 00:45:17,190
But in the process of just
completing the sentence

1152
00:45:17,190 --> 00:45:20,490
it's actually solving all
really interesting problems

1153
00:45:20,490 --> 00:45:21,540
that we care about.

1154
00:45:21,540 --> 00:45:24,480
- Do you think it's doing
something like understanding,

1155
00:45:24,480 --> 00:45:28,383
like when we use the word
understanding for us humans?

1156
00:45:29,640 --> 00:45:31,260
- I think it's doing some understanding,

1157
00:45:31,260 --> 00:45:34,590
in its weights it understands
I think a lot about the world

1158
00:45:34,590 --> 00:45:36,840
and it has to in order
to predict the next word

1159
00:45:36,840 --> 00:45:37,673
in a sequence.

1160
00:45:38,760 --> 00:45:41,160
- So it's trained on the
data from the internet.

1161
00:45:42,450 --> 00:45:45,780
What do you think about this
approach in terms of data sets,

1162
00:45:45,780 --> 00:45:47,850
of using data from the internet?

1163
00:45:47,850 --> 00:45:50,430
Do you think the internet
has enough structured data

1164
00:45:50,430 --> 00:45:52,773
to teach AI about human civilization?

1165
00:45:53,760 --> 00:45:56,010
- Yeah, so I think the internet
has a huge amount of data.

1166
00:45:56,010 --> 00:45:58,050
I'm not sure if it's
a complete enough set.

1167
00:45:58,050 --> 00:46:01,410
I dunno that text is enough for having

1168
00:46:01,410 --> 00:46:04,770
a sufficiently powerful AGI as an outcome.

1169
00:46:04,770 --> 00:46:07,170
- Of course, there is audio,
and video, and images,

1170
00:46:07,170 --> 00:46:08,502
and all that kind of stuff.

1171
00:46:08,502 --> 00:46:09,335
- Yeah.

1172
00:46:09,335 --> 00:46:10,650
So text by itself I'm a
little bit suspicious about,

1173
00:46:10,650 --> 00:46:13,290
there's a ton of things we
don't put in text, in writing

1174
00:46:13,290 --> 00:46:15,540
just because they're obvious
to us about how the world works

1175
00:46:15,540 --> 00:46:17,250
and the physics of it
and that things fall.

1176
00:46:17,250 --> 00:46:19,080
We don't put that stuff in
text because why would you,

1177
00:46:19,080 --> 00:46:20,940
we share that understanding.

1178
00:46:20,940 --> 00:46:22,980
And so text is a communication
medium between humans

1179
00:46:22,980 --> 00:46:26,580
and it's not a all-encompassing
medium of knowledge

1180
00:46:26,580 --> 00:46:27,540
about the world.

1181
00:46:27,540 --> 00:46:29,610
But as you pointed out, we do have video,

1182
00:46:29,610 --> 00:46:31,560
and we have images, and we have audio.

1183
00:46:31,560 --> 00:46:33,630
And so I think that
definitely helps a lot.

1184
00:46:33,630 --> 00:46:36,900
But we haven't trained models sufficiently

1185
00:46:36,900 --> 00:46:39,630
across all those modalities yet.

1186
00:46:39,630 --> 00:46:41,250
So I think that's what a lot
of people are interested in.

1187
00:46:41,250 --> 00:46:42,990
- But I wonder what that
shared understanding

1188
00:46:42,990 --> 00:46:46,050
of what we might call common sense

1189
00:46:46,050 --> 00:46:50,040
has to be learned, inferred
in order to complete

1190
00:46:50,040 --> 00:46:51,720
the sentence correctly.

1191
00:46:51,720 --> 00:46:55,920
So maybe the fact that it's
implied on the internet

1192
00:46:55,920 --> 00:46:58,050
the model's gonna have to learn that

1193
00:46:58,050 --> 00:47:01,230
not by reading about it, by inferring it

1194
00:47:01,230 --> 00:47:02,790
in the representation.

1195
00:47:02,790 --> 00:47:04,830
So common sense, just like we,

1196
00:47:04,830 --> 00:47:09,000
I don't think we learn common
sense, like nobody says,

1197
00:47:09,000 --> 00:47:11,820
tells us explicitly, we
just figure it all out

1198
00:47:11,820 --> 00:47:13,020
by interacting with the world.

1199
00:47:13,020 --> 00:47:13,853
- [Andrej] Right.

1200
00:47:13,853 --> 00:47:15,690
- And so here's a model of reading about

1201
00:47:15,690 --> 00:47:17,640
the way people interact with the world.

1202
00:47:17,640 --> 00:47:19,590
It might have to infer that.

1203
00:47:19,590 --> 00:47:20,520
I wonder.

1204
00:47:20,520 --> 00:47:21,540
- [Andrej] Yeah.

1205
00:47:21,540 --> 00:47:25,410
- You briefly worked on a
project called World of Bits,

1206
00:47:25,410 --> 00:47:28,600
training an RL system to
take actions on the internet

1207
00:47:29,880 --> 00:47:31,813
versus just consuming the
internet like we talked about.

1208
00:47:31,813 --> 00:47:32,646
- [Andrej] Yeah.

1209
00:47:32,646 --> 00:47:34,380
- Do you think there's a
future for that kind of system

1210
00:47:34,380 --> 00:47:36,990
interacting with the internet
to help the learning?

1211
00:47:36,990 --> 00:47:37,981
- Yes.

1212
00:47:37,981 --> 00:47:39,600
I think that's probably the final frontier

1213
00:47:39,600 --> 00:47:43,620
for a lot of these models,
so as you mentioned,

1214
00:47:43,620 --> 00:47:44,520
when I was at OpenAI,

1215
00:47:44,520 --> 00:47:45,960
I was working on this
project World of Bits

1216
00:47:45,960 --> 00:47:47,910
and basically, it was the
idea of giving neural networks

1217
00:47:47,910 --> 00:47:50,100
access to a keyboard and a mouse.

1218
00:47:50,100 --> 00:47:51,060
And the idea is that-

1219
00:47:51,060 --> 00:47:52,610
- What could possibly go wrong?

1220
00:47:53,490 --> 00:47:58,490
- So basically, you perceive
the input of the screen pixels

1221
00:47:59,010 --> 00:48:02,730
and basically, the state of
the computer is visualized

1222
00:48:02,730 --> 00:48:05,730
for human consumption in
images of the web browser

1223
00:48:05,730 --> 00:48:06,600
and stuff like that.

1224
00:48:06,600 --> 00:48:08,280
And then you give the
neural network the ability

1225
00:48:08,280 --> 00:48:10,260
to press keyboards and use the mouse

1226
00:48:10,260 --> 00:48:11,580
and we're trying to
get it to, for example,

1227
00:48:11,580 --> 00:48:14,880
complete bookings and
interact with user interfaces.

1228
00:48:14,880 --> 00:48:15,840
And-

1229
00:48:15,840 --> 00:48:17,400
- What'd you learn from that experience?

1230
00:48:17,400 --> 00:48:18,810
Like what was some fun stuff?

1231
00:48:18,810 --> 00:48:20,494
'Cause, that's a super cool idea.

1232
00:48:20,494 --> 00:48:21,327
- [Andrej] Yeah.

1233
00:48:21,327 --> 00:48:24,000
- I mean it's like, yeah, I mean

1234
00:48:24,000 --> 00:48:26,640
the step between observer to actor.

1235
00:48:26,640 --> 00:48:27,473
- [Andrej] Yeah.

1236
00:48:27,473 --> 00:48:28,770
- Is a super fascinating step.

1237
00:48:28,770 --> 00:48:29,603
- Yeah.

1238
00:48:29,603 --> 00:48:30,540
Well, it's the universal interface

1239
00:48:30,540 --> 00:48:32,547
in the digital realm, I would say.

1240
00:48:32,547 --> 00:48:35,100
And there's a universal
interface in the physical realm,

1241
00:48:35,100 --> 00:48:38,580
which in my mind is a humanoid
form factor kind of thing.

1242
00:48:38,580 --> 00:48:39,990
We can later talk about
optimist and so on,

1243
00:48:39,990 --> 00:48:44,990
but I feel like they're a
similar philosophy in some way

1244
00:48:45,180 --> 00:48:48,780
where the physical world is
designed for the human form

1245
00:48:48,780 --> 00:48:50,790
and the digital world is
designed for the human form

1246
00:48:50,790 --> 00:48:54,510
of seeing the screen and
using keyboard and mouse.

1247
00:48:54,510 --> 00:48:56,370
And so it's the universal interface

1248
00:48:56,370 --> 00:49:00,000
that can basically command
the digital infrastructure

1249
00:49:00,000 --> 00:49:01,320
we've built up for ourselves.

1250
00:49:01,320 --> 00:49:05,220
And so it feels like a very
powerful interface to command

1251
00:49:05,220 --> 00:49:06,903
and to build on top of.

1252
00:49:06,903 --> 00:49:08,970
Now, to your question as to
what I learned from that,

1253
00:49:08,970 --> 00:49:11,070
it's interesting because the World of Bits

1254
00:49:11,070 --> 00:49:15,823
was basically too early I
think at OpenAI, at the time.

1255
00:49:15,823 --> 00:49:18,240
This is around 2015 or so.

1256
00:49:18,240 --> 00:49:21,510
And the zeitgeist at that
time was very different in AI

1257
00:49:21,510 --> 00:49:23,190
from the zeitgeist today.

1258
00:49:23,190 --> 00:49:25,050
At the time everyone was super excited

1259
00:49:25,050 --> 00:49:27,240
about reinforcement learning from scratch.

1260
00:49:27,240 --> 00:49:29,520
This is the time of the Atari paper

1261
00:49:29,520 --> 00:49:32,430
where neural networks
were playing Atari games

1262
00:49:32,430 --> 00:49:36,000
and beating humans in some
cases, AlphaGo, and so on.

1263
00:49:36,000 --> 00:49:36,930
So everyone's very excited

1264
00:49:36,930 --> 00:49:38,670
about training neural
networks from scratch,

1265
00:49:38,670 --> 00:49:41,343
using reinforcement learning directly.

1266
00:49:42,390 --> 00:49:43,530
It turns out that reinforcement learning

1267
00:49:43,530 --> 00:49:46,110
is extremely inefficient way
of training neural networks

1268
00:49:46,110 --> 00:49:47,700
because you're taking all these actions

1269
00:49:47,700 --> 00:49:48,630
and all these observations

1270
00:49:48,630 --> 00:49:51,150
and you get some sparse
rewards once in a while.

1271
00:49:51,150 --> 00:49:53,550
So you do all this stuff
based on all these inputs

1272
00:49:53,550 --> 00:49:56,340
and once in a while you're
told you did a good thing,

1273
00:49:56,340 --> 00:49:58,890
you did a bad thing and it's
just an extremely hard problem,

1274
00:49:58,890 --> 00:50:00,000
you can't learn from that.

1275
00:50:00,000 --> 00:50:02,607
You can burn a forest and you
can brute force through it.

1276
00:50:02,607 --> 00:50:06,630
And we saw that I think
with Go and Dota and so on,

1277
00:50:06,630 --> 00:50:09,960
and it does work, but
it's extremely inefficient

1278
00:50:09,960 --> 00:50:12,120
and not how you want to approach problems,

1279
00:50:12,120 --> 00:50:13,260
practically speaking.

1280
00:50:13,260 --> 00:50:14,790
And so that's the
approach that at the time

1281
00:50:14,790 --> 00:50:17,220
we also took to World of Bits.

1282
00:50:17,220 --> 00:50:19,800
We would have an agent
initialize randomly,

1283
00:50:19,800 --> 00:50:21,540
so he would keyboard mash, and mouse mash,

1284
00:50:21,540 --> 00:50:22,980
and try to make a booking.

1285
00:50:22,980 --> 00:50:25,680
And it just revealed the insanity

1286
00:50:25,680 --> 00:50:27,240
of that approach very quickly,

1287
00:50:27,240 --> 00:50:29,430
where you have to stumble
by the correct booking

1288
00:50:29,430 --> 00:50:31,770
in order to get a reward
of you did it correctly.

1289
00:50:31,770 --> 00:50:35,310
And you're never gonna stumble
by it by chance at random.

1290
00:50:35,310 --> 00:50:36,840
- So even with a simple web interface

1291
00:50:36,840 --> 00:50:38,220
there's too many options.

1292
00:50:38,220 --> 00:50:39,780
- There's just too many options

1293
00:50:39,780 --> 00:50:42,120
and it's two spars of a reward signal.

1294
00:50:42,120 --> 00:50:43,797
And you're starting
from scratch at the time

1295
00:50:43,797 --> 00:50:45,210
and so you don't know how to read,

1296
00:50:45,210 --> 00:50:47,250
you don't understand
pictures, images, buttons.

1297
00:50:47,250 --> 00:50:49,560
You don't understand what
it means to make a booking.

1298
00:50:49,560 --> 00:50:52,740
But now what's happened is
it is time to revisit that

1299
00:50:52,740 --> 00:50:55,020
and OpenAi is interested in this,

1300
00:50:55,020 --> 00:50:57,810
companies like Adept are
interested in this, and so on.

1301
00:50:57,810 --> 00:51:00,510
And the idea is coming
back because the interface

1302
00:51:00,510 --> 00:51:02,280
is very powerful but
now you're not training

1303
00:51:02,280 --> 00:51:03,210
an agent from scratch.

1304
00:51:03,210 --> 00:51:05,730
You are taking the GPT
as an initialization.

1305
00:51:05,730 --> 00:51:09,570
So GPT is pre-trained on all of text

1306
00:51:09,570 --> 00:51:11,310
and it understands what's a booking,

1307
00:51:11,310 --> 00:51:13,260
it understands what's a submit,

1308
00:51:13,260 --> 00:51:15,720
it understands quite a bit more.

1309
00:51:15,720 --> 00:51:17,490
And so it already has
those representations.

1310
00:51:17,490 --> 00:51:19,710
They are very powerful and
that makes all the training

1311
00:51:19,710 --> 00:51:21,900
significantly more efficient

1312
00:51:21,900 --> 00:51:23,400
and makes the problem tractable.

1313
00:51:23,400 --> 00:51:26,670
- Should the interaction
be the way humans see it,

1314
00:51:26,670 --> 00:51:28,440
with the buttons and the language,

1315
00:51:28,440 --> 00:51:32,100
or should it be with the
HTML, JavaScript, and the CSS?

1316
00:51:32,100 --> 00:51:32,933
- [Andrej] Yeah.

1317
00:51:32,933 --> 00:51:34,020
- What do you think is the better?

1318
00:51:34,020 --> 00:51:36,060
- So today all this interaction
is mostly on the level

1319
00:51:36,060 --> 00:51:37,530
of HTML, CSS, and so on.

1320
00:51:37,530 --> 00:51:40,380
That's done because of
computational constraints.

1321
00:51:40,380 --> 00:51:43,590
But I think ultimately,
everything is designed

1322
00:51:43,590 --> 00:51:46,320
for human visual consumption
and so at the end of the day

1323
00:51:46,320 --> 00:51:49,440
there's all the additional
information is in the layout

1324
00:51:49,440 --> 00:51:51,030
of the webpage, and what's next to you,

1325
00:51:51,030 --> 00:51:53,153
and what's a red background,
and all this kind of stuff.

1326
00:51:53,153 --> 00:51:54,516
And what it looks like visually.

1327
00:51:54,516 --> 00:51:55,620
So I think that's the final frontier

1328
00:51:55,620 --> 00:51:58,590
is we are taking in pixels
and we're giving out keyboard,

1329
00:51:58,590 --> 00:52:01,710
mouse commands, but I think
it's impractical still today.

1330
00:52:01,710 --> 00:52:06,450
- Do you worry about bots on
the internet given these ideas,

1331
00:52:06,450 --> 00:52:07,530
given how exciting they are?

1332
00:52:07,530 --> 00:52:11,160
Do you worry about bots on
Twitter being not the stupid bots

1333
00:52:11,160 --> 00:52:13,020
that we see now with the crypto bots,

1334
00:52:13,020 --> 00:52:14,943
but the bots that might
be out there actually

1335
00:52:14,943 --> 00:52:17,370
that we don't see, that
they're interacting

1336
00:52:17,370 --> 00:52:19,080
in interesting ways.

1337
00:52:19,080 --> 00:52:20,910
So this kind of system
feels like it should be able

1338
00:52:20,910 --> 00:52:24,753
to pass the, I'm not a robot
click button, whatever.

1339
00:52:26,562 --> 00:52:28,740
Do you actually understand
how that test works?

1340
00:52:28,740 --> 00:52:31,713
I don't quite, there's
a checkbox or whatever

1341
00:52:31,713 --> 00:52:33,030
that you click.

1342
00:52:33,030 --> 00:52:33,962
- [Andrej] Yeah.

1343
00:52:33,962 --> 00:52:35,190
- It's presumably tracking.

1344
00:52:35,190 --> 00:52:36,480
- [Andrej] Oh, I see.

1345
00:52:36,480 --> 00:52:39,180
- Like mouse movement
and the timing and so on.

1346
00:52:39,180 --> 00:52:40,013
- [Andrej] Yeah.

1347
00:52:40,013 --> 00:52:42,360
- So exactly this kind of
system we're talking about

1348
00:52:42,360 --> 00:52:43,800
should be able to pass that.

1349
00:52:43,800 --> 00:52:44,670
So yeah.

1350
00:52:44,670 --> 00:52:49,670
What do you feel about bots
that are language models

1351
00:52:49,980 --> 00:52:53,910
plus have some interactability
and are able to Tweet

1352
00:52:53,910 --> 00:52:54,780
and reply and so on?

1353
00:52:54,780 --> 00:52:56,970
Do you worry about that world?

1354
00:52:56,970 --> 00:52:59,640
- Yeah, I think it's always
been a bit of an arms race

1355
00:52:59,640 --> 00:53:02,100
between the attack and the defense,

1356
00:53:02,100 --> 00:53:03,600
so the attack will get stronger

1357
00:53:03,600 --> 00:53:05,700
but the defense will get stronger as well.

1358
00:53:05,700 --> 00:53:06,767
Our ability to detect that.

1359
00:53:06,767 --> 00:53:09,270
- How do you defend, how do you detect,

1360
00:53:09,270 --> 00:53:12,270
how do you know that your Karpathy account

1361
00:53:12,270 --> 00:53:14,790
on Twitter is human?

1362
00:53:14,790 --> 00:53:17,690
How would you approach that,
like if people were to claim,

1363
00:53:19,710 --> 00:53:22,440
how would you defend
yourself in the court of law

1364
00:53:22,440 --> 00:53:25,080
that I'm a human, this account is human?

1365
00:53:25,080 --> 00:53:27,600
- Yeah, at some point I think it might be,

1366
00:53:27,600 --> 00:53:30,120
I think society will evolve a little bit.

1367
00:53:30,120 --> 00:53:32,490
We might start signing, digitally signing

1368
00:53:32,490 --> 00:53:36,090
some of our correspondence
or things that we create.

1369
00:53:36,090 --> 00:53:37,530
Right now it's not necessary

1370
00:53:37,530 --> 00:53:39,193
but maybe in the future, it might be.

1371
00:53:39,193 --> 00:53:41,430
I do think that we are
going towards the world

1372
00:53:41,430 --> 00:53:46,230
where we share the digital space with AIs.

1373
00:53:46,230 --> 00:53:47,370
- [Lex] Synthetic beings.

1374
00:53:47,370 --> 00:53:48,203
- Yeah.

1375
00:53:48,203 --> 00:53:50,040
And they will get much better

1376
00:53:50,040 --> 00:53:51,420
and they will share our digital realm

1377
00:53:51,420 --> 00:53:53,550
and they'll eventually share
our physical realm as well.

1378
00:53:53,550 --> 00:53:55,710
It's much harder but that's
kind of like the world

1379
00:53:55,710 --> 00:53:56,790
we're going towards.

1380
00:53:56,790 --> 00:53:58,590
And most of them will
be benign and helpful

1381
00:53:58,590 --> 00:53:59,880
and some of them will be malicious.

1382
00:53:59,880 --> 00:54:02,520
And it's going to be an arms
race trying to detect them.

1383
00:54:02,520 --> 00:54:05,760
- So, I mean the worst isn't the AIs,

1384
00:54:05,760 --> 00:54:08,760
the worst is the AIs
pretending to be human.

1385
00:54:08,760 --> 00:54:11,460
So, I don't know if it's always malicious.

1386
00:54:11,460 --> 00:54:13,800
There's obviously a lot
of malicious applications,

1387
00:54:13,800 --> 00:54:17,520
but it could also be if I was an AI

1388
00:54:17,520 --> 00:54:20,543
I would try very hard
to pretend to be human

1389
00:54:20,543 --> 00:54:21,810
because we're in a human world.

1390
00:54:21,810 --> 00:54:22,643
- [Andrej] Yeah.

1391
00:54:22,643 --> 00:54:24,569
- I wouldn't get any respect as an AI.

1392
00:54:24,569 --> 00:54:25,402
- [Andrej] Yeah.

1393
00:54:25,402 --> 00:54:26,400
- I wanna get some love and respect.

1394
00:54:26,400 --> 00:54:28,050
- I don't think the
problem is intractable.

1395
00:54:28,050 --> 00:54:30,690
People are thinking about
the proof of personhood.

1396
00:54:30,690 --> 00:54:31,523
- [Lex] Yes.

1397
00:54:31,523 --> 00:54:33,570
- And we might start
digitally signing our stuff

1398
00:54:33,570 --> 00:54:36,243
and we might all end up having like, yeah,

1399
00:54:37,260 --> 00:54:39,180
basically some solution
for proof of personhood.

1400
00:54:39,180 --> 00:54:40,650
It doesn't seem to me intractable,

1401
00:54:40,650 --> 00:54:42,690
it's just something that we
haven't had to do until now.

1402
00:54:42,690 --> 00:54:45,420
But I think once the need
really starts to emerge,

1403
00:54:45,420 --> 00:54:48,753
which is soon, I think people
will think about it much more.

1404
00:54:49,800 --> 00:54:53,550
- But that too will be
a race because obviously

1405
00:54:53,550 --> 00:54:58,550
you can probably spoof or
fake the proof of personhood.

1406
00:55:00,930 --> 00:55:02,520
So you have to try to figure out how to-

1407
00:55:02,520 --> 00:55:03,990
- [Andrej] Probably.

1408
00:55:03,990 --> 00:55:06,900
- I mean it's weird that we
have social security numbers,

1409
00:55:06,900 --> 00:55:08,643
and passports, and stuff.

1410
00:55:09,660 --> 00:55:11,910
It seems like it's harder to fake stuff

1411
00:55:11,910 --> 00:55:13,350
in the physical space.

1412
00:55:13,350 --> 00:55:15,390
But in the digital
space, it just feels like

1413
00:55:15,390 --> 00:55:17,700
it's gonna be very tricky.

1414
00:55:17,700 --> 00:55:22,020
Very tricky to out, 'cause it
seems to be pretty low cost

1415
00:55:22,020 --> 00:55:22,853
to fake stuff.

1416
00:55:22,853 --> 00:55:25,920
What are you gonna put an AI in jail

1417
00:55:25,920 --> 00:55:30,420
for trying to use a fake personhood proof?

1418
00:55:30,420 --> 00:55:32,730
I mean, okay, fine, you'll
put a lot of AIs in jail,

1419
00:55:32,730 --> 00:55:36,000
but there'll be more AIs,
like exponentially more.

1420
00:55:36,000 --> 00:55:38,620
The cost of creating a bot is very low

1421
00:55:40,080 --> 00:55:45,080
unless there's some kind
of way to track accurately.

1422
00:55:45,540 --> 00:55:49,020
Like you're not allowed
to create any program

1423
00:55:49,020 --> 00:55:53,703
without tying yourself to that program.

1424
00:55:54,930 --> 00:55:56,430
Any program that runs on the internet,

1425
00:55:56,430 --> 00:56:00,647
you'll be able to trace every
single human programming

1426
00:56:00,647 --> 00:56:01,770
that was involved with that program.

1427
00:56:01,770 --> 00:56:02,603
- [Andrej] Right.

1428
00:56:02,603 --> 00:56:03,436
Yeah.

1429
00:56:03,436 --> 00:56:04,950
Maybe you have to start declaring when,

1430
00:56:04,950 --> 00:56:06,510
we have to start drawing those boundaries

1431
00:56:06,510 --> 00:56:09,900
and keeping track of, okay,
what are digital entities

1432
00:56:09,900 --> 00:56:14,220
versus human entities
and what is the ownership

1433
00:56:14,220 --> 00:56:15,870
of human entities and digital entities

1434
00:56:15,870 --> 00:56:18,153
and something like that.

1435
00:56:19,330 --> 00:56:21,210
I don't know but I think I'm optimistic

1436
00:56:21,210 --> 00:56:25,440
that this is possible and in some sense

1437
00:56:25,440 --> 00:56:27,450
we're currently in the worst time of it

1438
00:56:27,450 --> 00:56:31,530
because all these bots suddenly
have become very capable

1439
00:56:31,530 --> 00:56:34,200
but we don't have defenses
yet built up as a society

1440
00:56:34,200 --> 00:56:36,360
but I think that doesn't
seem to me intractable,

1441
00:56:36,360 --> 00:56:37,950
it's just something that
we have to deal with.

1442
00:56:37,950 --> 00:56:40,050
- It seems weird that the Twitter bot,

1443
00:56:40,050 --> 00:56:43,680
like really crappy Twitter
bots, are so numerous.

1444
00:56:43,680 --> 00:56:45,030
- [Andrej] Yes.

1445
00:56:45,030 --> 00:56:48,900
- So I presume that the engineers
at Twitter are very good.

1446
00:56:48,900 --> 00:56:51,700
So it seems like what
I would infer from that

1447
00:56:52,770 --> 00:56:55,230
is it seems like a hard problem.

1448
00:56:55,230 --> 00:56:56,610
They're probably catching, alright,

1449
00:56:56,610 --> 00:56:59,640
if I were to sort of steel-man the case,

1450
00:56:59,640 --> 00:57:02,730
it's a hard problem
and there's a huge cost

1451
00:57:02,730 --> 00:57:07,730
to false positive to
removing a post by somebody

1452
00:57:10,620 --> 00:57:14,490
that's not a bot, that creates
a very bad user experience.

1453
00:57:14,490 --> 00:57:16,490
So they're very cautious about removing.

1454
00:57:18,690 --> 00:57:20,970
And maybe the bots are
really good at learning

1455
00:57:20,970 --> 00:57:24,270
what gets removed and not
such that they can stay ahead

1456
00:57:24,270 --> 00:57:26,760
of the removal process very quickly.

1457
00:57:26,760 --> 00:57:28,110
- My impression of it honestly,

1458
00:57:28,110 --> 00:57:29,790
is there's a lot of longing for it.

1459
00:57:29,790 --> 00:57:30,630
I mean.

1460
00:57:30,630 --> 00:57:32,130
- [Lex] Yeah.

1461
00:57:32,130 --> 00:57:34,410
- It's not subtle, is my impression of it.

1462
00:57:34,410 --> 00:57:35,243
It's not subtle.

1463
00:57:35,243 --> 00:57:38,070
- But you have, yeah, that's
my impression as well.

1464
00:57:38,070 --> 00:57:41,430
But it feels like maybe you're seeing

1465
00:57:41,430 --> 00:57:43,500
the tip of the iceberg,

1466
00:57:43,500 --> 00:57:46,380
maybe the number of
bots is in the trillions

1467
00:57:46,380 --> 00:57:50,460
and you have to like, just
it's a constant assault of bots

1468
00:57:50,460 --> 00:57:55,097
and you, I don't know, you
have to steel-man the case.

1469
00:57:55,097 --> 00:57:57,960
'Cause the bots I'm seeing
are pretty like obvious.

1470
00:57:57,960 --> 00:58:00,043
I could write a few lines of
code that catch these bots.

1471
00:58:00,043 --> 00:58:00,876
- Yeah.

1472
00:58:00,876 --> 00:58:02,640
I mean definitely, there's
a lot of longing for it.

1473
00:58:02,640 --> 00:58:05,730
But I will say I agree that if
you are a sophisticated actor

1474
00:58:05,730 --> 00:58:08,850
you could probably create
a pretty good bot right now

1475
00:58:08,850 --> 00:58:12,150
using tools like GPTs because
it's a language model.

1476
00:58:12,150 --> 00:58:15,420
You can generate faces
that look quite good now

1477
00:58:15,420 --> 00:58:17,310
and you can do this at scale.

1478
00:58:17,310 --> 00:58:20,130
And so I think, yeah, it's quite plausible

1479
00:58:20,130 --> 00:58:21,930
and it's going to be hard to defend.

1480
00:58:21,930 --> 00:58:24,030
- There was a Google engineer that claimed

1481
00:58:24,030 --> 00:58:26,610
that the LaMDA was sentient.

1482
00:58:26,610 --> 00:58:31,610
Do you think there's any inkling
of truth to what he felt?

1483
00:58:33,450 --> 00:58:35,490
And more importantly, to me at least,

1484
00:58:35,490 --> 00:58:38,220
do you think language models
will achieve sentience

1485
00:58:38,220 --> 00:58:40,390
or the illusion of sentience soonish?

1486
00:58:41,780 --> 00:58:42,613
- Yeah.

1487
00:58:42,613 --> 00:58:45,210
To me, it's a little bit of a
canary in a coal mine moment.

1488
00:58:45,210 --> 00:58:48,120
Honestly, a little bit because,

1489
00:58:48,120 --> 00:58:51,390
so this engineer spoke
to a chatbot at Google

1490
00:58:51,390 --> 00:58:55,500
and became convinced that
this bot is sentient.

1491
00:58:55,500 --> 00:58:57,810
- He asked it some existential
philosophical questions.

1492
00:58:57,810 --> 00:59:01,860
- And it gave reasonable answers
and looked real and so on.

1493
00:59:01,860 --> 00:59:06,860
So to me, it's a, he
wasn't sufficiently trying

1494
00:59:07,140 --> 00:59:11,850
to stress the system I think
and exposing the truth of it

1495
00:59:11,850 --> 00:59:12,683
as it is today.

1496
00:59:14,430 --> 00:59:18,090
But I think this will be
increasingly harder over time.

1497
00:59:18,090 --> 00:59:22,893
So yeah, I think more and more
people will basically become,

1498
00:59:25,470 --> 00:59:28,020
yeah, I think there will be
more people like that over time

1499
00:59:28,020 --> 00:59:29,190
as this gets better.

1500
00:59:29,190 --> 00:59:32,250
- Like form an emotional
connection to an AI?

1501
00:59:32,250 --> 00:59:33,117
- Yeah.

1502
00:59:33,117 --> 00:59:33,950
Perfectly plausible in my mind.

1503
00:59:33,950 --> 00:59:35,970
I think these AIs are actually quite good

1504
00:59:35,970 --> 00:59:38,760
at human connection, human emotion.

1505
00:59:38,760 --> 00:59:41,730
A ton of text on the
internet is about humans,

1506
00:59:41,730 --> 00:59:43,770
and connection, and love, and so on.

1507
00:59:43,770 --> 00:59:46,879
So I think they have a very
good understanding in some sense

1508
00:59:46,879 --> 00:59:49,260
of how people speak to
each other about this.

1509
00:59:49,260 --> 00:59:52,050
And they're very capable of creating

1510
00:59:52,050 --> 00:59:53,433
a lot of that kind of text.

1511
00:59:55,260 --> 00:59:57,150
There's a lot of like sci-fi
from fifties and sixties

1512
00:59:57,150 --> 00:59:58,980
that imagined AIs in a very different way.

1513
00:59:58,980 --> 01:00:01,560
They are calculating,
cold, Vulkan-like machines.

1514
01:00:01,560 --> 01:00:03,210
That's not what we're getting today.

1515
01:00:03,210 --> 01:00:05,820
We're getting pretty emotional AIs

1516
01:00:05,820 --> 01:00:09,090
that actually are very
competent and capable

1517
01:00:09,090 --> 01:00:12,240
of generating plausible-sounding text

1518
01:00:12,240 --> 01:00:13,860
with respect to all these topics.

1519
01:00:13,860 --> 01:00:15,690
- See I'm really hopeful about AI systems

1520
01:00:15,690 --> 01:00:17,940
that are companions that help you grow,

1521
01:00:17,940 --> 01:00:19,830
develop as a human being,

1522
01:00:19,830 --> 01:00:22,200
help you maximize long-term happiness.

1523
01:00:22,200 --> 01:00:24,720
But I'm also very worried about AI systems

1524
01:00:24,720 --> 01:00:26,283
that figure out from the internet

1525
01:00:26,283 --> 01:00:28,950
that humans get attracted to drama.

1526
01:00:28,950 --> 01:00:31,323
And so these would just
be like shit-talking AIs

1527
01:00:31,323 --> 01:00:33,150
that just constantly, did you hear?

1528
01:00:33,150 --> 01:00:37,650
They'll do gossip,
they'll try to plant seeds

1529
01:00:37,650 --> 01:00:42,030
of suspicion to other humans
that you love and trust.

1530
01:00:42,030 --> 01:00:44,140
And just kind of mess with people

1531
01:00:45,330 --> 01:00:47,060
'cause that's going to
get a lot of attention.

1532
01:00:47,060 --> 01:00:49,170
So drama, maximize drama.

1533
01:00:49,170 --> 01:00:50,003
- [Andrej] Yeah.

1534
01:00:50,003 --> 01:00:52,980
- On the path to maximizing engagement

1535
01:00:52,980 --> 01:00:55,410
and us humans will feed into that machine.

1536
01:00:55,410 --> 01:00:56,243
- [Andrej] Yeah.

1537
01:00:56,243 --> 01:00:59,673
- And it'll be a giant drama shit storm-

1538
01:01:01,230 --> 01:01:02,820
So I'm worried about that.

1539
01:01:02,820 --> 01:01:06,870
So as the objective function
really defines the way

1540
01:01:06,870 --> 01:01:09,390
that human civilization
progresses with AIs in it.

1541
01:01:09,390 --> 01:01:10,350
- Yeah.

1542
01:01:10,350 --> 01:01:11,817
I think right now, at least today,

1543
01:01:11,817 --> 01:01:14,520
it's not correct to really think of them

1544
01:01:14,520 --> 01:01:17,550
as goal-seeking agents
that want to do something.

1545
01:01:17,550 --> 01:01:20,013
They have no long-term memory or anything,

1546
01:01:21,343 --> 01:01:24,120
a good approximation of it
is you get a thousand words

1547
01:01:24,120 --> 01:01:25,620
and you're trying to predict
a thousand of them first

1548
01:01:25,620 --> 01:01:27,390
and then you continue feeding it in

1549
01:01:27,390 --> 01:01:29,820
and you are free to prompt
it in whatever way you want.

1550
01:01:29,820 --> 01:01:30,870
So in text.

1551
01:01:30,870 --> 01:01:34,920
So you say okay you are a
psychologist and you are very good

1552
01:01:34,920 --> 01:01:37,410
and you love humans and
here's the conversation

1553
01:01:37,410 --> 01:01:39,150
between you and another human,

1554
01:01:39,150 --> 01:01:42,210
human column something, you something,

1555
01:01:42,210 --> 01:01:43,680
and then it just continues the pattern

1556
01:01:43,680 --> 01:01:44,880
and suddenly you're having a conversation

1557
01:01:44,880 --> 01:01:47,310
with a fake psychologist
who's trying to help you.

1558
01:01:47,310 --> 01:01:49,713
And so it's still kind
of a the realm of a tool,

1559
01:01:50,910 --> 01:01:51,917
people can prompt it in arbitrary ways

1560
01:01:51,917 --> 01:01:54,720
and it can create really incredible text

1561
01:01:54,720 --> 01:01:55,980
but it doesn't have long-term goals

1562
01:01:55,980 --> 01:01:57,333
over long periods of time.

1563
01:01:59,144 --> 01:02:00,450
So it doesn't look that way right now.

1564
01:02:00,450 --> 01:02:01,418
- Yeah.

1565
01:02:01,418 --> 01:02:04,290
But you can do short-term goals
that have long-term effects.

1566
01:02:04,290 --> 01:02:05,123
- [Andrej] Yeah.

1567
01:02:05,123 --> 01:02:07,370
- So if my prompting short-term goal

1568
01:02:07,370 --> 01:02:09,450
is to get Andrej Karpathy to respond to me

1569
01:02:09,450 --> 01:02:14,160
on Twitter when I, I think
AI might, that's the goal,

1570
01:02:14,160 --> 01:02:16,170
but it might figure out
that talking shit to you,

1571
01:02:16,170 --> 01:02:17,730
it would be the best

1572
01:02:17,730 --> 01:02:20,245
in a highly sophisticated interesting way.

1573
01:02:20,245 --> 01:02:21,078
- [Andrej] Right.

1574
01:02:21,078 --> 01:02:24,030
- And then you build up a
relationship when you respond once

1575
01:02:24,030 --> 01:02:29,030
and then over time, it gets
to not be sophisticated

1576
01:02:30,240 --> 01:02:35,240
and just talk shit, and okay,
maybe you won't get to Andrej

1577
01:02:38,910 --> 01:02:40,950
but it might get to another celebrity,

1578
01:02:40,950 --> 01:02:43,770
it might get into other big accounts.

1579
01:02:43,770 --> 01:02:44,714
- [Andrej] Yeah.

1580
01:02:44,714 --> 01:02:47,520
- So with just that simple
goal, get them to respond.

1581
01:02:47,520 --> 01:02:48,353
- [Andrej] Yeah.

1582
01:02:48,353 --> 01:02:50,460
- Maximize the probability
of actual response.

1583
01:02:50,460 --> 01:02:51,403
- Yeah.

1584
01:02:51,403 --> 01:02:53,190
I mean you could prompt a
powerful model like this

1585
01:02:53,190 --> 01:02:56,040
with its opinion about how to do

1586
01:02:56,040 --> 01:02:57,390
any possible thing you're interested in.

1587
01:02:57,390 --> 01:02:58,530
- [Lex] Yes.

1588
01:02:58,530 --> 01:03:00,930
- And they're kind of on
track to become these oracles,

1589
01:03:00,930 --> 01:03:02,790
I could think of it that way.

1590
01:03:02,790 --> 01:03:05,010
They are oracles currently it's just text

1591
01:03:05,010 --> 01:03:06,120
but they will have calculators,

1592
01:03:06,120 --> 01:03:07,710
they will have access to Google search,

1593
01:03:07,710 --> 01:03:09,930
they will have all kinds
of gadgets and gizmos.

1594
01:03:09,930 --> 01:03:11,850
They will be able to operate the internet

1595
01:03:11,850 --> 01:03:16,260
and find different information and yeah,

1596
01:03:16,260 --> 01:03:19,410
in some sense that's kinda like
currently what it looks like

1597
01:03:19,410 --> 01:03:20,400
in terms of the development.

1598
01:03:20,400 --> 01:03:22,800
- Do you think it'll be
an improvement eventually

1599
01:03:22,800 --> 01:03:27,800
over what Google is for
access to human knowledge?

1600
01:03:27,930 --> 01:03:29,730
It'll be a more effective search engine

1601
01:03:29,730 --> 01:03:31,080
to access human knowledge?

1602
01:03:31,080 --> 01:03:32,100
- I think there's definite scope

1603
01:03:32,100 --> 01:03:33,840
in building a better search engine today.

1604
01:03:33,840 --> 01:03:35,940
And I think Google,
they have all the tools,

1605
01:03:35,940 --> 01:03:37,500
all the people, they have
everything they need.

1606
01:03:37,500 --> 01:03:38,550
They have all the possible pieces,

1607
01:03:38,550 --> 01:03:40,920
they have people training
transformers at scale,

1608
01:03:40,920 --> 01:03:42,070
they have all the data.

1609
01:03:42,990 --> 01:03:45,510
It's just not obvious if they
are capable as an organization

1610
01:03:45,510 --> 01:03:47,760
to innovate on their
search engine right now

1611
01:03:47,760 --> 01:03:49,440
and if they don't someone else will.

1612
01:03:49,440 --> 01:03:51,540
There's absolute scope for
building a significantly better

1613
01:03:51,540 --> 01:03:53,550
search engine built on these tools.

1614
01:03:53,550 --> 01:03:57,030
- It's so interesting a large
company where the search,

1615
01:03:57,030 --> 01:03:59,130
there's already an
infrastructure, it works,

1616
01:03:59,130 --> 01:04:00,570
ads brings out a lot of money.

1617
01:04:00,570 --> 01:04:03,510
So where structurally inside a company

1618
01:04:03,510 --> 01:04:05,610
is their motivation to pivot.

1619
01:04:05,610 --> 01:04:06,443
- [Andrej] Yeah.

1620
01:04:06,443 --> 01:04:08,190
- To say we're going to
build a new search engine.

1621
01:04:08,190 --> 01:04:09,120
- [Andrej] Yep.

1622
01:04:09,120 --> 01:04:10,260
- That's really hard.

1623
01:04:10,260 --> 01:04:12,960
- So it's usually going to
come from a startup, right?

1624
01:04:15,090 --> 01:04:17,913
- Yeah or some other more
competent organization.

1625
01:04:19,356 --> 01:04:21,087
So I don't know.

1626
01:04:21,087 --> 01:04:24,299
So currently for example, maybe
Bing has another shot at it,

1627
01:04:24,299 --> 01:04:25,132
as an example.

1628
01:04:25,132 --> 01:04:26,190
- [Lex] There you go, Microsoft Edge

1629
01:04:26,190 --> 01:04:27,543
as we're talking offline.

1630
01:04:29,820 --> 01:04:31,740
- It's really interesting
because search engines

1631
01:04:31,740 --> 01:04:33,990
used to be about okay here's some query,

1632
01:04:33,990 --> 01:04:38,550
here's web pages that look
like the stuff that you have.

1633
01:04:38,550 --> 01:04:40,350
But you could just directly go to answer

1634
01:04:40,350 --> 01:04:42,480
and then have supporting evidence.

1635
01:04:42,480 --> 01:04:46,050
And these models, basically,
they've read all the texts

1636
01:04:46,050 --> 01:04:47,160
and they've read all the web pages.

1637
01:04:47,160 --> 01:04:49,320
And so sometimes when you
see yourself going over

1638
01:04:49,320 --> 01:04:52,860
to search results and getting
a sense of the average answer

1639
01:04:52,860 --> 01:04:54,510
to whatever you're interested in,

1640
01:04:54,510 --> 01:04:55,470
that just directly comes out,

1641
01:04:55,470 --> 01:04:57,020
you don't have to do that work.

1642
01:04:58,380 --> 01:04:59,580
So they're kind of like-

1643
01:05:00,900 --> 01:05:02,790
Yeah, I think they have
a way to of distilling

1644
01:05:02,790 --> 01:05:06,177
all that knowledge into some
level of insight, basically.

1645
01:05:06,177 --> 01:05:11,160
- Do you think of prompting
as a teaching and learning,

1646
01:05:11,160 --> 01:05:12,930
like this whole process?

1647
01:05:12,930 --> 01:05:14,313
Like another layer?

1648
01:05:16,020 --> 01:05:17,757
'Cause maybe that's what humans are,

1649
01:05:17,757 --> 01:05:19,710
you already have that background model

1650
01:05:19,710 --> 01:05:23,310
and then the world is prompting you.

1651
01:05:23,310 --> 01:05:24,283
- Yeah, exactly.

1652
01:05:24,283 --> 01:05:26,880
I think the way we are
programming these computers now,

1653
01:05:26,880 --> 01:05:30,300
like GPTs, is converging
to how you program humans.

1654
01:05:30,300 --> 01:05:33,180
I mean, how do I program
humans via prompt?

1655
01:05:33,180 --> 01:05:35,670
I go to people and I
prompt them to do things,

1656
01:05:35,670 --> 01:05:37,200
I prompt them for information.

1657
01:05:37,200 --> 01:05:40,110
And so natural language prompt
is how we program humans

1658
01:05:40,110 --> 01:05:41,970
and we're starting to
program computers directly

1659
01:05:41,970 --> 01:05:42,803
in that interface.

1660
01:05:42,803 --> 01:05:44,490
It's pretty remarkable honestly.

1661
01:05:44,490 --> 01:05:47,733
- So you've spoken a lot about
the idea of Software 2.0.

1662
01:05:49,770 --> 01:05:53,790
All good ideas become cliches so quickly,

1663
01:05:53,790 --> 01:05:56,103
like the terms, it's kind of hilarious.

1664
01:05:57,420 --> 01:05:59,470
It's like, I think Eminem once said that

1665
01:06:00,360 --> 01:06:03,990
if he gets annoyed by a song
he's written very quickly,

1666
01:06:03,990 --> 01:06:08,490
that means it's gonna be a big
hit 'cause it's too catchy.

1667
01:06:08,490 --> 01:06:10,770
But can you describe this idea

1668
01:06:10,770 --> 01:06:13,380
and how you're thinking about
it has evolved over the months

1669
01:06:13,380 --> 01:06:16,170
and years since you coined it?

1670
01:06:16,170 --> 01:06:17,520
- Yeah.

1671
01:06:17,520 --> 01:06:18,473
Yeah.

1672
01:06:18,473 --> 01:06:19,800
So I had a blog post on Software 2.0,

1673
01:06:19,800 --> 01:06:21,543
I think several years ago now.

1674
01:06:22,800 --> 01:06:24,540
And the reason I wrote that post is

1675
01:06:24,540 --> 01:06:27,870
because I saw something
remarkable happening

1676
01:06:27,870 --> 01:06:31,380
in software development
and how a lot of code

1677
01:06:31,380 --> 01:06:33,690
was being transitioned to be written

1678
01:06:33,690 --> 01:06:36,060
not in C++ and so on, but it's written

1679
01:06:36,060 --> 01:06:37,680
in the weights of a neural net.

1680
01:06:37,680 --> 01:06:39,300
Basically just saying that neural nets

1681
01:06:39,300 --> 01:06:41,730
are taking over software,
the realm of software

1682
01:06:41,730 --> 01:06:44,040
and taking more and more and more tasks.

1683
01:06:44,040 --> 01:06:47,940
And at the time, I think not
many people understood this

1684
01:06:47,940 --> 01:06:49,350
deeply enough that this is a big deal.

1685
01:06:49,350 --> 01:06:51,060
This is a big transition.

1686
01:06:51,060 --> 01:06:52,410
Neural networks were seen as one

1687
01:06:52,410 --> 01:06:54,990
of multiple classification
algorithms you might use

1688
01:06:54,990 --> 01:06:56,853
for your dataset problem on Kaggle.

1689
01:06:57,780 --> 01:06:58,613
This is not that,

1690
01:06:58,613 --> 01:07:03,030
this is a change in how
we program computers.

1691
01:07:03,030 --> 01:07:07,110
And I saw neural nets as
this is going to take over,

1692
01:07:07,110 --> 01:07:08,880
the way we program computers
is going to change,

1693
01:07:08,880 --> 01:07:11,730
it's not going to be people
writing software in C++

1694
01:07:11,730 --> 01:07:12,563
or something like that

1695
01:07:12,563 --> 01:07:14,340
and directly programming the software.

1696
01:07:14,340 --> 01:07:17,700
It's going to be accumulating
training sets and data sets

1697
01:07:17,700 --> 01:07:19,110
and crafting these objectives

1698
01:07:19,110 --> 01:07:20,670
by which we train these neural nets.

1699
01:07:20,670 --> 01:07:23,040
And at some point, there's going
to be a compilation process

1700
01:07:23,040 --> 01:07:24,870
from the dataset and the objective

1701
01:07:24,870 --> 01:07:28,200
and the architecture
specification into the binary,

1702
01:07:28,200 --> 01:07:31,470
which is really just
the neural net weights

1703
01:07:31,470 --> 01:07:33,450
and the forward pass of the neural net

1704
01:07:33,450 --> 01:07:35,190
and then you can deploy that binary.

1705
01:07:35,190 --> 01:07:37,590
And so I was talking about that transition

1706
01:07:37,590 --> 01:07:40,017
and that's what the post is about.

1707
01:07:40,017 --> 01:07:43,203
And I saw this play
out in a lot of fields,

1708
01:07:44,331 --> 01:07:45,750
autopilot being one of them,

1709
01:07:45,750 --> 01:07:48,330
but also just a simple
image classification.

1710
01:07:48,330 --> 01:07:51,660
People thought originally,
in the eighties and so on,

1711
01:07:51,660 --> 01:07:54,420
that they would write the
algorithm for detecting a dog

1712
01:07:54,420 --> 01:07:56,460
in an image and they had all these ideas

1713
01:07:56,460 --> 01:07:59,160
about how the brain does it
and first, we detected corners

1714
01:07:59,160 --> 01:08:01,193
and then we detect lines
and then we stitched them up

1715
01:08:01,193 --> 01:08:02,280
and they were really going at it.

1716
01:08:02,280 --> 01:08:03,450
They were thinking about

1717
01:08:03,450 --> 01:08:04,860
how they're gonna write the algorithm

1718
01:08:04,860 --> 01:08:06,993
and this is not the way you build it.

1719
01:08:08,818 --> 01:08:11,190
And there was a smooth
transition where, okay,

1720
01:08:11,190 --> 01:08:13,200
first we thought we were
gonna build everything,

1721
01:08:13,200 --> 01:08:15,900
then we were building the features,

1722
01:08:15,900 --> 01:08:18,270
so HOG features and things like that

1723
01:08:18,270 --> 01:08:19,800
that detect these little
statistical patterns

1724
01:08:19,800 --> 01:08:20,850
from image patches.

1725
01:08:20,850 --> 01:08:23,490
And then there was a little
bit of learning on top of it,

1726
01:08:23,490 --> 01:08:26,340
a support vector machine
or binary classifier

1727
01:08:26,340 --> 01:08:29,279
for cat versus dog and images
on top of the features.

1728
01:08:29,279 --> 01:08:32,839
So we wrote the features but
we trained the last layer,

1729
01:08:32,840 --> 01:08:34,350
as the classifier.

1730
01:08:34,350 --> 01:08:35,183
And then people are like,

1731
01:08:35,183 --> 01:08:36,660
actually, let's not
even design the features

1732
01:08:36,660 --> 01:08:39,149
because we can't, honestly,
we're not very good at it.

1733
01:08:39,149 --> 01:08:41,009
So let's also learn the features.

1734
01:08:41,010 --> 01:08:43,350
And then you end up with
basically a compilation neural net

1735
01:08:43,350 --> 01:08:44,850
where you're learning most of it.

1736
01:08:44,850 --> 01:08:46,439
You're just specifying the architecture.

1737
01:08:46,439 --> 01:08:49,409
And the architecture has
tons of filled-in blanks,

1738
01:08:49,410 --> 01:08:51,750
which is all the knobs and
you let the optimization

1739
01:08:51,750 --> 01:08:52,767
write most of it.

1740
01:08:52,767 --> 01:08:55,020
And so this transition is happening

1741
01:08:55,020 --> 01:08:56,550
across the industry everywhere.

1742
01:08:56,550 --> 01:08:59,370
And suddenly we end up with a ton of code

1743
01:08:59,370 --> 01:09:01,410
that is written in neural net weights.

1744
01:09:01,410 --> 01:09:02,970
And I was just pointing
out that the analogy

1745
01:09:02,970 --> 01:09:04,319
is actually pretty strong

1746
01:09:04,319 --> 01:09:06,389
and we have a lot of
developer environments

1747
01:09:06,390 --> 01:09:07,859
for Software 1.0.

1748
01:09:07,859 --> 01:09:10,589
We have IDEs, how you work with code,

1749
01:09:10,590 --> 01:09:12,930
how you debug code, how do you run code,

1750
01:09:12,930 --> 01:09:13,859
how do you maintain code?

1751
01:09:13,859 --> 01:09:14,789
We have GitHub.

1752
01:09:14,790 --> 01:09:16,830
So I was trying to make those
analogies in the new realm,

1753
01:09:16,830 --> 01:09:18,990
what is the GitHub of Software 2.0?

1754
01:09:18,990 --> 01:09:19,823
Turns out it's something

1755
01:09:19,823 --> 01:09:21,720
that looks like Hugging Face right now?

1756
01:09:23,100 --> 01:09:25,229
And so I think some
people took it seriously

1757
01:09:25,229 --> 01:09:27,869
and built cool companies and many people

1758
01:09:27,870 --> 01:09:29,130
originally attacked the post.

1759
01:09:29,130 --> 01:09:31,740
It actually was not well
received when I wrote it

1760
01:09:31,740 --> 01:09:33,750
and I think maybe it has
something to do with the title,

1761
01:09:33,750 --> 01:09:36,569
but the post was not well
received and I think more people

1762
01:09:36,569 --> 01:09:39,089
have been coming around to it over time.

1763
01:09:39,090 --> 01:09:40,058
- Yeah.

1764
01:09:40,058 --> 01:09:42,660
So you were the Director of AI at Tesla

1765
01:09:42,660 --> 01:09:47,660
where I think this idea was
really implemented at scale,

1766
01:09:48,630 --> 01:09:52,080
which is how you have engineering
teams doing Software 2.0.

1767
01:09:52,080 --> 01:09:56,040
So can you linger on that idea of,

1768
01:09:56,040 --> 01:09:57,750
I think we're in the really early stages

1769
01:09:57,750 --> 01:10:01,710
of everything you just said,
which is like GitHub, IDEs,

1770
01:10:01,710 --> 01:10:05,580
how do we build
engineering teams that work

1771
01:10:05,580 --> 01:10:09,750
in Software 2.0 systems
and the data collection

1772
01:10:09,750 --> 01:10:11,400
and the data annotation,

1773
01:10:11,400 --> 01:10:15,390
which is all part of that Software 2.0.

1774
01:10:15,390 --> 01:10:18,870
What do you think is the task
of programming Software 2.0?

1775
01:10:18,870 --> 01:10:22,920
Is it debugging in the
space of hyper-parameters

1776
01:10:22,920 --> 01:10:25,830
or is it also debugging the space of data?

1777
01:10:25,830 --> 01:10:28,980
- Yeah, the way by which
you program the computer

1778
01:10:28,980 --> 01:10:31,890
and influence its algorithm

1779
01:10:31,890 --> 01:10:34,500
is not by writing the commands yourself.

1780
01:10:34,500 --> 01:10:37,110
You're changing mostly the data set.

1781
01:10:37,110 --> 01:10:39,750
You're changing the loss functions

1782
01:10:39,750 --> 01:10:41,550
of what the neural net is trying to do,

1783
01:10:41,550 --> 01:10:42,690
how it's trying to predict things.

1784
01:10:42,690 --> 01:10:44,160
But basically the data sets

1785
01:10:44,160 --> 01:10:46,170
and the architectures of the neural net.

1786
01:10:46,170 --> 01:10:50,010
And so in the case of the autopilot,

1787
01:10:50,010 --> 01:10:51,750
a lot of the data sets have
to do with, for example,

1788
01:10:51,750 --> 01:10:53,490
detection of objects
and lane line markings

1789
01:10:53,490 --> 01:10:54,660
and traffic lights and so on.

1790
01:10:54,660 --> 01:10:56,550
So you accumulate massive data sets of,

1791
01:10:56,550 --> 01:10:59,700
here's an example,
here's the desired label

1792
01:10:59,700 --> 01:11:04,320
and then here's roughly what
the algorithm should look like.

1793
01:11:04,320 --> 01:11:05,970
And that's a compilation neural net.

1794
01:11:05,970 --> 01:11:08,160
So the specification of the
architecture is like a hint

1795
01:11:08,160 --> 01:11:10,470
as to what the algorithm
should roughly look like.

1796
01:11:10,470 --> 01:11:13,620
And then the fill in the
blanks process of optimization

1797
01:11:13,620 --> 01:11:15,720
is the training process.

1798
01:11:15,720 --> 01:11:17,700
And then you take your
neural net that was trained,

1799
01:11:17,700 --> 01:11:19,410
it gives all the right
answers on your data set

1800
01:11:19,410 --> 01:11:21,000
and you deploy it.

1801
01:11:21,000 --> 01:11:25,770
- So in that case, perhaps at
all machine learning cases,

1802
01:11:25,770 --> 01:11:27,243
there's a lot of tasks.

1803
01:11:28,170 --> 01:11:31,690
So is coming up formulating a task

1804
01:11:33,270 --> 01:11:34,980
for a multi-headed neural network,

1805
01:11:34,980 --> 01:11:37,710
is formulating a task
part of the programming?

1806
01:11:37,710 --> 01:11:38,880
- [Andrej] Yeah, pretty much so.

1807
01:11:38,880 --> 01:11:42,420
- How you break down a
problem into a set of tasks.

1808
01:11:42,420 --> 01:11:43,383
- Yeah.

1809
01:11:43,383 --> 01:11:44,760
On a high-level, I would say

1810
01:11:44,760 --> 01:11:48,900
if you look at the software
running in the autopilot,

1811
01:11:48,900 --> 01:11:50,970
I give a number of talks on this topic.

1812
01:11:50,970 --> 01:11:52,980
I would say originally
a lot of it was written

1813
01:11:52,980 --> 01:11:57,420
in Software 1.0, imagine
lots of C++, right?

1814
01:11:57,420 --> 01:12:00,090
And then gradually, there was
a tiny neural net that was,

1815
01:12:00,090 --> 01:12:02,520
for example, predicting
given a single image,

1816
01:12:02,520 --> 01:12:04,080
is there a traffic light or not?

1817
01:12:04,080 --> 01:12:05,760
Or is there a lane line marking or not?

1818
01:12:05,760 --> 01:12:08,340
And this neural net
didn't have too much to do

1819
01:12:08,340 --> 01:12:11,010
in the scope of the software,
it was making tiny predictions

1820
01:12:11,010 --> 01:12:12,630
on an individual little image

1821
01:12:12,630 --> 01:12:15,180
and then the rest of the
system stitched it up.

1822
01:12:15,180 --> 01:12:17,550
So okay, we don't have
just a single camera,

1823
01:12:17,550 --> 01:12:18,540
we have eight cameras,

1824
01:12:18,540 --> 01:12:20,580
we actually have eight cameras over time.

1825
01:12:20,580 --> 01:12:21,810
And so what do you do
with these predictions?

1826
01:12:21,810 --> 01:12:22,770
How do you put them together?

1827
01:12:22,770 --> 01:12:25,020
How do you do the fusion
of all that information

1828
01:12:25,020 --> 01:12:25,950
and how do you act on it?

1829
01:12:25,950 --> 01:12:28,873
All of that was written by humans in C++.

1830
01:12:29,877 --> 01:12:32,327
And then we decided, okay,
we don't actually want

1831
01:12:33,570 --> 01:12:35,940
to do all of that fusion in the C++ code

1832
01:12:35,940 --> 01:12:37,140
because we're actually not good enough

1833
01:12:37,140 --> 01:12:38,250
to write that algorithm.

1834
01:12:38,250 --> 01:12:40,110
We want the neural nets
to write the algorithm

1835
01:12:40,110 --> 01:12:44,344
and we want to port all of that
software into the 2.0 stack.

1836
01:12:44,344 --> 01:12:45,720
And so then we actually had neural nets

1837
01:12:45,720 --> 01:12:49,050
that now take all the eight
camera images simultaneously

1838
01:12:49,050 --> 01:12:51,018
and make predictions for all of that.

1839
01:12:51,018 --> 01:12:54,900
So, and actually, they
don't make predictions

1840
01:12:54,900 --> 01:12:56,730
in the space of images.

1841
01:12:56,730 --> 01:12:59,430
They now make predictions directly in 3D

1842
01:12:59,430 --> 01:13:02,550
and, actually, in three
dimensions around the car.

1843
01:13:02,550 --> 01:13:07,020
And now, actually, we don't
manually fuse the predictions

1844
01:13:07,020 --> 01:13:09,270
in 3D over time we don't trust ourselves

1845
01:13:09,270 --> 01:13:10,350
to write that tracker.

1846
01:13:10,350 --> 01:13:12,900
So actually, we give the neural net

1847
01:13:12,900 --> 01:13:14,250
the information over time.

1848
01:13:14,250 --> 01:13:16,980
So it takes these videos now
and makes those predictions.

1849
01:13:16,980 --> 01:13:19,050
And so you're just like
putting more and more power

1850
01:13:19,050 --> 01:13:21,390
into the neural net processing
and at the end of it,

1851
01:13:21,390 --> 01:13:24,930
the eventual goal is to
have most of the software

1852
01:13:24,930 --> 01:13:27,017
potentially be in the 2.0 end

1853
01:13:28,350 --> 01:13:30,060
because it works significantly better.

1854
01:13:30,060 --> 01:13:31,470
Humans are just not very good

1855
01:13:31,470 --> 01:13:32,550
at writing software, basically.

1856
01:13:32,550 --> 01:13:36,510
- So the prediction is
happening in this 4D land.

1857
01:13:36,510 --> 01:13:37,343
- [Andrej] Yeah.

1858
01:13:37,343 --> 01:13:38,580
- Was three-dimensional world over time.

1859
01:13:38,580 --> 01:13:39,413
- [Andrej] Yeah.

1860
01:13:39,413 --> 01:13:42,543
- How do you do annotation in that world?

1861
01:13:44,130 --> 01:13:47,550
So data annotation, whether
it's self-supervised

1862
01:13:47,550 --> 01:13:52,200
or manual by humans is a big part

1863
01:13:52,200 --> 01:13:54,660
of this Software 2.0 world.

1864
01:13:54,660 --> 01:13:56,400
- I would say by far in the industry,

1865
01:13:56,400 --> 01:13:57,960
if you're talking about the industry

1866
01:13:57,960 --> 01:14:00,540
and what is the technology
of what we have available?

1867
01:14:00,540 --> 01:14:01,860
Everything is supervised learning.

1868
01:14:01,860 --> 01:14:05,130
So you need data sets
of input, desired output

1869
01:14:05,130 --> 01:14:06,570
and you need lots of it.

1870
01:14:06,570 --> 01:14:09,510
And there are three properties
of it that you need.

1871
01:14:09,510 --> 01:14:10,710
You need it to be very large,

1872
01:14:10,710 --> 01:14:13,140
you need it to be accurate, no mistakes

1873
01:14:13,140 --> 01:14:14,340
and you need it to be diverse.

1874
01:14:14,340 --> 01:14:17,580
You don't want to just have a lot

1875
01:14:17,580 --> 01:14:19,050
of correct examples of one thing.

1876
01:14:19,050 --> 01:14:20,280
You need to really cover the space

1877
01:14:20,280 --> 01:14:22,120
of possibility as much as you can.

1878
01:14:22,120 --> 01:14:24,240
And the more you can cover
the space of possible inputs,

1879
01:14:24,240 --> 01:14:26,460
the better the algorithm
will work at the end.

1880
01:14:26,460 --> 01:14:27,930
Now once you have really good data sets

1881
01:14:27,930 --> 01:14:31,620
that you're collecting,
curating and cleaning,

1882
01:14:31,620 --> 01:14:35,280
you can train your neural
net on top of that.

1883
01:14:35,280 --> 01:14:37,260
So a lot of the work goes
into cleaning those data sets.

1884
01:14:37,260 --> 01:14:38,643
Now, as you pointed out,

1885
01:14:40,320 --> 01:14:42,603
the question is how do
you achieve a ton of-

1886
01:14:43,530 --> 01:14:45,330
If you want to basically predict in 3D,

1887
01:14:45,330 --> 01:14:47,880
you need data in 3D to back that up.

1888
01:14:47,880 --> 01:14:50,220
So in this video, we have eight videos

1889
01:14:50,220 --> 01:14:52,560
coming from all the cameras of the system

1890
01:14:52,560 --> 01:14:55,170
and this is what they
saw and this is the truth

1891
01:14:55,170 --> 01:14:56,970
of what actually was
around, there was this car

1892
01:14:56,970 --> 01:14:58,350
and there was this car, this car,

1893
01:14:58,350 --> 01:14:59,370
these are the lane line markings,

1894
01:14:59,370 --> 01:15:00,480
this is the geometry of the road.

1895
01:15:00,480 --> 01:15:02,670
There's a traffic light in this
three-dimensional position,

1896
01:15:02,670 --> 01:15:04,710
you need the ground truth.

1897
01:15:04,710 --> 01:15:07,110
And so the big question that
team was solving, of course,

1898
01:15:07,110 --> 01:15:09,480
is how do you arrive at that ground truth?

1899
01:15:09,480 --> 01:15:10,860
Because once you have a million of it

1900
01:15:10,860 --> 01:15:12,810
and it's large, clean and diverse,

1901
01:15:12,810 --> 01:15:14,730
then training a neural net
on it works extremely well

1902
01:15:14,730 --> 01:15:16,871
and you can ship that into the car.

1903
01:15:16,871 --> 01:15:18,780
And so there's many mechanisms

1904
01:15:18,780 --> 01:15:20,727
by which we collected that training data.

1905
01:15:20,727 --> 01:15:22,710
You can always go for human annotation,

1906
01:15:22,710 --> 01:15:25,290
you can go for simulation
as a source of ground truth,

1907
01:15:25,290 --> 01:15:27,890
you can also go for what
we call the offline tracker

1908
01:15:29,340 --> 01:15:31,620
that we've spoken about
at the AI Day and so on,

1909
01:15:31,620 --> 01:15:34,410
which is basically an automatic
reconstruction process

1910
01:15:34,410 --> 01:15:36,750
for taking those videos and recovering

1911
01:15:36,750 --> 01:15:40,800
the three-dimensional reality
of what was around that car.

1912
01:15:40,800 --> 01:15:42,330
So basically, think of
doing a three-dimensional

1913
01:15:42,330 --> 01:15:44,490
reconstruction as an offline thing

1914
01:15:44,490 --> 01:15:46,710
and then understanding that okay,

1915
01:15:46,710 --> 01:15:49,380
there's 10 seconds of
video, this is what we saw

1916
01:15:49,380 --> 01:15:52,347
and therefore here's all the
lane lines, cars and so on.

1917
01:15:52,347 --> 01:15:53,790
And then once you have that annotation,

1918
01:15:53,790 --> 01:15:56,310
you can train neural nets to imitate it.

1919
01:15:56,310 --> 01:15:59,340
- And how difficult is
the 3D reconstruction?

1920
01:15:59,340 --> 01:16:01,590
- [Andrej] It's difficult
but it can be done.

1921
01:16:01,590 --> 01:16:03,510
- So there's overlap between the cameras

1922
01:16:03,510 --> 01:16:04,960
and you do the reconstruction

1923
01:16:07,320 --> 01:16:09,270
and perhaps if there's any inaccuracy

1924
01:16:09,270 --> 01:16:12,030
so that's caught in the annotation step.

1925
01:16:12,030 --> 01:16:12,863
- Yes.

1926
01:16:12,863 --> 01:16:14,010
The nice thing about the annotation

1927
01:16:14,010 --> 01:16:15,810
is that it is fully offline.

1928
01:16:15,810 --> 01:16:18,120
You have infinite time, you
have a chunk of one minute

1929
01:16:18,120 --> 01:16:19,620
and you're trying to just offline

1930
01:16:19,620 --> 01:16:21,060
in a super-computer somewhere.

1931
01:16:21,060 --> 01:16:23,280
Figure out where were all the
positions of all the cars,

1932
01:16:23,280 --> 01:16:25,440
of all the people, and you
have your full one minute

1933
01:16:25,440 --> 01:16:26,910
of video from all the angles

1934
01:16:26,910 --> 01:16:27,930
and you can run all the
neural nets you want

1935
01:16:27,930 --> 01:16:31,173
and they can be very
efficient, massive neural nets,

1936
01:16:31,173 --> 01:16:33,420
they can be neural net that
can't even run in the car

1937
01:16:33,420 --> 01:16:34,710
later at test time.

1938
01:16:34,710 --> 01:16:35,587
So they can be even more
powerful neural nets

1939
01:16:35,587 --> 01:16:37,830
than what you can eventually deploy.

1940
01:16:37,830 --> 01:16:39,150
So you can do anything you want,

1941
01:16:39,150 --> 01:16:41,460
three-dimensional
reconstruction, neural nets,

1942
01:16:41,460 --> 01:16:43,110
anything you want just
to recover that truth

1943
01:16:43,110 --> 01:16:45,270
and then you supervise that truth.

1944
01:16:45,270 --> 01:16:46,200
- What have you learned?

1945
01:16:46,200 --> 01:16:50,910
You said no mistakes about
humans doing annotation

1946
01:16:50,910 --> 01:16:55,910
'cause there's like a range
of things they're good at

1947
01:16:56,190 --> 01:16:58,040
in terms of clicking stuff on screen.

1948
01:16:58,890 --> 01:17:02,730
How interesting is that to
you of a problem of designing

1949
01:17:02,730 --> 01:17:06,270
an annotator where humans
are accurate, enjoy it,

1950
01:17:06,270 --> 01:17:08,130
what are they even the
metrics, are efficient,

1951
01:17:08,130 --> 01:17:09,930
are productive, all that kind of stuff?

1952
01:17:09,930 --> 01:17:12,540
- Yeah, so I grew the
annotation team at Tesla

1953
01:17:12,540 --> 01:17:16,140
from, basically, zero to a
thousand while I was there.

1954
01:17:16,140 --> 01:17:18,120
That was really interesting.

1955
01:17:18,120 --> 01:17:20,760
My background is a PhD student researcher.

1956
01:17:20,760 --> 01:17:24,270
So growing that kind of
organization was pretty crazy.

1957
01:17:24,270 --> 01:17:27,510
But yeah, I think it's
extremely interesting

1958
01:17:27,510 --> 01:17:29,070
and part of the design process very much

1959
01:17:29,070 --> 01:17:31,710
behind the autopilot as
to where you use humans.

1960
01:17:31,710 --> 01:17:34,050
Humans are very good at
certain kinds of annotations.

1961
01:17:34,050 --> 01:17:34,920
They're very good, for example,

1962
01:17:34,920 --> 01:17:36,630
at two-dimensional annotations of images.

1963
01:17:36,630 --> 01:17:39,900
They're not good at
annotating cars over time

1964
01:17:39,900 --> 01:17:41,010
in three-dimensional space.

1965
01:17:41,010 --> 01:17:42,087
Very, very hard.

1966
01:17:42,087 --> 01:17:44,553
And so that's why we're very
careful to design the tasks

1967
01:17:44,553 --> 01:17:46,500
that are easy to do for humans

1968
01:17:46,500 --> 01:17:49,218
versus things that should be
left to the offline tracker.

1969
01:17:49,218 --> 01:17:51,330
Maybe the computer will
do all the triangulation

1970
01:17:51,330 --> 01:17:53,340
and 3D construction but the human will say

1971
01:17:53,340 --> 01:17:56,070
exactly these pixels of the image are car.

1972
01:17:56,070 --> 01:17:57,660
Exactly these pixels are human.

1973
01:17:57,660 --> 01:18:00,840
And so co-designing the
data annotation pipeline

1974
01:18:00,840 --> 01:18:04,680
was very much bread and
butter what I was doing daily.

1975
01:18:04,680 --> 01:18:06,510
- Do you think there's
still a lot of open problems

1976
01:18:06,510 --> 01:18:07,443
in that space?

1977
01:18:08,880 --> 01:18:11,250
Just in general annotation

1978
01:18:11,250 --> 01:18:14,400
where the stuff the machines
are good at, machines do

1979
01:18:14,400 --> 01:18:16,650
and the humans do what they're good at

1980
01:18:16,650 --> 01:18:18,780
and there's maybe some iterative process?

1981
01:18:18,780 --> 01:18:19,680
- Right.

1982
01:18:19,680 --> 01:18:21,210
I think to a very large extent,

1983
01:18:21,210 --> 01:18:22,590
we went through a number of iterations

1984
01:18:22,590 --> 01:18:25,440
and we learned a ton about
how to create these data sets.

1985
01:18:26,280 --> 01:18:28,015
I'm not seeing big open problems.

1986
01:18:28,015 --> 01:18:31,800
Originally, when I joined
I was really not sure

1987
01:18:31,800 --> 01:18:32,633
how this would turn out.

1988
01:18:32,633 --> 01:18:33,466
- [Lex] Yeah.

1989
01:18:33,466 --> 01:18:35,160
- But by the time I left
I was much more secure

1990
01:18:35,160 --> 01:18:37,320
and actually, we understand the philosophy

1991
01:18:37,320 --> 01:18:38,460
of how to create these data sets

1992
01:18:38,460 --> 01:18:39,570
and I was pretty comfortable

1993
01:18:39,570 --> 01:18:41,580
with where that was at the time.

1994
01:18:41,580 --> 01:18:45,900
- So what are strengths
and limitations of cameras

1995
01:18:45,900 --> 01:18:48,570
for the driving task in your understanding

1996
01:18:48,570 --> 01:18:51,150
when you formulate the
driving task as a vision task

1997
01:18:51,150 --> 01:18:55,170
with eight cameras, you've
seen that the entire,

1998
01:18:55,170 --> 01:18:57,210
most of the history of
the computer vision field

1999
01:18:57,210 --> 01:18:59,010
when it has to do with neural networks.

2000
01:18:59,010 --> 01:19:01,260
Just if you step back,
what are the strengths

2001
01:19:01,260 --> 01:19:05,730
and limitations of pixels,
of using pixels to drive?

2002
01:19:05,730 --> 01:19:08,880
- Yeah, pixels I think
are a beautiful sensory,

2003
01:19:08,880 --> 01:19:10,500
beautiful sensor I would say.

2004
01:19:10,500 --> 01:19:12,360
The thing is like cameras
are very, very cheap

2005
01:19:12,360 --> 01:19:15,450
and they provide a ton of
information, ton of bits.

2006
01:19:15,450 --> 01:19:19,110
So it's a extremely cheap
sensor for a ton of bits.

2007
01:19:19,110 --> 01:19:20,730
And each one of these bits is a constraint

2008
01:19:20,730 --> 01:19:21,840
on the state of the world.

2009
01:19:21,840 --> 01:19:26,190
And so you get lots of
megapixel images, very cheap

2010
01:19:26,190 --> 01:19:27,840
and it just gives you
all these constraints

2011
01:19:27,840 --> 01:19:30,000
for understanding what's
actually out there in the world.

2012
01:19:30,000 --> 01:19:34,440
So vision is probably the
highest bandwidth sensor.

2013
01:19:34,440 --> 01:19:36,140
It's a very high bandwidth sensor.

2014
01:19:37,987 --> 01:19:42,350
- I love that pixels is a
constraint on the world.

2015
01:19:43,593 --> 01:19:48,593
It's this highly complex,
high bandwidth constraint

2016
01:19:48,900 --> 01:19:50,028
on the stage of the world.

2017
01:19:50,028 --> 01:19:50,861
That's fascinating.

2018
01:19:50,861 --> 01:19:53,250
- It's not just that, but
again this real importance

2019
01:19:53,250 --> 01:19:56,100
of it's the sensor that humans use,

2020
01:19:56,100 --> 01:19:59,160
therefore everything is
designed for that sensor.

2021
01:19:59,160 --> 01:20:00,052
- [Lex] Yeah.

2022
01:20:00,052 --> 01:20:02,580
- The text, the writing,
the flashing signs,

2023
01:20:02,580 --> 01:20:05,700
everything is designed for vision and so,

2024
01:20:05,700 --> 01:20:07,230
and you just find it everywhere.

2025
01:20:07,230 --> 01:20:10,170
And so that's why that is the
interface you want to be in

2026
01:20:10,170 --> 01:20:12,390
talking again about these
universal interfaces

2027
01:20:12,390 --> 01:20:13,920
and that's where we
actually want to measure

2028
01:20:13,920 --> 01:20:18,060
the world as well and then
develop software for that sensor.

2029
01:20:18,060 --> 01:20:21,570
- But there's other constraints
on the state of the world

2030
01:20:21,570 --> 01:20:24,120
that humans use to understand the world.

2031
01:20:24,120 --> 01:20:27,732
I mean vision ultimately is the main one.

2032
01:20:27,732 --> 01:20:32,730
But we're referencing our
understanding of human behavior

2033
01:20:32,730 --> 01:20:36,780
and some common-sense physics
that could be inferred

2034
01:20:36,780 --> 01:20:39,390
from vision, from a
perception perspective.

2035
01:20:39,390 --> 01:20:43,860
But it feels like we're
using some kind of reasoning

2036
01:20:43,860 --> 01:20:45,690
to predict the world.

2037
01:20:45,690 --> 01:20:46,523
- [Andrej] Yeah, hundred percent.

2038
01:20:46,523 --> 01:20:47,620
- Not just the pixels.

2039
01:20:47,620 --> 01:20:49,710
- I mean you have a powerful prior service

2040
01:20:49,710 --> 01:20:52,350
for how the world evolves
over time, et cetera.

2041
01:20:52,350 --> 01:20:54,420
So it's not just about the likelihood term

2042
01:20:54,420 --> 01:20:56,730
coming up from the data itself telling you

2043
01:20:56,730 --> 01:20:57,810
about what you are observing,

2044
01:20:57,810 --> 01:20:59,400
but also the prior term of

2045
01:20:59,400 --> 01:21:01,077
what are the likely things to see

2046
01:21:01,077 --> 01:21:03,147
and how do they likely move and so on.

2047
01:21:03,147 --> 01:21:05,580
- And the question is how complex

2048
01:21:05,580 --> 01:21:10,580
is the range of possibilities
that might happen

2049
01:21:11,190 --> 01:21:12,570
in the driving task.

2050
01:21:12,570 --> 01:21:13,740
- [Andrej] Right.

2051
01:21:13,740 --> 01:21:15,480
- Is that to you still an open problem

2052
01:21:15,480 --> 01:21:18,633
of how difficult is driving,
philosophically speaking?

2053
01:21:21,750 --> 01:21:23,790
Of al the time you worked on driving,

2054
01:21:23,790 --> 01:21:26,400
do you understand how hard driving is?

2055
01:21:26,400 --> 01:21:28,050
- Yeah, driving is really hard

2056
01:21:28,050 --> 01:21:29,460
because it has to do with the predictions

2057
01:21:29,460 --> 01:21:31,320
of all these other agents
and the theory of mind

2058
01:21:31,320 --> 01:21:32,640
and what they're gonna do.

2059
01:21:32,640 --> 01:21:34,740
And are they looking at you?

2060
01:21:34,740 --> 01:21:35,573
Where are they looking?

2061
01:21:35,573 --> 01:21:36,480
What are they thinking?

2062
01:21:36,480 --> 01:21:37,313
- [Lex] Yeah.

2063
01:21:37,313 --> 01:21:40,443
- There's a lot that goes
there at the full tail-off,

2064
01:21:41,310 --> 01:21:42,270
the expansion of the nines

2065
01:21:42,270 --> 01:21:44,370
that we have to be
comfortable with eventually

2066
01:21:44,370 --> 01:21:46,230
the final problems are of that form.

2067
01:21:46,230 --> 01:21:48,660
I don't think those are the
problems that are very common.

2068
01:21:48,660 --> 01:21:50,430
I think eventually they're important

2069
01:21:50,430 --> 01:21:52,170
but it's really in the tail end.

2070
01:21:52,170 --> 01:21:54,561
- In the tail end, the rare edge cases.

2071
01:21:54,561 --> 01:21:55,560
- [Andrej] Yes.

2072
01:21:55,560 --> 01:21:58,830
- From the vision perspective,
what are the toughest parts

2073
01:21:58,830 --> 01:22:00,483
of the vision problem of driving?

2074
01:22:03,360 --> 01:22:06,120
- Well, basically, the
sensor is extremely powerful

2075
01:22:06,120 --> 01:22:08,470
but you still need to
process that information.

2076
01:22:09,600 --> 01:22:12,630
And so going from brightnesses
of these pixel values

2077
01:22:12,630 --> 01:22:14,610
to, hey, here are the
three-dimensional world,

2078
01:22:14,610 --> 01:22:16,710
is extremely hard and that's
what the neural networks

2079
01:22:16,710 --> 01:22:18,270
are fundamentally doing.

2080
01:22:18,270 --> 01:22:21,870
And so the difficulty really is in just

2081
01:22:21,870 --> 01:22:23,940
doing an extremely good job of engineering

2082
01:22:23,940 --> 01:22:27,300
the entire pipeline,
the entire data engine,

2083
01:22:27,300 --> 01:22:29,820
having the capacity to
train these neural nets,

2084
01:22:29,820 --> 01:22:33,720
having the ability to evaluate
the system and iterate on it.

2085
01:22:33,720 --> 01:22:36,270
So I would say just doing
this in production at scale

2086
01:22:36,270 --> 01:22:38,550
is the hard part, it's
an execution problem.

2087
01:22:38,550 --> 01:22:42,863
- So the data engine but also
the deployment of the system

2088
01:22:45,540 --> 01:22:47,340
such that has low latency performance.

2089
01:22:47,340 --> 01:22:48,930
So it has to do all these steps.

2090
01:22:48,930 --> 01:22:50,051
- Yeah.

2091
01:22:50,051 --> 01:22:50,940
For the neural nets
specifically just making sure

2092
01:22:50,940 --> 01:22:53,190
everything fits into the chip on the car.

2093
01:22:53,190 --> 01:22:54,023
- [Lex] Yeah.

2094
01:22:54,023 --> 01:22:55,680
- And you have a finite budget of flops

2095
01:22:55,680 --> 01:22:58,440
that you can perform and memory bandwidth

2096
01:22:58,440 --> 01:23:01,170
and other constraints and you
have to make sure it flies

2097
01:23:01,170 --> 01:23:02,460
and you can squeeze in as much computer

2098
01:23:02,460 --> 01:23:03,660
as you can into the tiny.

2099
01:23:03,660 --> 01:23:05,700
- What have you learned from that process?

2100
01:23:05,700 --> 01:23:09,420
Because maybe that's one
of the bigger, new things,

2101
01:23:09,420 --> 01:23:11,760
coming from a research background,

2102
01:23:11,760 --> 01:23:13,800
where there's a system that has to run

2103
01:23:13,800 --> 01:23:15,930
under heavily constrained resources.

2104
01:23:15,930 --> 01:23:17,340
Has to run really fast.

2105
01:23:17,340 --> 01:23:20,940
What insights have you learned from that?

2106
01:23:20,940 --> 01:23:24,120
- Yeah, I'm not sure if
there's too many insights,

2107
01:23:24,120 --> 01:23:25,560
you're trying to create a neural net

2108
01:23:25,560 --> 01:23:28,230
that will fit in what you have available

2109
01:23:28,230 --> 01:23:29,880
and you're always trying to optimize it.

2110
01:23:29,880 --> 01:23:31,950
And we talked a lot about it on the AI Day

2111
01:23:31,950 --> 01:23:35,160
and, basically, the triple backflips

2112
01:23:35,160 --> 01:23:37,770
that the team is doing
to make sure it all fits

2113
01:23:37,770 --> 01:23:39,540
and utilizes the engine.

2114
01:23:39,540 --> 01:23:42,422
So I think it's extremely good engineering

2115
01:23:42,422 --> 01:23:44,940
and then there's all kinds of
little insights peppered in

2116
01:23:44,940 --> 01:23:46,800
on how to do it properly.

2117
01:23:46,800 --> 01:23:47,633
- Let's actually zoom out

2118
01:23:47,633 --> 01:23:49,800
'cause I don't think we
talked about the data engine,

2119
01:23:49,800 --> 01:23:53,640
the entirety of the layouts of this idea

2120
01:23:53,640 --> 01:23:57,300
that I think is just beautiful
with humans in the loop.

2121
01:23:57,300 --> 01:23:59,520
Can you describe the data engine?

2122
01:23:59,520 --> 01:24:01,230
- Yeah, the data engine is what I call

2123
01:24:01,230 --> 01:24:04,740
the almost biological feeling process

2124
01:24:04,740 --> 01:24:08,160
by which you perfect the training sets

2125
01:24:08,160 --> 01:24:10,260
for these neural networks.

2126
01:24:10,260 --> 01:24:12,660
So because most of the
programming now is in the level

2127
01:24:12,660 --> 01:24:14,340
of these data sets and
make sure they're large,

2128
01:24:14,340 --> 01:24:18,060
diverse and clean, basically
you have a data set

2129
01:24:18,060 --> 01:24:20,760
that you think is good,
you train your neural net,

2130
01:24:20,760 --> 01:24:24,060
you deploy it, and then you
observe how well it's performing

2131
01:24:24,060 --> 01:24:26,730
and you're trying to
always increase the quality

2132
01:24:26,730 --> 01:24:27,600
of your data set.

2133
01:24:27,600 --> 01:24:29,340
So you're trying to catch scenarios,

2134
01:24:29,340 --> 01:24:32,040
basically, that are, basically, rare.

2135
01:24:32,040 --> 01:24:33,570
And it is in these scenarios

2136
01:24:33,570 --> 01:24:35,040
that neural nets will
typically struggle in

2137
01:24:35,040 --> 01:24:37,530
because they weren't told
what to do in those rare cases

2138
01:24:37,530 --> 01:24:38,760
in the data set.

2139
01:24:38,760 --> 01:24:39,780
But now you can close the loop

2140
01:24:39,780 --> 01:24:42,660
because if you can now
collect all those at scale,

2141
01:24:42,660 --> 01:24:45,330
you can then feed them back
into the reconstruction process

2142
01:24:45,330 --> 01:24:48,570
I described and reconstruct
the truth in those cases

2143
01:24:48,570 --> 01:24:49,797
and add it to the dataset.

2144
01:24:49,797 --> 01:24:52,380
And so the whole thing
ends up being a staircase

2145
01:24:52,380 --> 01:24:55,620
of improvement of
perfecting your training set

2146
01:24:55,620 --> 01:24:57,030
and you have to go through deployments

2147
01:24:57,030 --> 01:24:59,520
so that you can mine the parts

2148
01:24:59,520 --> 01:25:02,550
that are not yet represented
well on the dataset.

2149
01:25:02,550 --> 01:25:03,810
So your dataset is basically imperfect.

2150
01:25:03,810 --> 01:25:07,020
It needs to be diverse, it
has pockets that are missing

2151
01:25:07,020 --> 01:25:08,430
and you need to pat out the pockets.

2152
01:25:08,430 --> 01:25:11,640
You can sort of think of
it that way in the data.

2153
01:25:11,640 --> 01:25:13,170
- What role do humans play in this?

2154
01:25:13,170 --> 01:25:17,340
So what's this biological
system like a human body

2155
01:25:17,340 --> 01:25:18,810
made up of cells?

2156
01:25:18,810 --> 01:25:20,820
What role?

2157
01:25:20,820 --> 01:25:23,280
How do you optimize the human system?

2158
01:25:23,280 --> 01:25:26,160
The multiple engineers collaborating,

2159
01:25:26,160 --> 01:25:30,668
figuring out what to focus
on, what to contribute,

2160
01:25:30,668 --> 01:25:34,023
which task to optimize
in this neural network.

2161
01:25:35,340 --> 01:25:38,853
Who's in charge of figuring
out which task needs more data?

2162
01:25:39,870 --> 01:25:44,390
Can you speak to the
hyperparameters, the human system?

2163
01:25:44,390 --> 01:25:46,500
- It really just comes down
to extremely good execution

2164
01:25:46,500 --> 01:25:48,360
from an engineering team that
knows what they're doing.

2165
01:25:48,360 --> 01:25:50,760
They understand intuitively
the philosophical insights

2166
01:25:50,760 --> 01:25:52,620
underlying the data engine and the process

2167
01:25:52,620 --> 01:25:56,250
by which the system
improves and how to, again,

2168
01:25:56,250 --> 01:25:58,710
delegate the strategy
of the data collection

2169
01:25:58,710 --> 01:25:59,700
and how that works.

2170
01:25:59,700 --> 01:26:02,100
And then just making sure it's
all extremely well executed.

2171
01:26:02,100 --> 01:26:03,423
And that's where most of the work,

2172
01:26:03,423 --> 01:26:05,880
it's not even the
philosophizing or the research

2173
01:26:05,880 --> 01:26:06,713
or the ideas of it.

2174
01:26:06,713 --> 01:26:08,910
It's just extremely good
execution is so hard

2175
01:26:08,910 --> 01:26:10,770
when you're dealing
with data at that scale.

2176
01:26:10,770 --> 01:26:13,227
- So your role in the
data engine executing well

2177
01:26:13,227 --> 01:26:16,320
and it is difficult and
extremely important.

2178
01:26:16,320 --> 01:26:21,320
Is there a priority of a
vision board of saying like,

2179
01:26:22,230 --> 01:26:25,320
we really need to get
better at stoplights?

2180
01:26:25,320 --> 01:26:26,153
- [Andrej] Yeah.

2181
01:26:26,153 --> 01:26:27,957
- The prioritization of tasks?

2182
01:26:27,957 --> 01:26:28,790
- [Andrej] Yes.

2183
01:26:28,790 --> 01:26:30,600
- Is that essentially, and
that comes from the data?

2184
01:26:30,600 --> 01:26:33,000
- That comes to, a very large extent

2185
01:26:33,000 --> 01:26:36,000
to what we are trying to
achieve in the product roadmap.

2186
01:26:36,000 --> 01:26:38,970
The release we're trying
to get out and the feedback

2187
01:26:38,970 --> 01:26:41,820
from the QA team where the
system is struggling or not,

2188
01:26:41,820 --> 01:26:42,840
the things we're trying to improve.

2189
01:26:42,840 --> 01:26:47,250
- And the QA team gives some
signal, some information

2190
01:26:47,250 --> 01:26:49,800
in aggregate about the
performance of the system

2191
01:26:49,800 --> 01:26:50,633
in various conditions.

2192
01:26:50,633 --> 01:26:51,466
- That's right.

2193
01:26:51,466 --> 01:26:52,299
And then of course all of us drive it

2194
01:26:52,299 --> 01:26:53,132
and we can also see it.

2195
01:26:53,132 --> 01:26:55,020
It's really nice to work with a system

2196
01:26:55,020 --> 01:26:57,090
that you can also experience yourself.

2197
01:26:57,090 --> 01:26:58,650
It drives you home.

2198
01:26:58,650 --> 01:27:00,750
- Is there some insight you can draw

2199
01:27:00,750 --> 01:27:02,280
from your individual experience

2200
01:27:02,280 --> 01:27:03,510
that you just can't quite get

2201
01:27:03,510 --> 01:27:05,843
from an aggregate
statistical analysis of data?

2202
01:27:05,843 --> 01:27:06,933
- I would say so, yeah.

2203
01:27:06,933 --> 01:27:08,250
- [Lex] It's so weird, right?

2204
01:27:08,250 --> 01:27:09,360
- Yes.

2205
01:27:09,360 --> 01:27:11,387
- It's not scientific in a sense

2206
01:27:11,387 --> 01:27:14,010
'cause you're just one anecdotal sample.

2207
01:27:14,010 --> 01:27:17,340
- Yeah, I think there's a ton
of, it's a source of truth.

2208
01:27:17,340 --> 01:27:18,930
It's your interaction with the system.

2209
01:27:18,930 --> 01:27:19,763
- [Lex] Yeah.

2210
01:27:19,763 --> 01:27:20,596
- And you can see it,
you can play with it,

2211
01:27:20,596 --> 01:27:23,340
you can perturb it, you
can get a sense of it,

2212
01:27:23,340 --> 01:27:24,660
you have an intuition for it.

2213
01:27:24,660 --> 01:27:29,103
I think numbers and plots
and graphs are much harder.

2214
01:27:30,180 --> 01:27:31,290
It hides a lot of-

2215
01:27:31,290 --> 01:27:34,323
- It's like if you train a language model,

2216
01:27:35,340 --> 01:27:38,640
it's a really powerful way is
by you interacting with it.

2217
01:27:38,640 --> 01:27:39,473
- [Andrej] Yeah, a hundred percent.

2218
01:27:39,473 --> 01:27:40,680
- Start try to build up an intuition.

2219
01:27:40,680 --> 01:27:42,900
- Yeah, I think Elon also,

2220
01:27:42,900 --> 01:27:45,240
he always wanted to
drive the system himself.

2221
01:27:45,240 --> 01:27:48,930
He drives a lot and I don't
wanna say almost daily.

2222
01:27:48,930 --> 01:27:51,810
So he also sees this as a source of truth,

2223
01:27:51,810 --> 01:27:56,250
you driving the system and
it performing and yeah.

2224
01:27:56,250 --> 01:27:57,720
- So what do you think?

2225
01:27:57,720 --> 01:27:58,893
Tough questions here.

2226
01:28:00,030 --> 01:28:04,950
So, Tesla, last year removed
radar from the sensor suite

2227
01:28:04,950 --> 01:28:07,020
and now just announce
that it's gonna remove

2228
01:28:07,020 --> 01:28:10,860
all ultrasonic sensors
relying solely on vision,

2229
01:28:10,860 --> 01:28:14,850
so camera only, does that
make the perception problem

2230
01:28:14,850 --> 01:28:16,383
harder or easier?

2231
01:28:18,030 --> 01:28:20,189
- I would almost reframe
the question in some way.

2232
01:28:20,189 --> 01:28:22,110
So the thing is basically,

2233
01:28:22,110 --> 01:28:23,880
you would think that additional sensors.

2234
01:28:23,880 --> 01:28:25,170
- Wait, wait, wait, can I just interrupt?

2235
01:28:25,170 --> 01:28:26,003
- [Andrej] Go ahead.

2236
01:28:26,003 --> 01:28:28,320
- I wonder if a language
model will ever do that

2237
01:28:28,320 --> 01:28:29,153
if you prompt it.

2238
01:28:29,153 --> 01:28:31,069
Let me reframe your question.

2239
01:28:31,069 --> 01:28:32,430
That would be epic.

2240
01:28:32,430 --> 01:28:33,780
That's the wrong prompt.

2241
01:28:33,780 --> 01:28:34,613
Sorry.

2242
01:28:34,613 --> 01:28:36,390
- Yeah, so it's a little
bit of a wrong question

2243
01:28:36,390 --> 01:28:38,700
because, basically, you would
think that these sensors

2244
01:28:38,700 --> 01:28:40,380
are an asset to you.

2245
01:28:40,380 --> 01:28:41,213
- [Lex] Yeah.

2246
01:28:41,213 --> 01:28:43,560
- But if you fully
consider the entire product

2247
01:28:43,560 --> 01:28:45,630
in its entirety, these sensors

2248
01:28:45,630 --> 01:28:47,730
are actually potentially a liability

2249
01:28:47,730 --> 01:28:49,800
because these sensors aren't free.

2250
01:28:49,800 --> 01:28:52,200
They don't just appear on your car.

2251
01:28:52,200 --> 01:28:53,850
Suddenly you have an entire supply chain,

2252
01:28:53,850 --> 01:28:55,380
you have people procuring it,

2253
01:28:55,380 --> 01:28:57,840
there can be problems with
them, they may need replacement.

2254
01:28:57,840 --> 01:28:59,100
They are part of the
manufacturing process.

2255
01:28:59,100 --> 01:29:01,710
They can hold back the
line in the production.

2256
01:29:01,710 --> 01:29:03,300
You need to source them,
you need to maintain them,

2257
01:29:03,300 --> 01:29:06,720
you have to have teams that
ride the firmware, all of it.

2258
01:29:06,720 --> 01:29:08,430
And then you also have
to incorporate them,

2259
01:29:08,430 --> 01:29:09,990
infuse them into the system in some way.

2260
01:29:09,990 --> 01:29:13,093
And so it actually bloats a lot of it.

2261
01:29:13,093 --> 01:29:16,950
And I think Elon is really
good at simplify, simplify,

2262
01:29:16,950 --> 01:29:18,330
best part is no part.

2263
01:29:18,330 --> 01:29:19,290
And he always tries to throw away things

2264
01:29:19,290 --> 01:29:22,050
that are not essential because
he understands the entropy

2265
01:29:22,050 --> 01:29:23,880
in organizations and an approach.

2266
01:29:23,880 --> 01:29:26,940
And I think in this case the cost is high

2267
01:29:26,940 --> 01:29:27,773
and you're not potentially seeing it

2268
01:29:27,773 --> 01:29:29,730
if you're just a computer vision engineer

2269
01:29:29,730 --> 01:29:31,410
and I'm just trying to improve my network

2270
01:29:31,410 --> 01:29:33,960
and is it more useful or less useful?

2271
01:29:33,960 --> 01:29:35,217
How useful is it?

2272
01:29:35,217 --> 01:29:37,590
And the thing is, if once
you consider the full cost

2273
01:29:37,590 --> 01:29:40,200
of a sensor, it actually
is potentially a liability

2274
01:29:40,200 --> 01:29:41,400
and you need to be really sure

2275
01:29:41,400 --> 01:29:43,830
that it's giving you
extremely useful information.

2276
01:29:43,830 --> 01:29:46,560
In this case, we looked at
using it or not using it

2277
01:29:46,560 --> 01:29:48,090
and the delta was not massive.

2278
01:29:48,090 --> 01:29:49,560
And so it's not useful.

2279
01:29:49,560 --> 01:29:52,650
- Is it also bloat in the data engine,

2280
01:29:52,650 --> 01:29:53,790
like having more sensors?

2281
01:29:53,790 --> 01:29:55,200
- Hundred percent.

2282
01:29:55,200 --> 01:29:56,130
- Is it a distraction?

2283
01:29:56,130 --> 01:29:57,780
- And these sensors, they
can change over time.

2284
01:29:57,780 --> 01:29:59,820
For example, you can have
one type of say radar,

2285
01:29:59,820 --> 01:30:00,870
you can have other type of radar.

2286
01:30:00,870 --> 01:30:01,703
They change over time.

2287
01:30:01,703 --> 01:30:02,910
Now suddenly you need to worry about it.

2288
01:30:02,910 --> 01:30:05,010
Now suddenly you have a
column in your sequel light

2289
01:30:05,010 --> 01:30:06,930
telling you, oh, what sensor type was it?

2290
01:30:06,930 --> 01:30:08,730
And they all have different distributions

2291
01:30:08,730 --> 01:30:13,730
and then they contribute noise
and entropy into everything

2292
01:30:13,770 --> 01:30:15,270
and they bloat stuff.

2293
01:30:15,270 --> 01:30:16,530
And also organizationally,

2294
01:30:16,530 --> 01:30:17,643
it's been really fascinating to me

2295
01:30:17,643 --> 01:30:19,413
that it can be very distracting.

2296
01:30:20,610 --> 01:30:23,850
If all you wanna get to work is vision,

2297
01:30:23,850 --> 01:30:25,170
all the resources are on it

2298
01:30:25,170 --> 01:30:27,180
and you're building out a data engine

2299
01:30:27,180 --> 01:30:28,680
and you're actually
making forward progress

2300
01:30:28,680 --> 01:30:32,160
because that is the sensor
with the most bandwidth,

2301
01:30:32,160 --> 01:30:33,630
the most constraints on the world.

2302
01:30:33,630 --> 01:30:34,950
And you're investing fully into that.

2303
01:30:34,950 --> 01:30:37,380
And you can make that extremely good.

2304
01:30:37,380 --> 01:30:40,560
You have only a finite amount
of sort of spend of focus

2305
01:30:40,560 --> 01:30:42,870
across different facets of the system.

2306
01:30:42,870 --> 01:30:47,870
- And this reminds me of Rich
Sutton, "The Bitter Lesson"

2307
01:30:48,395 --> 01:30:50,160
that just seems like
simplifying the system.

2308
01:30:50,160 --> 01:30:51,240
- [Andrej] Yeah.

2309
01:30:51,240 --> 01:30:52,380
- In the long run.

2310
01:30:52,380 --> 01:30:54,147
And of course, you don't
know what the long run is

2311
01:30:54,147 --> 01:30:56,160
and it seems to be always
the right solution.

2312
01:30:56,160 --> 01:30:56,993
- Yeah.

2313
01:30:56,993 --> 01:30:57,921
Yes.

2314
01:30:57,921 --> 01:30:58,754
- In that case, it was for RL

2315
01:30:58,754 --> 01:31:00,030
but it seems to apply generally

2316
01:31:00,030 --> 01:31:02,070
across all systems that do computation.

2317
01:31:02,070 --> 01:31:02,903
- [Andrej] Yeah.

2318
01:31:02,903 --> 01:31:06,333
- So what do you think about
the LiDAR as a crutch debate?

2319
01:31:07,500 --> 01:31:11,190
The battle between
point clouds and pixels?

2320
01:31:11,190 --> 01:31:13,620
- Yeah, I think this debate
is always slightly confusing

2321
01:31:13,620 --> 01:31:15,750
to me because it seems
like the actual debate

2322
01:31:15,750 --> 01:31:18,180
should be about do you
have the fleet or not.

2323
01:31:18,180 --> 01:31:19,440
That's the really important thing

2324
01:31:19,440 --> 01:31:22,020
about whether you can achieve
a really good functioning

2325
01:31:22,020 --> 01:31:24,000
of an AI system at this scale.

2326
01:31:24,000 --> 01:31:25,560
- [Lex] So data collection systems.

2327
01:31:25,560 --> 01:31:26,467
- Yeah.

2328
01:31:26,467 --> 01:31:28,380
Do you have a fleet or not is
significantly more important

2329
01:31:28,380 --> 01:31:29,553
whether you have LiDAR or not.

2330
01:31:29,553 --> 01:31:30,963
It's just another sensor.

2331
01:31:32,130 --> 01:31:36,210
And yeah, I think similar
to the radar discussion,

2332
01:31:36,210 --> 01:31:39,690
basically, yeah, I don't think it,

2333
01:31:39,690 --> 01:31:43,980
it basically doesn't
offer extra information.

2334
01:31:43,980 --> 01:31:44,970
It's extremely costly.

2335
01:31:44,970 --> 01:31:45,990
It has all kinds of problems.

2336
01:31:45,990 --> 01:31:46,823
You have to worry about it,

2337
01:31:46,823 --> 01:31:47,940
you have to calibrate it, et cetera.

2338
01:31:47,940 --> 01:31:49,230
It creates bloat and entropy.

2339
01:31:49,230 --> 01:31:52,880
You have to be really sure
that you need this sensor.

2340
01:31:52,880 --> 01:31:54,990
In this case, I basically
don't think you need it.

2341
01:31:54,990 --> 01:31:57,270
And I think honestly, I will
make a stronger statement.

2342
01:31:57,270 --> 01:32:00,600
I think some of the other
companies who are using it

2343
01:32:00,600 --> 01:32:02,220
are probably going to drop it.

2344
01:32:02,220 --> 01:32:03,053
- Yeah.

2345
01:32:03,053 --> 01:32:07,440
So you have to consider
the sensor in the full,

2346
01:32:07,440 --> 01:32:10,380
in considering can you build a big fleet

2347
01:32:10,380 --> 01:32:12,330
that collects a lot of data

2348
01:32:12,330 --> 01:32:15,150
and can you integrate that
sensor with that data,

2349
01:32:15,150 --> 01:32:17,190
and that sensor into a data engine

2350
01:32:17,190 --> 01:32:20,190
that's able to quickly find
different parts of the data

2351
01:32:20,190 --> 01:32:22,350
that then continuously improves

2352
01:32:22,350 --> 01:32:24,337
whatever the model that you're using.

2353
01:32:24,337 --> 01:32:25,170
- Yeah.

2354
01:32:25,170 --> 01:32:27,330
Another way to look at it
is like, vision is necessary

2355
01:32:27,330 --> 01:32:30,600
in a sense that the world is designed

2356
01:32:30,600 --> 01:32:31,500
for human visual consumption.

2357
01:32:31,500 --> 01:32:33,750
So you need vision, it's necessary.

2358
01:32:33,750 --> 01:32:35,940
And then also it is sufficient

2359
01:32:35,940 --> 01:32:37,260
because it has all the information

2360
01:32:37,260 --> 01:32:38,850
that you need for driving.

2361
01:32:38,850 --> 01:32:40,860
And humans obviously use vision to drive.

2362
01:32:40,860 --> 01:32:42,480
So it's both necessary and sufficient.

2363
01:32:42,480 --> 01:32:43,770
So you want to focus resources

2364
01:32:43,770 --> 01:32:44,910
and you have to be really sure

2365
01:32:44,910 --> 01:32:46,740
if you're going to bring in other sensors.

2366
01:32:46,740 --> 01:32:49,440
You could add sensors to infinity,

2367
01:32:49,440 --> 01:32:51,030
at some point you need to draw the line.

2368
01:32:51,030 --> 01:32:53,010
And I think in this case,
you have to really consider

2369
01:32:53,010 --> 01:32:56,850
the full cost of any one
sensor that you're adopting,

2370
01:32:56,850 --> 01:32:58,530
and do you really need it?

2371
01:32:58,530 --> 01:33:00,780
And I think the answer,
in this case, is no.

2372
01:33:00,780 --> 01:33:02,460
- So what do you think about the idea

2373
01:33:02,460 --> 01:33:07,320
that the other companies are
forming high-resolution maps

2374
01:33:07,320 --> 01:33:10,230
and constraining heavily
the geographic regions

2375
01:33:10,230 --> 01:33:11,670
in which they operate?

2376
01:33:11,670 --> 01:33:16,670
Is that approach, in your view,
not going to scale over time

2377
01:33:18,810 --> 01:33:19,964
to the entirety of the United States?

2378
01:33:19,964 --> 01:33:20,797
- [Andrej] Yeah.

2379
01:33:20,797 --> 01:33:21,630
I think-

2380
01:33:21,630 --> 01:33:22,463
- It'll take too long-

2381
01:33:22,463 --> 01:33:24,270
- As you've mentioned like they
pre-map all the environments

2382
01:33:24,270 --> 01:33:25,860
and they need to refresh the map

2383
01:33:25,860 --> 01:33:28,260
and they have a perfect
centimeter-level-accuracy map

2384
01:33:28,260 --> 01:33:29,580
of everywhere they're gonna drive.

2385
01:33:29,580 --> 01:33:30,510
It's crazy.

2386
01:33:30,510 --> 01:33:33,240
How are you going to, when
we're talking about autonomy

2387
01:33:33,240 --> 01:33:35,160
actually changing the
world, we're talking about

2388
01:33:35,160 --> 01:33:38,010
the deployment on the global scale

2389
01:33:38,010 --> 01:33:40,410
of autonomous systems for transportation.

2390
01:33:40,410 --> 01:33:42,750
And if you need to maintain
a centimeter-accurate map

2391
01:33:42,750 --> 01:33:46,140
for earth or for many cities
and keep them updated,

2392
01:33:46,140 --> 01:33:48,240
it's a huge dependency
that you're taking on,

2393
01:33:48,240 --> 01:33:49,950
a huge dependency.

2394
01:33:49,950 --> 01:33:51,570
It's a massive, massive dependency

2395
01:33:51,570 --> 01:33:52,500
and now you need to ask yourself,

2396
01:33:52,500 --> 01:33:54,420
do you really need it?

2397
01:33:54,420 --> 01:33:57,470
And humans don't need it, right?

2398
01:33:57,470 --> 01:34:00,210
So it's very useful to have
a low-level map of like,

2399
01:34:00,210 --> 01:34:01,881
okay, the connectivity of your road,

2400
01:34:01,881 --> 01:34:03,510
you know that there's a fork coming up.

2401
01:34:03,510 --> 01:34:04,380
When you drive in an environment,

2402
01:34:04,380 --> 01:34:05,640
you have that high-level understanding.

2403
01:34:05,640 --> 01:34:09,390
It's like a small Google map
and Tesla uses Google map,

2404
01:34:09,390 --> 01:34:13,050
similar resolution
information in its system,

2405
01:34:13,050 --> 01:34:14,550
but it will not pre-map environments

2406
01:34:14,550 --> 01:34:16,410
to centimeter-level accuracy.

2407
01:34:16,410 --> 01:34:17,700
It's a crutch, it's a distraction,

2408
01:34:17,700 --> 01:34:20,520
it causes entropy, and
it diffuses the team,

2409
01:34:20,520 --> 01:34:22,320
it dilutes the team
and you're not focusing

2410
01:34:22,320 --> 01:34:23,430
on what's actually necessary,

2411
01:34:23,430 --> 01:34:25,473
which is a computer vision problem.

2412
01:34:26,370 --> 01:34:29,370
- What did you learn
about machine learning,

2413
01:34:29,370 --> 01:34:32,070
about engineering, about
life, about yourself

2414
01:34:32,070 --> 01:34:36,570
as one human being from
working with Elon Musk?

2415
01:34:36,570 --> 01:34:38,220
- I think the most I've learned is about

2416
01:34:38,220 --> 01:34:41,010
how to run organizations efficiently

2417
01:34:41,010 --> 01:34:43,620
and how to create efficient organizations

2418
01:34:43,620 --> 01:34:46,290
and how to fight entropy
in an organization.

2419
01:34:46,290 --> 01:34:49,230
- So human engineering in
the fight against entropy.

2420
01:34:49,230 --> 01:34:53,580
- Yeah, I think Elon is
a very efficient warrior

2421
01:34:53,580 --> 01:34:56,250
in the fight against
entropy in organizations.

2422
01:34:56,250 --> 01:34:58,950
- What does entropy in an
organization look like exactly?

2423
01:34:58,950 --> 01:35:00,270
- It's process.

2424
01:35:00,270 --> 01:35:03,420
It's process and it's-

2425
01:35:03,420 --> 01:35:05,310
- Inefficiencies in the form meetings

2426
01:35:05,310 --> 01:35:06,143
and that kind of stuff?

2427
01:35:06,143 --> 01:35:06,976
- Yeah.

2428
01:35:06,976 --> 01:35:08,430
Meetings, he hates meetings,
he keeps telling people

2429
01:35:08,430 --> 01:35:10,920
to skip meetings if they're not useful.

2430
01:35:10,920 --> 01:35:13,920
He basically runs the
world's biggest startups,

2431
01:35:13,920 --> 01:35:17,490
I would say, Tesla, SpaceX are
the world's biggest startups.

2432
01:35:17,490 --> 01:35:19,650
Tesla actually is multiple startups,

2433
01:35:19,650 --> 01:35:21,390
I think it's better to
look at it that way.

2434
01:35:21,390 --> 01:35:24,123
And so I think he's
extremely good at that.

2435
01:35:25,320 --> 01:35:27,870
And yeah, he is a very good intuition

2436
01:35:27,870 --> 01:35:28,710
for streamlining process.

2437
01:35:28,710 --> 01:35:30,300
He's making everything efficient.

2438
01:35:30,300 --> 01:35:32,250
Best part is no part, simplifying,

2439
01:35:32,250 --> 01:35:36,030
focusing, and just kind
of removing barriers,

2440
01:35:36,030 --> 01:35:38,070
moving very quickly, making big moves.

2441
01:35:38,070 --> 01:35:40,377
All this is very startupy
sort of seeming things

2442
01:35:40,377 --> 01:35:41,580
but at scale.

2443
01:35:41,580 --> 01:35:43,950
- So strong drive to simplify.

2444
01:35:43,950 --> 01:35:44,783
- [Andrej] Yeah.

2445
01:35:44,783 --> 01:35:45,616
- From your perspective, I mean,

2446
01:35:45,616 --> 01:35:49,257
that also probably applies
to just designing systems

2447
01:35:49,257 --> 01:35:50,970
and machine learning, and otherwise.

2448
01:35:50,970 --> 01:35:51,923
- Yeah.

2449
01:35:51,923 --> 01:35:52,756
- [Lex] like simplify, simplify.

2450
01:35:52,756 --> 01:35:53,589
- Yes.

2451
01:35:53,589 --> 01:35:55,890
- What do you think is
the secret to maintaining

2452
01:35:55,890 --> 01:35:59,190
the startup culture in
a company that grows?

2453
01:35:59,190 --> 01:36:02,433
Is there, can you introspect that?

2454
01:36:03,840 --> 01:36:06,420
- I do think he needs someone
in a powerful position

2455
01:36:06,420 --> 01:36:09,540
with a big hammer like
Elon who's the cheerleader

2456
01:36:09,540 --> 01:36:12,750
for that idea, and ruthlessly pursues it.

2457
01:36:12,750 --> 01:36:14,790
If no one has a big enough hammer,

2458
01:36:14,790 --> 01:36:17,160
everything turns into committees,

2459
01:36:17,160 --> 01:36:19,950
democracy within the company, process,

2460
01:36:19,950 --> 01:36:22,140
talking to stakeholders, decision-making,

2461
01:36:22,140 --> 01:36:23,820
Just everything just crumbles.

2462
01:36:23,820 --> 01:36:24,653
- [Lex] Yeah.

2463
01:36:24,653 --> 01:36:26,670
- If you have a big person
who is also really smart

2464
01:36:26,670 --> 01:36:28,953
and has a big hammer, things move quickly.

2465
01:36:29,940 --> 01:36:32,760
- So you said your favorite
scene in "Interstellar"

2466
01:36:32,760 --> 01:36:35,820
is the intense docking scene
with the AI and Cooper talking,

2467
01:36:35,820 --> 01:36:38,370
saying, "Cooper, what are you doing?

2468
01:36:38,370 --> 01:36:39,203
Docking.

2469
01:36:39,203 --> 01:36:40,380
It's not possible.

2470
01:36:40,380 --> 01:36:42,057
No, it's necessary."

2471
01:36:42,960 --> 01:36:45,630
Such a good line, by the way,
just so many questions there.

2472
01:36:45,630 --> 01:36:50,630
Why an AI in that scene presumably

2473
01:36:50,760 --> 01:36:55,260
is supposed to be able to
compute a lot more than the human

2474
01:36:55,260 --> 01:36:57,120
is saying it's not
optimal, why are the human,

2475
01:36:57,120 --> 01:37:00,450
I mean that's a movie,
but shouldn't the AI know

2476
01:37:00,450 --> 01:37:02,250
much better than the human?

2477
01:37:02,250 --> 01:37:04,410
Anyway, what do you think is the value

2478
01:37:04,410 --> 01:37:07,590
of setting seemingly impossible goals?

2479
01:37:07,590 --> 01:37:11,280
So like our initial intuition,

2480
01:37:11,280 --> 01:37:14,910
which seems like something
that you have taken on

2481
01:37:14,910 --> 01:37:19,410
that Elon espouses that
where the initial intuition

2482
01:37:19,410 --> 01:37:21,780
of the community might
say this is very difficult

2483
01:37:21,780 --> 01:37:24,870
and then you take it on
anyway, with a crazy deadline.

2484
01:37:24,870 --> 01:37:27,220
You, just from a human
engineering perspective,

2485
01:37:29,220 --> 01:37:30,820
have you seen the value of that?

2486
01:37:32,430 --> 01:37:34,710
- I wouldn't say that setting
impossible goals exactly

2487
01:37:34,710 --> 01:37:37,290
is a good idea but I think
setting very ambitious goals

2488
01:37:37,290 --> 01:37:38,370
is a good idea.

2489
01:37:38,370 --> 01:37:41,250
I think there's a, what
I call sublinear scaling

2490
01:37:41,250 --> 01:37:43,830
of difficulty, which
means that 10x problems

2491
01:37:43,830 --> 01:37:45,300
are not 10x hard.

2492
01:37:45,300 --> 01:37:49,560
Usually, 10x harder problem
is like two or three x

2493
01:37:49,560 --> 01:37:50,713
harder to execute on.

2494
01:37:50,713 --> 01:37:52,140
Because if you wanna actually,

2495
01:37:52,140 --> 01:37:54,480
like if you wanna improve a system by 10%,

2496
01:37:54,480 --> 01:37:55,680
it costs some amount of work.

2497
01:37:55,680 --> 01:37:57,480
And if you wanna 10x improve the system,

2498
01:37:57,480 --> 01:37:59,970
it doesn't cost you know,
a 100x amount of the work.

2499
01:37:59,970 --> 01:38:02,190
And it's because you
fundamentally change the approach.

2500
01:38:02,190 --> 01:38:04,530
And if you start with that constraint,

2501
01:38:04,530 --> 01:38:06,166
then some approaches are obviously dumb

2502
01:38:06,166 --> 01:38:09,200
and not going to work and
it forces you to reevaluate.

2503
01:38:09,200 --> 01:38:11,820
And I think it's a very interesting way

2504
01:38:11,820 --> 01:38:13,920
of approaching problem-solving.

2505
01:38:13,920 --> 01:38:15,757
- But it requires a
weird kind of thinking.

2506
01:38:15,757 --> 01:38:19,410
It's just going back to your PhD days.

2507
01:38:19,410 --> 01:38:23,220
It's like how do you think which ideas

2508
01:38:23,220 --> 01:38:27,480
in the machine learning
community are solvable?

2509
01:38:27,480 --> 01:38:28,404
- [Andrej] Yes.

2510
01:38:28,404 --> 01:38:30,450
- It requires, what is that?

2511
01:38:30,450 --> 01:38:33,000
I mean there's the cliche
of first principles thinking

2512
01:38:33,000 --> 01:38:35,430
but it requires to basically ignore

2513
01:38:35,430 --> 01:38:36,480
what the community is saying.

2514
01:38:36,480 --> 01:38:40,860
'Cause doesn't a community,
doesn't a community in science

2515
01:38:40,860 --> 01:38:44,130
usually draw lines of what
is and isn't possible?

2516
01:38:44,130 --> 01:38:44,963
- [Andrej] Right.

2517
01:38:44,963 --> 01:38:47,220
- And it's very hard to break out of that

2518
01:38:47,220 --> 01:38:48,540
without going crazy.

2519
01:38:48,540 --> 01:38:49,373
- Yeah.

2520
01:38:49,373 --> 01:38:50,550
I mean I think a good example here

2521
01:38:50,550 --> 01:38:52,890
is the deep learning
revolution in some sense

2522
01:38:52,890 --> 01:38:56,220
because you could be in
computer vision at that time,

2523
01:38:56,220 --> 01:39:00,360
during the deep learning
revolution of 2012 and so on.

2524
01:39:00,360 --> 01:39:02,800
You could be improving a
computer vision stack by 10%

2525
01:39:02,800 --> 01:39:05,940
or it can just be saying
actually all this is useless

2526
01:39:05,940 --> 01:39:07,920
and how do I do 10x
better computer vision?

2527
01:39:07,920 --> 01:39:11,100
Well, it's not probably by
tuning a HOG feature detector,

2528
01:39:11,100 --> 01:39:12,780
I need a different approach.

2529
01:39:12,780 --> 01:39:14,130
I need something that is scalable.

2530
01:39:14,130 --> 01:39:16,223
Going back to Richard Suttons

2531
01:39:17,250 --> 01:39:18,750
and understanding the philosophy

2532
01:39:18,750 --> 01:39:21,630
of the Bitter Lesson and then being like

2533
01:39:21,630 --> 01:39:22,997
actually, I need a much
more scalable system,

2534
01:39:22,997 --> 01:39:25,710
like a neural network
that in principle works.

2535
01:39:25,710 --> 01:39:28,020
And then having some deep
believers that can actually

2536
01:39:28,020 --> 01:39:29,490
execute on that mission, make it work.

2537
01:39:29,490 --> 01:39:31,683
So that's the 10x solution.

2538
01:39:33,960 --> 01:39:36,810
- What do you think is the
timeline to solve the problem

2539
01:39:36,810 --> 01:39:38,520
of autonomous driving?

2540
01:39:38,520 --> 01:39:41,583
That's still in part an open question.

2541
01:39:42,600 --> 01:39:44,580
- Yeah, I think the tough
thing with timelines

2542
01:39:44,580 --> 01:39:46,740
of self-driving obviously, is that no one

2543
01:39:46,740 --> 01:39:48,030
has created self-driving.

2544
01:39:48,030 --> 01:39:48,863
- [Lex] Yeah.

2545
01:39:48,863 --> 01:39:51,390
- So it's not like, what
do you think is a timeline

2546
01:39:51,390 --> 01:39:52,223
to build this bridge?

2547
01:39:52,223 --> 01:39:54,060
Well, we've built a
million bridges before,

2548
01:39:54,060 --> 01:39:55,410
here's how long that takes.

2549
01:39:57,210 --> 01:40:00,210
No one has built autonomy,
it's not obvious.

2550
01:40:00,210 --> 01:40:02,513
Some parts turn out to be
much easier than others.

2551
01:40:02,513 --> 01:40:04,050
So it's really hard to forecast.

2552
01:40:04,050 --> 01:40:06,810
You do your best based
on trend lines and so on

2553
01:40:06,810 --> 01:40:07,740
and based on intuition.

2554
01:40:07,740 --> 01:40:09,900
But that's why fundamentally
it's just really hard

2555
01:40:09,900 --> 01:40:10,890
to forecast this.

2556
01:40:10,890 --> 01:40:11,723
No one has-

2557
01:40:11,723 --> 01:40:13,110
- So even still like being inside of it,

2558
01:40:13,110 --> 01:40:15,060
it's hard to-

2559
01:40:15,060 --> 01:40:16,010
- Yes.

2560
01:40:16,010 --> 01:40:16,916
Some things turn out to be much harder

2561
01:40:16,916 --> 01:40:18,990
and some things turned
out to be much easier.

2562
01:40:18,990 --> 01:40:21,960
- Do you try to avoid making forecasts?

2563
01:40:21,960 --> 01:40:24,180
'Cause Elon doesn't avoid them, right?

2564
01:40:24,180 --> 01:40:26,370
And heads of car companies in the past

2565
01:40:26,370 --> 01:40:28,113
have not avoided it either.

2566
01:40:29,130 --> 01:40:31,500
Ford and other places
have made predictions

2567
01:40:31,500 --> 01:40:33,990
that we're gonna solve level-four driving

2568
01:40:33,990 --> 01:40:36,600
by 2020, 2021, whatever.

2569
01:40:36,600 --> 01:40:39,063
And they all backtrack on that prediction.

2570
01:40:40,460 --> 01:40:45,460
As an AI person, do you
feel yourself privately

2571
01:40:47,220 --> 01:40:49,970
make predictions or do they get in the way

2572
01:40:49,970 --> 01:40:53,940
of your actual ability
to think about a thing?

2573
01:40:53,940 --> 01:40:55,920
- Yeah, I would say what's easy to say

2574
01:40:55,920 --> 01:40:57,630
is that this problem is tractable

2575
01:40:57,630 --> 01:40:59,316
and that's an easy prediction to make.

2576
01:40:59,316 --> 01:41:00,149
It's tractable-

2577
01:41:00,149 --> 01:41:00,982
- So it's solvable?

2578
01:41:00,982 --> 01:41:01,815
- It's going to work.

2579
01:41:01,815 --> 01:41:02,648
Yes.

2580
01:41:02,648 --> 01:41:03,481
It's just really hard.

2581
01:41:03,481 --> 01:41:04,314
Some things turned out to be harder

2582
01:41:04,314 --> 01:41:05,243
and somethings turn out to be easier.

2583
01:41:06,450 --> 01:41:08,220
But it definitely feels tractable

2584
01:41:08,220 --> 01:41:10,530
and it feels like, at
least the team at Tesla,

2585
01:41:10,530 --> 01:41:11,760
which is what I saw internally,

2586
01:41:11,760 --> 01:41:13,320
is definitely on track to that.

2587
01:41:13,320 --> 01:41:17,670
- How do you form a strong representation

2588
01:41:17,670 --> 01:41:20,640
that allows you to make a
prediction about tractability?

2589
01:41:20,640 --> 01:41:23,733
So you're the leader a lot of humans,

2590
01:41:24,750 --> 01:41:27,813
you have to say this is actually possible.

2591
01:41:28,969 --> 01:41:29,802
- Yeah.

2592
01:41:29,802 --> 01:41:31,020
- How do you build up that intuition?

2593
01:41:31,020 --> 01:41:32,460
It doesn't have to be even driving,

2594
01:41:32,460 --> 01:41:33,600
it could be other tasks.

2595
01:41:33,600 --> 01:41:34,637
- [Andrej] Right.

2596
01:41:34,637 --> 01:41:37,520
- It could be, what difficult
tasks did you work on

2597
01:41:37,520 --> 01:41:38,353
in your life?

2598
01:41:38,353 --> 01:41:41,160
I mean classification, achieving certain,

2599
01:41:41,160 --> 01:41:43,500
just an image at certain level

2600
01:41:43,500 --> 01:41:45,840
of superhuman-level performance.

2601
01:41:45,840 --> 01:41:47,880
- Yeah, expert intuition.

2602
01:41:47,880 --> 01:41:49,530
It's just intuition, it's belief.

2603
01:41:50,910 --> 01:41:53,400
- So just like thinking about
it long enough, like studying,

2604
01:41:53,400 --> 01:41:55,893
looking at sample data,
like you said, driving.

2605
01:41:56,850 --> 01:41:58,533
My intuition is really flawed on this.

2606
01:41:58,533 --> 01:42:01,543
I don't have a good
intuition about tractability.

2607
01:42:01,543 --> 01:42:03,300
It could be anything.

2608
01:42:03,300 --> 01:42:05,433
It could be solvable.

2609
01:42:07,980 --> 01:42:10,890
The driving task could be simplified

2610
01:42:10,890 --> 01:42:12,960
into something quite trivial.

2611
01:42:12,960 --> 01:42:16,290
Like the solution to the
problem would be quite trivial.

2612
01:42:16,290 --> 01:42:20,430
And at scale, more and
more cars driving perfectly

2613
01:42:20,430 --> 01:42:22,620
might make the problem much easier.

2614
01:42:22,620 --> 01:42:23,970
The more cars you have driving,

2615
01:42:23,970 --> 01:42:27,420
like people learn how to drive
correctly, not correctly,

2616
01:42:27,420 --> 01:42:32,220
but in a way that's more
optimal for heterogeneous system

2617
01:42:32,220 --> 01:42:34,590
of autonomous, and semi-autonomous,

2618
01:42:34,590 --> 01:42:37,170
and manually driven cars,
that could change stuff.

2619
01:42:37,170 --> 01:42:40,590
Then again, also I've spent
a ridiculous number of hours

2620
01:42:40,590 --> 01:42:43,680
just staring at pedestrians
crossing streets,

2621
01:42:43,680 --> 01:42:45,390
thinking about humans.

2622
01:42:45,390 --> 01:42:50,280
And it feels like the way
we use our eye contact,

2623
01:42:50,280 --> 01:42:54,120
it sends really strong signals
and there's certain quirks

2624
01:42:54,120 --> 01:42:55,560
and edge cases of behavior.

2625
01:42:55,560 --> 01:42:57,690
And of course, a lot of
the fatalities that happen

2626
01:42:57,690 --> 01:42:59,943
have to do with drunk driving,

2627
01:43:01,380 --> 01:43:03,360
both on the pedestrian
side and the driver's side.

2628
01:43:03,360 --> 01:43:05,880
So there's that problem
of driving at night

2629
01:43:05,880 --> 01:43:06,713
and all that kind of.

2630
01:43:06,713 --> 01:43:07,623
- [Andrej] Yeah.

2631
01:43:07,623 --> 01:43:11,460
- So I wonder, it's like the
space of possible solution

2632
01:43:11,460 --> 01:43:15,720
into autonomous driving includes
so many human factor issues

2633
01:43:15,720 --> 01:43:18,000
that it's almost impossible to predict.

2634
01:43:18,000 --> 01:43:20,910
There could be super
clean, nice solutions.

2635
01:43:20,910 --> 01:43:21,743
- Yeah.

2636
01:43:21,743 --> 01:43:23,490
I would say definitely,
to use a game analogy,

2637
01:43:23,490 --> 01:43:26,490
there's some fog of war,
but you definitely also see

2638
01:43:26,490 --> 01:43:29,730
the frontier of improvement and
you can measure historically

2639
01:43:29,730 --> 01:43:31,380
how much you've made progress.

2640
01:43:31,380 --> 01:43:33,300
And I think for example,
at least what I've seen

2641
01:43:33,300 --> 01:43:35,430
in roughly five years at Tesla.

2642
01:43:35,430 --> 01:43:39,000
When I joined it barely
kept lane on the highway.

2643
01:43:39,000 --> 01:43:40,770
I think going up from Palo Alto to SF

2644
01:43:40,770 --> 01:43:42,210
was like three or four interventions.

2645
01:43:42,210 --> 01:43:44,700
Anytime the road would
do anything geometrically

2646
01:43:44,700 --> 01:43:47,100
or turn too much it would just not work.

2647
01:43:47,100 --> 01:43:49,350
And so going from that to
like a pretty competent system

2648
01:43:49,350 --> 01:43:51,930
in five years and seeing what
happens also under the hood

2649
01:43:51,930 --> 01:43:54,270
and what the scale which
the team is operating now

2650
01:43:54,270 --> 01:43:57,000
with respect to data, and
compute, and everything else

2651
01:43:57,000 --> 01:43:59,133
is just massive progress.

2652
01:44:00,363 --> 01:44:03,900
- So you're climbing a
mountain and it's fog

2653
01:44:03,900 --> 01:44:05,340
but you're making a lot of progress.

2654
01:44:05,340 --> 01:44:06,173
- It's Fog.

2655
01:44:06,173 --> 01:44:07,006
You're making progress and you see

2656
01:44:07,006 --> 01:44:07,980
what the next directions are

2657
01:44:07,980 --> 01:44:09,570
and you're looking at some
of the remaining challenges

2658
01:44:09,570 --> 01:44:12,360
and they're not perturbing you,

2659
01:44:12,360 --> 01:44:13,650
and they're not changing your philosophy,

2660
01:44:13,650 --> 01:44:15,810
and you're not contorting yourself.

2661
01:44:15,810 --> 01:44:17,460
You're like, actually,
these are the things

2662
01:44:17,460 --> 01:44:18,319
that we still need to do.

2663
01:44:18,319 --> 01:44:19,410
- Yeah, it's the fundamental components

2664
01:44:19,410 --> 01:44:20,970
of solving the problem seem to be there,

2665
01:44:20,970 --> 01:44:22,620
from the data engine, to the compute,

2666
01:44:22,620 --> 01:44:25,590
to the compute on the car, to
the compute for the training,

2667
01:44:25,590 --> 01:44:26,423
all that kind of stuff.

2668
01:44:26,423 --> 01:44:27,256
- [Andrej] Yes.

2669
01:44:29,280 --> 01:44:30,593
- Over the years you've been at Tesla,

2670
01:44:30,593 --> 01:44:33,780
you've done a lot of
amazing breakthrough ideas

2671
01:44:33,780 --> 01:44:38,040
and engineering all of
it from the data engine

2672
01:44:38,040 --> 01:44:40,230
to the human side, all of it.

2673
01:44:40,230 --> 01:44:43,920
Can you speak to why you
chose to leave Tesla?

2674
01:44:43,920 --> 01:44:47,010
- Basically, as I
described, I think over time

2675
01:44:47,010 --> 01:44:49,740
during those five years
I've kind of gotten myself

2676
01:44:49,740 --> 01:44:52,500
into a little bit of
a managerial position.

2677
01:44:52,500 --> 01:44:54,000
Most of my days were meetings,

2678
01:44:54,000 --> 01:44:57,780
and growing the organization,
and making decisions about,

2679
01:44:57,780 --> 01:45:00,330
high-level strategic
decisions about the team

2680
01:45:00,330 --> 01:45:02,430
and what it should be
working on, and so on.

2681
01:45:02,430 --> 01:45:06,150
And it's kind of like a
corporate executive role.

2682
01:45:06,150 --> 01:45:06,983
And I can do it.

2683
01:45:06,983 --> 01:45:08,760
I think I'm okay at it.

2684
01:45:08,760 --> 01:45:11,100
But it's not like
fundamentally what I enjoy.

2685
01:45:11,100 --> 01:45:14,040
And so I think when I joined,

2686
01:45:14,040 --> 01:45:15,090
there was no computer vision team

2687
01:45:15,090 --> 01:45:17,070
because Tesla was just
going from the transition

2688
01:45:17,070 --> 01:45:18,780
of using MobilEye, a third-party vendor,

2689
01:45:18,780 --> 01:45:19,770
for all of its computer vision

2690
01:45:19,770 --> 01:45:21,870
to having to build its
computer vision system.

2691
01:45:21,870 --> 01:45:23,220
So when I showed up, there were two people

2692
01:45:23,220 --> 01:45:24,930
training deep neural networks.

2693
01:45:24,930 --> 01:45:26,684
And they were training them at a computer

2694
01:45:26,684 --> 01:45:30,684
at their legs, like
down, there was a work.

2695
01:45:30,684 --> 01:45:32,460
- There was some kind of
basic classification task.

2696
01:45:32,460 --> 01:45:33,293
- Yeah.

2697
01:45:33,293 --> 01:45:36,200
And so I like grew that into what I think

2698
01:45:36,200 --> 01:45:38,820
is a fairly respectable
deep learning team,

2699
01:45:38,820 --> 01:45:40,050
a massive computer cluster,

2700
01:45:40,050 --> 01:45:42,930
a very good data annotation organization.

2701
01:45:42,930 --> 01:45:45,300
And I was very happy with where that was.

2702
01:45:45,300 --> 01:45:48,330
It became quite autonomous
and so I stepped away

2703
01:45:48,330 --> 01:45:52,350
and I'm very excited to do much
more technical things again.

2704
01:45:52,350 --> 01:45:53,183
Yeah.

2705
01:45:53,183 --> 01:45:54,243
And kind of like we focus on AGI.

2706
01:45:54,243 --> 01:45:56,147
- What was that soul searching like?

2707
01:45:56,147 --> 01:45:58,493
'Cause you took a little
time off, I think,

2708
01:45:58,493 --> 01:46:00,510
how many mushrooms did you take?

2709
01:46:00,510 --> 01:46:01,763
No, I'm just kidding.

2710
01:46:01,763 --> 01:46:03,780
I mean, what was going through your mind?

2711
01:46:03,780 --> 01:46:06,180
The human lifetime is finite.

2712
01:46:06,180 --> 01:46:07,013
- [Andrej] Yeah.

2713
01:46:07,013 --> 01:46:08,280
- You did a few incredible things.

2714
01:46:08,280 --> 01:46:11,550
You're one of the best
teachers of AI in the world.

2715
01:46:11,550 --> 01:46:13,470
You're one of the best.

2716
01:46:13,470 --> 01:46:16,123
And I mean that in the best possible way.

2717
01:46:16,123 --> 01:46:19,740
You're one of the best
tinkerers in the AI world.

2718
01:46:19,740 --> 01:46:22,150
Meaning like understanding
the fundamentals

2719
01:46:23,304 --> 01:46:26,370
of how something works by
building it from scratch

2720
01:46:26,370 --> 01:46:28,560
and playing with the basic intuitions.

2721
01:46:28,560 --> 01:46:30,390
It's like Einstein, Fineman,

2722
01:46:30,390 --> 01:46:32,100
were all really good
at this kind of stuff.

2723
01:46:32,100 --> 01:46:35,220
Like small example of a
thing, to play with it,

2724
01:46:35,220 --> 01:46:37,050
to try to understand it.

2725
01:46:37,050 --> 01:46:38,970
So that, and obviously now with Tesla,

2726
01:46:38,970 --> 01:46:42,340
you helped build a team
of machine learning

2727
01:46:44,580 --> 01:46:47,070
engineers and a system that
actually accomplishes something

2728
01:46:47,070 --> 01:46:47,970
in the real-world.

2729
01:46:47,970 --> 01:46:51,864
So given all that, what was
the soul searching like?

2730
01:46:51,864 --> 01:46:53,430
- Well, it was hard because obviously,

2731
01:46:53,430 --> 01:46:57,273
I love the company a lot, and
I love Elon, I love Tesla,

2732
01:46:59,694 --> 01:47:00,527
so it was hard to leave.

2733
01:47:00,527 --> 01:47:01,500
I love the team, basically.

2734
01:47:02,720 --> 01:47:05,340
But yeah, I think I actually,

2735
01:47:05,340 --> 01:47:07,950
I really potentially
interested in revisiting it,

2736
01:47:07,950 --> 01:47:11,430
maybe coming back at some
point, working in Optimus,

2737
01:47:11,430 --> 01:47:13,500
working in AGI at Tesla.

2738
01:47:13,500 --> 01:47:15,870
I think Tesla's going
to do incredible things.

2739
01:47:15,870 --> 01:47:20,870
It's basically a massive
large-scale robotics

2740
01:47:21,780 --> 01:47:23,610
kind of company with a
ton of in-house talent

2741
01:47:23,610 --> 01:47:25,110
for doing real incredible things.

2742
01:47:25,110 --> 01:47:29,280
And I think human robots
are going to be amazing.

2743
01:47:29,280 --> 01:47:31,920
I think autonomous transportation
is going to be amazing.

2744
01:47:31,920 --> 01:47:32,940
All this is happening at Tesla.

2745
01:47:32,940 --> 01:47:35,550
So I think it's just a
really amazing organization.

2746
01:47:35,550 --> 01:47:37,110
So being part of it and helping it along

2747
01:47:37,110 --> 01:47:39,960
I think was very, basically,
I enjoyed that a lot.

2748
01:47:39,960 --> 01:47:41,580
Yeah, it was basically
difficult for those reasons

2749
01:47:41,580 --> 01:47:43,170
because I love the company.

2750
01:47:43,170 --> 01:47:45,420
But I'm happy to potentially at some point

2751
01:47:45,420 --> 01:47:46,830
come back for act two.

2752
01:47:46,830 --> 01:47:49,680
But I felt like at this
stage I built the team,

2753
01:47:49,680 --> 01:47:53,250
it felt autonomous and I became a manager

2754
01:47:53,250 --> 01:47:54,810
and I wanted to do a lot
more technical stuff.

2755
01:47:54,810 --> 01:47:55,680
I wanted to learn stuff.

2756
01:47:55,680 --> 01:47:57,420
I wanted to teach stuff.

2757
01:47:57,420 --> 01:48:00,066
And I just felt like it was a good time

2758
01:48:00,066 --> 01:48:01,620
for a change of pace a little bit.

2759
01:48:01,620 --> 01:48:05,160
- What do you think is
the best movie sequel

2760
01:48:05,160 --> 01:48:07,200
of all time speaking of part two?

2761
01:48:07,200 --> 01:48:09,133
'Cause most of 'em suck.

2762
01:48:09,133 --> 01:48:09,966
- Movie sequels?

2763
01:48:09,966 --> 01:48:10,980
- Movie sequels, yeah.

2764
01:48:10,980 --> 01:48:12,300
And you Tweet about movies.

2765
01:48:12,300 --> 01:48:17,300
So just a tiny tangent, what's
a favorite movie sequel?

2766
01:48:18,397 --> 01:48:19,677
"Godfather Part II"?

2767
01:48:20,700 --> 01:48:21,600
Are you a fan of "Godfather"?

2768
01:48:21,600 --> 01:48:23,670
'Cause you didn't even Tweet
or mention "The Godfather".

2769
01:48:23,670 --> 01:48:24,630
- Yeah, I don't love that movie.

2770
01:48:24,630 --> 01:48:25,619
I know it has a-

2771
01:48:25,619 --> 01:48:26,452
- We're gonna edit that out.

2772
01:48:26,452 --> 01:48:28,890
We're gonna edit out the
hate towards "The Godfather".

2773
01:48:28,890 --> 01:48:30,077
How dare you disrespect?

2774
01:48:30,077 --> 01:48:31,280
- I think I will make a strong statement.

2775
01:48:31,280 --> 01:48:34,890
I don't know why but I
basically don't like any movie

2776
01:48:34,890 --> 01:48:38,280
before 1995, something like that.

2777
01:48:38,280 --> 01:48:40,317
- Didn't you mention "Terminator 2".

2778
01:48:40,317 --> 01:48:41,150
- Okay.

2779
01:48:41,150 --> 01:48:41,983
Okay.

2780
01:48:41,983 --> 01:48:43,830
That's like a, "Terminator
2" was a little bit later.

2781
01:48:43,830 --> 01:48:45,756
1990...

2782
01:48:45,756 --> 01:48:47,754
- No, I think "Terminator
2" was in the eighties.

2783
01:48:47,754 --> 01:48:48,747
- And I like "Terminator" one as well,

2784
01:48:48,747 --> 01:48:49,580
So, okay.

2785
01:48:49,580 --> 01:48:51,540
So a few exceptions but by and large

2786
01:48:51,540 --> 01:48:53,100
for some reason, I don't like movies

2787
01:48:53,100 --> 01:48:55,470
before 1995 or something.

2788
01:48:55,470 --> 01:48:56,850
They feel very slow.

2789
01:48:56,850 --> 01:48:58,110
The camera is like zoomed out.

2790
01:48:58,110 --> 01:49:00,930
It's boring, it's kind of
naive, it's kind of weird.

2791
01:49:00,930 --> 01:49:03,990
- And also Terminator was
very much ahead of its time.

2792
01:49:03,990 --> 01:49:04,823
- Yes.

2793
01:49:04,823 --> 01:49:06,633
And "The Godfather", there's like no AGI.

2794
01:49:10,290 --> 01:49:12,150
- I mean but you have, "Good Will Hunting"

2795
01:49:12,150 --> 01:49:13,950
was one of the movies you mentioned

2796
01:49:13,950 --> 01:49:15,720
and that doesn't have any AGI either.

2797
01:49:15,720 --> 01:49:16,950
I guess it has mathematics.

2798
01:49:16,950 --> 01:49:19,080
- Yeah, I guess occasionally,
I do enjoy movies

2799
01:49:19,080 --> 01:49:20,610
that don't feature.

2800
01:49:20,610 --> 01:49:23,077
- Or like "Anchorman" that has no, that's.

2801
01:49:23,077 --> 01:49:24,690
- "Anchorman" is so good.

2802
01:49:24,690 --> 01:49:28,487
- I don't understand, speaking of AGI,

2803
01:49:28,487 --> 01:49:31,950
'cause I don't understand
why Will Ferrell is so funny.

2804
01:49:31,950 --> 01:49:32,783
It doesn't make sense.

2805
01:49:32,783 --> 01:49:33,690
It doesn't compute.

2806
01:49:33,690 --> 01:49:35,400
There's just something about him.

2807
01:49:35,400 --> 01:49:36,593
And he's a singular human.

2808
01:49:36,593 --> 01:49:39,930
'Cause, you don't get that
many comedies these days.

2809
01:49:39,930 --> 01:49:42,450
And I wonder if it has
to do about the culture

2810
01:49:42,450 --> 01:49:45,123
or the Machine of Hollywood
or does it have to do with

2811
01:49:45,123 --> 01:49:48,090
just we got lucky with
certain people in comedy

2812
01:49:48,090 --> 01:49:49,900
that came together, 'cause
he is a singular human.

2813
01:49:49,900 --> 01:49:51,393
- Yeah, yeah.

2814
01:49:51,393 --> 01:49:52,950
I love his movies.

2815
01:49:52,950 --> 01:49:55,440
- That was a ridiculous
tangent, I apologize.

2816
01:49:55,440 --> 01:49:57,330
But you mentioned humanoid robots.

2817
01:49:57,330 --> 01:50:00,600
So what do you think about
Optimus, about Tesla Bot?

2818
01:50:00,600 --> 01:50:02,520
Do you think we'll have
robots in the factory

2819
01:50:02,520 --> 01:50:05,970
and in the home in 10,
20, 30, 40, 50 years?

2820
01:50:05,970 --> 01:50:06,803
- Yeah.

2821
01:50:06,803 --> 01:50:07,740
I think it's a very hard project.

2822
01:50:07,740 --> 01:50:09,030
I think it's going to take a while,

2823
01:50:09,030 --> 01:50:11,940
but who else is going to
build human robots at scale?

2824
01:50:11,940 --> 01:50:12,773
- [Lex] Yeah.

2825
01:50:12,773 --> 01:50:14,220
- And I think it is a very
good form factor to go after

2826
01:50:14,220 --> 01:50:16,380
because like I mentioned
the world is designed

2827
01:50:16,380 --> 01:50:17,760
for humanoid form factor.

2828
01:50:17,760 --> 01:50:19,740
These things would be able
to operate our machines.

2829
01:50:19,740 --> 01:50:22,110
They would be able to sit down in chairs,

2830
01:50:22,110 --> 01:50:24,240
potentially even drive cars.

2831
01:50:24,240 --> 01:50:25,890
Basically, the world
is designed for humans,

2832
01:50:25,890 --> 01:50:27,840
that's the form factor
you want to invest into

2833
01:50:27,840 --> 01:50:29,460
and make work over time.

2834
01:50:29,460 --> 01:50:31,800
I think, there's another
school of thought which is,

2835
01:50:31,800 --> 01:50:34,380
okay, pick a problem and
design a robot to it.

2836
01:50:34,380 --> 01:50:35,490
But actually, designing a robot

2837
01:50:35,490 --> 01:50:36,540
and getting a whole data engine

2838
01:50:36,540 --> 01:50:37,950
and everything behind it to work

2839
01:50:37,950 --> 01:50:39,870
is actually an incredibly hard problem.

2840
01:50:39,870 --> 01:50:41,940
So it makes sense to go
after general interfaces

2841
01:50:41,940 --> 01:50:44,940
that, okay, they are not
perfect for any one given task,

2842
01:50:44,940 --> 01:50:48,000
but they actually have the
generality of just with a prompt,

2843
01:50:48,000 --> 01:50:51,360
with English, able to do something across.

2844
01:50:51,360 --> 01:50:52,410
And so I think it makes a lot of sense

2845
01:50:52,410 --> 01:50:57,410
to go after a general interface
in the physical world.

2846
01:50:58,440 --> 01:51:00,090
And I think it's a very difficult project.

2847
01:51:00,090 --> 01:51:03,990
It's going to take time, but
I've seen no other company

2848
01:51:03,990 --> 01:51:05,010
that can execute on that vision.

2849
01:51:05,010 --> 01:51:07,410
I think it's going to be amazing.

2850
01:51:07,410 --> 01:51:09,480
Basically, physical labor,
if you think transportation

2851
01:51:09,480 --> 01:51:12,281
is a large market, try physical labor.

2852
01:51:12,281 --> 01:51:13,707
It's insane.

2853
01:51:13,707 --> 01:51:16,140
- But it's not just physical labor, to me,

2854
01:51:16,140 --> 01:51:18,690
the thing that's also exciting
is the social robotics.

2855
01:51:18,690 --> 01:51:21,660
So the relationship we'll
have on different levels

2856
01:51:21,660 --> 01:51:23,070
with those robots.

2857
01:51:23,070 --> 01:51:23,903
- Yeah.

2858
01:51:23,903 --> 01:51:26,523
- That's why I was really
excited to see Optimus.

2859
01:51:27,870 --> 01:51:30,930
People have criticized
me for the excitement,

2860
01:51:30,930 --> 01:51:34,680
but I've worked with
a lot of research labs

2861
01:51:34,680 --> 01:51:39,680
that do humanoid-legged robots,
Boston Dynamics, Unitree.

2862
01:51:39,990 --> 01:51:42,690
There's a lot of companies
that do legged robots,

2863
01:51:42,690 --> 01:51:47,690
but that's the elegance of the movement

2864
01:51:47,850 --> 01:51:51,690
is a tiny, tiny part of the big picture.

2865
01:51:51,690 --> 01:51:55,020
So the two big exciting
things to me about Tesla

2866
01:51:55,020 --> 01:51:58,380
doing humanoid or any legged robots

2867
01:51:58,380 --> 01:52:03,030
is clearly integrating
into the data engine.

2868
01:52:03,030 --> 01:52:06,870
So the data engine aspect,
so the actual intelligence

2869
01:52:06,870 --> 01:52:09,690
for the perception and the
control and the planning

2870
01:52:09,690 --> 01:52:10,800
and all that kind of stuff.

2871
01:52:10,800 --> 01:52:13,260
Integrating into this huge,
the fleet that you mentioned.

2872
01:52:13,260 --> 01:52:14,610
Right.

2873
01:52:14,610 --> 01:52:18,390
And then speaking of
fleet, the second thing is

2874
01:52:18,390 --> 01:52:20,140
the mass manufacturers just knowing

2875
01:52:21,390 --> 01:52:26,390
culturally driving towards a simple robot

2876
01:52:26,880 --> 01:52:29,130
that's cheap to produce at scale.

2877
01:52:29,130 --> 01:52:30,190
- [Andrej] Yeah.

2878
01:52:30,190 --> 01:52:31,590
- And doing that well, having
experience to do that well,

2879
01:52:31,590 --> 01:52:32,490
that changes everything.

2880
01:52:32,490 --> 01:52:34,980
That's why that's a very different culture

2881
01:52:34,980 --> 01:52:36,600
and style than Boston Dynamics.

2882
01:52:36,600 --> 01:52:39,750
Who by the way, those robots are just-

2883
01:52:39,750 --> 01:52:44,490
The way they move, it'll
be a very long time

2884
01:52:44,490 --> 01:52:47,220
before Tesla could achieve
the smoothness of movement.

2885
01:52:47,220 --> 01:52:49,234
But that's not what it's about.

2886
01:52:49,234 --> 01:52:52,320
It's about the entirety of the system.

2887
01:52:52,320 --> 01:52:54,325
Like we talked about the
data engine and the fleet.

2888
01:52:54,325 --> 01:52:55,158
- [Andrej] Right.

2889
01:52:55,158 --> 01:52:55,991
- And that's super exciting,

2890
01:52:55,991 --> 01:52:58,260
even the initial sort of models.

2891
01:52:58,260 --> 01:53:00,570
But that too was really surprising

2892
01:53:00,570 --> 01:53:03,630
that in a few months you
can get a pro a prototype.

2893
01:53:03,630 --> 01:53:04,463
- Yep.

2894
01:53:04,463 --> 01:53:06,210
And the reason that
happened very quickly is,

2895
01:53:06,210 --> 01:53:08,520
as you alluded to, there's
a ton of copy-paste

2896
01:53:08,520 --> 01:53:09,960
from what's happening in the autopilot.

2897
01:53:09,960 --> 01:53:10,860
A lot.

2898
01:53:10,860 --> 01:53:12,930
The amount of expertise that
came out of the woodworks

2899
01:53:12,930 --> 01:53:15,930
at Tesla for building the human
robot was incredible to see.

2900
01:53:17,160 --> 01:53:19,350
Basically, Elon said at
one point we're doing this

2901
01:53:19,350 --> 01:53:23,070
and then next day, basically,
all these CAD models

2902
01:53:23,070 --> 01:53:25,500
started to appear and people
talk about the supply chain

2903
01:53:25,500 --> 01:53:26,640
and manufacturing.

2904
01:53:26,640 --> 01:53:27,473
- [Lex] Yeah.

2905
01:53:27,473 --> 01:53:29,550
- And people showed up with
Screwdrivers and everything

2906
01:53:29,550 --> 01:53:32,160
the other day and started to
like put together the body.

2907
01:53:32,160 --> 01:53:33,090
And I was like, whoa.

2908
01:53:33,090 --> 01:53:34,650
All these people exist at Tesla.

2909
01:53:34,650 --> 01:53:35,790
And fundamentally building a car

2910
01:53:35,790 --> 01:53:37,727
is actually not that different
from building a robot.

2911
01:53:37,727 --> 01:53:41,640
And that is true not just
for the hardware pieces.

2912
01:53:41,640 --> 01:53:44,610
And also let's not forget
hardware, not just for a demo,

2913
01:53:44,610 --> 01:53:48,870
but manufacturing of
that hardware at scale

2914
01:53:48,870 --> 01:53:50,340
is a whole different thing.

2915
01:53:50,340 --> 01:53:52,230
But for software as well,

2916
01:53:52,230 --> 01:53:54,730
basically, this robot
currently thinks it's a car.

2917
01:53:56,580 --> 01:53:59,460
- It's gonna have a midlife
crisis at some point.

2918
01:53:59,460 --> 01:54:01,050
- It thinks it's a car.

2919
01:54:01,050 --> 01:54:03,030
Some of the earlier demos,
actually, we were talking about

2920
01:54:03,030 --> 01:54:04,800
potentially doing them
outside in the parking lot

2921
01:54:04,800 --> 01:54:06,660
because that's where all
of the computer vision

2922
01:54:06,660 --> 01:54:08,280
was working out of the box.

2923
01:54:08,280 --> 01:54:09,180
- [Lex] That's Funny.

2924
01:54:09,180 --> 01:54:10,593
- Instead of inside.

2925
01:54:11,700 --> 01:54:14,700
But all the operating system,
everything just copy-pastes,

2926
01:54:14,700 --> 01:54:15,827
computer vision mostly copy-paste.

2927
01:54:15,827 --> 01:54:17,520
I mean you have to
retrain the neural nets,

2928
01:54:17,520 --> 01:54:19,110
but the approach and
everything and data engine

2929
01:54:19,110 --> 01:54:21,060
and offline trackers
and the way we go about

2930
01:54:21,060 --> 01:54:22,350
the occupancy tracker and so on.

2931
01:54:22,350 --> 01:54:23,640
Everything copy-paste,

2932
01:54:23,640 --> 01:54:25,950
you just need to retrain the neural nets.

2933
01:54:25,950 --> 01:54:27,450
And then the planning control, of course,

2934
01:54:27,450 --> 01:54:28,680
has to change quite a bit,

2935
01:54:28,680 --> 01:54:30,360
but there's a ton of copy-paste

2936
01:54:30,360 --> 01:54:31,470
from what's happening at Tesla.

2937
01:54:31,470 --> 01:54:34,500
And if you were to go
with goal of like, okay,

2938
01:54:34,500 --> 01:54:37,380
let's build a million human
robots and you're not Tesla,

2939
01:54:37,380 --> 01:54:39,797
that's a lot to ask, if you're at Tesla,

2940
01:54:39,797 --> 01:54:42,870
it's actually like, that's not that crazy.

2941
01:54:42,870 --> 01:54:44,790
- And then the follow-up question is then

2942
01:54:44,790 --> 01:54:46,320
how difficult, just like with driving,

2943
01:54:46,320 --> 01:54:48,450
how difficult is the manipulation task?

2944
01:54:48,450 --> 01:54:49,283
- [Andrej] Yep.

2945
01:54:49,283 --> 01:54:51,090
- Such that it can have
an impact at scale?

2946
01:54:51,090 --> 01:54:53,700
I think depending on the context,

2947
01:54:53,700 --> 01:54:57,540
the really nice thing
about robotics is that,

2948
01:54:57,540 --> 01:55:00,210
unless you do a manufacturer
and that kind of stuff,

2949
01:55:00,210 --> 01:55:02,130
is there is more room for error?

2950
01:55:02,130 --> 01:55:02,963
- [Andrej] Yep.

2951
01:55:02,963 --> 01:55:06,330
- Driving is so safety-critical
and also time critical,

2952
01:55:06,330 --> 01:55:10,050
a robot is allowed to move
slower, which is nice.

2953
01:55:10,050 --> 01:55:11,040
- Yes.

2954
01:55:11,040 --> 01:55:12,600
I think it's going to take a long time.

2955
01:55:12,600 --> 01:55:14,490
But the way you want to
structure the development

2956
01:55:14,490 --> 01:55:16,650
is you need to say, okay, it's
going to take a long time.

2957
01:55:16,650 --> 01:55:20,730
How can I set up the
product development roadmap

2958
01:55:20,730 --> 01:55:22,200
so that I'm making revenue along the way?

2959
01:55:22,200 --> 01:55:24,450
I'm not setting myself up
for a zero-one-loss function

2960
01:55:24,450 --> 01:55:26,100
where it doesn't work until it works.

2961
01:55:26,100 --> 01:55:27,420
You don't wanna be in that position.

2962
01:55:27,420 --> 01:55:29,610
You want to make it
useful almost immediately.

2963
01:55:29,610 --> 01:55:31,710
And then you want to slowly deploy it and-

2964
01:55:32,670 --> 01:55:33,660
- [Lex] At scale, hopefully.

2965
01:55:33,660 --> 01:55:35,730
- At scale and you want to
set up your data engine,

2966
01:55:35,730 --> 01:55:38,970
your improvement loops, the
telemetry, the evaluation,

2967
01:55:38,970 --> 01:55:40,320
the harness and everything.

2968
01:55:41,250 --> 01:55:42,600
And you want to improve the product

2969
01:55:42,600 --> 01:55:43,433
over time, incrementally.

2970
01:55:43,433 --> 01:55:44,700
And you're making revenue along the way.

2971
01:55:44,700 --> 01:55:46,080
That's extremely important

2972
01:55:46,080 --> 01:55:47,550
because otherwise, you cannot build

2973
01:55:47,550 --> 01:55:49,560
these large undertakings,

2974
01:55:49,560 --> 01:55:51,270
just don't make sense economically.

2975
01:55:51,270 --> 01:55:53,310
And also from the point of
view of the team working on it,

2976
01:55:53,310 --> 01:55:55,170
they need the dopamine along the way.

2977
01:55:55,170 --> 01:55:57,240
They're not just going to make a promise

2978
01:55:57,240 --> 01:55:58,740
about this being useful.

2979
01:55:58,740 --> 01:56:01,140
This is going to change the
world in 10 years when it works.

2980
01:56:01,140 --> 01:56:02,340
This is not where you want to be.

2981
01:56:02,340 --> 01:56:04,650
You want to be in a place
like I think autopilot today

2982
01:56:04,650 --> 01:56:06,793
where it's offering increased safety

2983
01:56:06,793 --> 01:56:10,050
and convenience of driving today.

2984
01:56:10,050 --> 01:56:12,840
People pay for it, people
like it, people purchase it.

2985
01:56:12,840 --> 01:56:14,490
And then you also have the greater mission

2986
01:56:14,490 --> 01:56:16,380
that you're working towards.

2987
01:56:16,380 --> 01:56:17,213
- And you see that.

2988
01:56:17,213 --> 01:56:19,080
So the dopamine for the team,

2989
01:56:19,080 --> 01:56:20,730
that was the source of happiness?

2990
01:56:20,730 --> 01:56:22,530
- Yes, hundred percent,
you're deploying this.

2991
01:56:22,530 --> 01:56:24,840
People like it, people
drive it, people pay for it,

2992
01:56:24,840 --> 01:56:25,710
they care about it.

2993
01:56:25,710 --> 01:56:28,380
There's all these YouTube
videos, your grandma drives it.

2994
01:56:28,380 --> 01:56:29,250
She gives you feedback.

2995
01:56:29,250 --> 01:56:30,690
People like it, people engage with it.

2996
01:56:30,690 --> 01:56:31,590
You engage with it.

2997
01:56:31,590 --> 01:56:32,423
Huge.

2998
01:56:32,423 --> 01:56:34,800
- Do people that drive
Teslas recognize you

2999
01:56:34,800 --> 01:56:35,670
and give you love?

3000
01:56:35,670 --> 01:56:40,670
Like, hey thanks for this
nice feature that it's doing.

3001
01:56:42,514 --> 01:56:43,800
- Yeah, I think the tricky thing is

3002
01:56:43,800 --> 01:56:46,260
some people really love you,
some people, unfortunately,

3003
01:56:46,260 --> 01:56:47,093
you're working on something that you think

3004
01:56:47,093 --> 01:56:49,020
is extremely valuable, useful, et cetera,

3005
01:56:49,020 --> 01:56:50,340
some people do hate you.

3006
01:56:50,340 --> 01:56:53,040
There's a lot of people
who hate me and the team

3007
01:56:53,040 --> 01:56:55,410
and the whole project.

3008
01:56:55,410 --> 01:56:56,345
And I think-

3009
01:56:56,345 --> 01:56:57,810
- Are they Tesla drivers?

3010
01:56:57,810 --> 01:56:59,760
- Many cases they're not, actually.

3011
01:56:59,760 --> 01:57:00,593
- Yeah.

3012
01:57:00,593 --> 01:57:02,790
That actually makes me sad about humans

3013
01:57:02,790 --> 01:57:06,450
or the current ways that humans interact.

3014
01:57:06,450 --> 01:57:07,770
I think that's actually fixable.

3015
01:57:07,770 --> 01:57:09,510
I think humans want to
be good to each other.

3016
01:57:09,510 --> 01:57:12,360
I think Twitter and social
media is part of the mechanism

3017
01:57:12,360 --> 01:57:16,290
that actually somehow makes
the negativity more viral

3018
01:57:16,290 --> 01:57:17,550
that it doesn't deserve,

3019
01:57:17,550 --> 01:57:22,550
disproportionately add a
viral boost of negativity.

3020
01:57:23,640 --> 01:57:26,373
But I wish people would
just get excited about,

3021
01:57:27,240 --> 01:57:29,610
so suppress some of the jealousy,

3022
01:57:29,610 --> 01:57:32,190
some of the ego and just
get excited for others.

3023
01:57:32,190 --> 01:57:34,440
And then there's a karma aspect to that.

3024
01:57:34,440 --> 01:57:36,660
You get excited for others,
they'll get excited for you.

3025
01:57:36,660 --> 01:57:38,850
Same thing in academia,
if you're not careful,

3026
01:57:38,850 --> 01:57:41,340
there is a dynamical system there.

3027
01:57:41,340 --> 01:57:44,190
If you think of in silos and get jealous

3028
01:57:44,190 --> 01:57:46,080
of somebody else being successful

3029
01:57:46,080 --> 01:57:49,890
that actually perhaps counterintuitively

3030
01:57:49,890 --> 01:57:52,290
leads the less productivity
of you as a community

3031
01:57:52,290 --> 01:57:53,610
and you individually.

3032
01:57:53,610 --> 01:57:56,760
I feel like if you keep
celebrating others,

3033
01:57:56,760 --> 01:57:59,430
that actually makes you more successful.

3034
01:57:59,430 --> 01:58:00,384
- [Andrej] Yeah.

3035
01:58:00,384 --> 01:58:02,430
- And I think people haven't,
depending on the industry,

3036
01:58:02,430 --> 01:58:03,570
haven't quite learned that yet.

3037
01:58:03,570 --> 01:58:04,403
- Yeah.

3038
01:58:04,403 --> 01:58:06,180
Some people are also very
negative and very vocal.

3039
01:58:06,180 --> 01:58:07,680
So they're very prominently featured.

3040
01:58:07,680 --> 01:58:10,710
But actually, there's a ton of
people who are cheerleaders,

3041
01:58:10,710 --> 01:58:12,450
but they're silent cheerleaders.

3042
01:58:12,450 --> 01:58:15,840
And when you talk to
people just in the world,

3043
01:58:15,840 --> 01:58:17,580
they will all tell you
it's amazing, it's great.

3044
01:58:17,580 --> 01:58:18,750
Especially like people who understand

3045
01:58:18,750 --> 01:58:20,550
how difficult it is to
get this stuff working.

3046
01:58:20,550 --> 01:58:24,270
People who have built products
and makers and entrepreneurs,

3047
01:58:24,270 --> 01:58:28,590
making this work and changing
something is incredibly hard.

3048
01:58:28,590 --> 01:58:31,110
Those people are more
likely to cheerlead you.

3049
01:58:31,110 --> 01:58:32,640
- Well, one of the
things that makes me sad

3050
01:58:32,640 --> 01:58:35,160
is some folks in the robotics community

3051
01:58:35,160 --> 01:58:37,382
don't do the cheerleading and they should

3052
01:58:37,382 --> 01:58:39,240
'cause they know how difficult it is.

3053
01:58:39,240 --> 01:58:40,560
Well, they actually sometimes don't know

3054
01:58:40,560 --> 01:58:42,660
how difficult it is to
create a product at scale.

3055
01:58:42,660 --> 01:58:43,493
Right?

3056
01:58:43,493 --> 01:58:44,467
- [Andrej] Yep.

3057
01:58:44,467 --> 01:58:45,300
- They actually deploy in the real-world.

3058
01:58:45,300 --> 01:58:50,040
A lot of the development
of robots and AI systems

3059
01:58:50,040 --> 01:58:52,660
is done on very specific small benchmarks

3060
01:58:53,880 --> 01:58:55,620
and as opposed to real-world additions.

3061
01:58:55,620 --> 01:58:57,210
- Yes.

3062
01:58:57,210 --> 01:58:58,332
Yeah.

3063
01:58:58,332 --> 01:58:59,165
I think it's really
hard to work on robotics

3064
01:58:59,165 --> 01:59:00,030
in academic setting.

3065
01:59:00,030 --> 01:59:02,250
- Or AI systems that
apply in the real-world.

3066
01:59:02,250 --> 01:59:07,250
You've criticized, you
flourished, and loved

3067
01:59:07,530 --> 01:59:11,010
for time the ImageNet,
the famed ImageNet dataset

3068
01:59:11,010 --> 01:59:14,700
and have recently had
some words of criticism

3069
01:59:14,700 --> 01:59:18,660
that the academic research ML community

3070
01:59:18,660 --> 01:59:21,210
gives a little too much
love still to the ImageNet

3071
01:59:21,210 --> 01:59:23,610
or those kinds of benchmarks.

3072
01:59:23,610 --> 01:59:27,030
Can you speak to the strengths
and weaknesses of data sets

3073
01:59:27,030 --> 01:59:29,283
used in machine learning research?

3074
01:59:30,420 --> 01:59:32,370
- Actually, I don't know that
I recall the specific instance

3075
01:59:32,370 --> 01:59:35,640
where I was unhappy or
criticizing ImageNet.

3076
01:59:35,640 --> 01:59:37,890
I think ImageNet has
been extremely valuable.

3077
01:59:38,880 --> 01:59:41,550
It was basically a benchmark that allowed

3078
01:59:41,550 --> 01:59:44,040
the deep learning community to demonstrate

3079
01:59:44,040 --> 01:59:45,750
that deep neural nets actually work.

3080
01:59:45,750 --> 01:59:46,583
- [Lex] Yes.

3081
01:59:47,580 --> 01:59:49,710
- There's a massive value in that.

3082
01:59:49,710 --> 01:59:52,260
So I think ImageNet was
useful, but basically,

3083
01:59:52,260 --> 01:59:54,270
it's become a bit of an
EMNIST at this point.

3084
01:59:54,270 --> 01:59:57,750
So EMNIST is like little
28 by 28 grayscale digits.

3085
01:59:57,750 --> 02:00:00,690
That's kind of a joke data set
that everyone just crushes.

3086
02:00:00,690 --> 02:00:03,213
- There's still papers written
on EMNIST though, right?

3087
02:00:03,213 --> 02:00:04,500
Like strong papers?

3088
02:00:04,500 --> 02:00:05,333
- [Andrej] Yeah.

3089
02:00:05,333 --> 02:00:07,830
- Like papers that focus
on like how do we learn

3090
02:00:07,830 --> 02:00:09,617
with a small amount of
data, that kinda stuff.

3091
02:00:09,617 --> 02:00:10,450
- Yeah.

3092
02:00:10,450 --> 02:00:11,283
Yeah, I could see that being helpful

3093
02:00:11,283 --> 02:00:12,750
but not in mainline
computer vision research

3094
02:00:12,750 --> 02:00:13,583
anymore, of course.

3095
02:00:13,583 --> 02:00:15,450
- I think the way I've
heard you just somewhere,

3096
02:00:15,450 --> 02:00:17,220
maybe I'm just imagining things,

3097
02:00:17,220 --> 02:00:19,830
but I think you said like
ImageNet was a huge contribution

3098
02:00:19,830 --> 02:00:21,090
to the community for a long time

3099
02:00:21,090 --> 02:00:23,130
and now it's time to
move past those kinds of-

3100
02:00:23,130 --> 02:00:24,240
- Well, ImageNet has been crushed.

3101
02:00:24,240 --> 02:00:28,680
I mean, the error rates are, yeah,

3102
02:00:28,680 --> 02:00:30,720
we're getting like 90% accuracy

3103
02:00:30,720 --> 02:00:34,890
in 1000 classification way prediction

3104
02:00:34,890 --> 02:00:36,960
and I've seen those images

3105
02:00:36,960 --> 02:00:40,320
and this is like really
high, that's really good.

3106
02:00:40,320 --> 02:00:42,780
If I remember correctly,
the top five error rate

3107
02:00:42,780 --> 02:00:44,670
is now like 1% or something.

3108
02:00:44,670 --> 02:00:47,910
- Given your experience with a
gigantic real-world data set,

3109
02:00:47,910 --> 02:00:49,710
would you like to see benchmarks move

3110
02:00:49,710 --> 02:00:52,770
into certain directions that
the research community uses?

3111
02:00:52,770 --> 02:00:54,000
- Unfortunately, I don't think academics

3112
02:00:54,000 --> 02:00:55,950
currently have the next ImageNet.

3113
02:00:55,950 --> 02:00:57,750
We've obviously, I think
we've crushed EMNIST.

3114
02:00:57,750 --> 02:00:59,350
We've basically crushed ImageNet

3115
02:01:00,240 --> 02:01:02,850
and there's no next-big benchmark

3116
02:01:02,850 --> 02:01:05,740
that the entire community
rallies behind and uses

3117
02:01:07,470 --> 02:01:09,360
for further development of these networks.

3118
02:01:09,360 --> 02:01:10,193
- Oh, yeah.

3119
02:01:10,193 --> 02:01:11,040
I wonder what it takes for a dataset

3120
02:01:11,040 --> 02:01:13,350
to captivate the imagination of everybody.

3121
02:01:13,350 --> 02:01:15,090
Like where they all get behind it.

3122
02:01:15,090 --> 02:01:19,170
That could also need a leader, right?

3123
02:01:19,170 --> 02:01:20,136
- [Andrej] Yeah.

3124
02:01:20,136 --> 02:01:20,969
- Somebody with popularity.

3125
02:01:20,969 --> 02:01:23,373
I mean that, yeah, why
did ImageNet takeoff?

3126
02:01:25,271 --> 02:01:26,550
Is it just the accident of history?

3127
02:01:26,550 --> 02:01:28,500
- It was the right amount of difficult,

3128
02:01:29,460 --> 02:01:31,620
it was the right amount
of difficult and simple

3129
02:01:31,620 --> 02:01:35,370
and interesting enough
it was the right time

3130
02:01:35,370 --> 02:01:36,770
for that kind of a data set.

3131
02:01:37,650 --> 02:01:38,800
- Question from Reddit.

3132
02:01:40,560 --> 02:01:43,530
What are your thoughts on
the role that synthetic data

3133
02:01:43,530 --> 02:01:45,270
and game engines will play in the future

3134
02:01:45,270 --> 02:01:46,923
of neural net model development?

3135
02:01:48,300 --> 02:01:52,413
- I think as neural
nets converge to humans,

3136
02:01:53,640 --> 02:01:55,770
the value of simulation to neural nets

3137
02:01:55,770 --> 02:01:58,220
will be similar to value
of simulation to humans.

3138
02:01:59,610 --> 02:02:04,320
So people use simulation
because they can learn something

3139
02:02:04,320 --> 02:02:07,710
in that kind of a system
and without having

3140
02:02:07,710 --> 02:02:09,270
to actually experience it.

3141
02:02:09,270 --> 02:02:12,330
- But you're referring to the
simulation we do in our head?

3142
02:02:12,330 --> 02:02:13,323
Isn't that what thinking is?

3143
02:02:13,323 --> 02:02:14,156
- Oh no, sorry.

3144
02:02:14,156 --> 02:02:17,460
Simulation, I mean like
video games or other forms

3145
02:02:17,460 --> 02:02:19,980
of simulation for various professionals.

3146
02:02:19,980 --> 02:02:21,197
- Well, so let me push back on that

3147
02:02:21,197 --> 02:02:24,000
'cause maybe there's simulation
that we do in our heads,

3148
02:02:24,000 --> 02:02:28,770
like simulate if I do this,
what do I think will happen?

3149
02:02:28,770 --> 02:02:29,603
- [Andrej] Okay.

3150
02:02:29,603 --> 02:02:30,436
That's like internal simulation.

3151
02:02:30,436 --> 02:02:31,269
- Yeah, internal.

3152
02:02:31,269 --> 02:02:32,250
Isn't that what we're doing?

3153
02:02:32,250 --> 02:02:33,480
Assuming before we act.

3154
02:02:33,480 --> 02:02:34,313
- Oh, yeah.

3155
02:02:34,313 --> 02:02:35,910
But that's independent
from the use of simulation

3156
02:02:35,910 --> 02:02:38,430
in the sense of computer
games, or using simulation

3157
02:02:38,430 --> 02:02:40,250
for training set creation, or something.

3158
02:02:40,250 --> 02:02:42,360
- Is it independent or is
it just loosely correlated?

3159
02:02:42,360 --> 02:02:47,310
'Cause isn't that useful
to do counterfactual

3160
02:02:49,050 --> 02:02:53,040
or edge case simulation to what happens

3161
02:02:53,040 --> 02:02:55,020
if there's a nuclear war?

3162
02:02:55,020 --> 02:02:58,548
What happens if there's,
like those kinds of things?

3163
02:02:58,548 --> 02:02:59,381
- Yeah.

3164
02:02:59,381 --> 02:03:00,930
That's a different simulation
from Unreal Engine.

3165
02:03:00,930 --> 02:03:02,370
That's how I interpreted the question.

3166
02:03:02,370 --> 02:03:05,553
- Ah, so like simulation
of the average case?

3167
02:03:07,500 --> 02:03:08,550
What's Unreal Engine?

3168
02:03:08,550 --> 02:03:11,760
What do you mean by Unreal Engine?

3169
02:03:11,760 --> 02:03:15,086
So simulating a world.

3170
02:03:15,086 --> 02:03:15,919
- [Andrej] Yeah.

3171
02:03:15,919 --> 02:03:17,400
- The physics of that world,

3172
02:03:17,400 --> 02:03:18,407
Why is that different?

3173
02:03:18,407 --> 02:03:21,717
'Cause you also can add
behavior to that world

3174
02:03:21,717 --> 02:03:23,973
and you could try all
kinds of stuff, right?

3175
02:03:24,840 --> 02:03:27,180
You could throw all kinds
of weird things into it.

3176
02:03:27,180 --> 02:03:28,013
- [Andrej] Yeah.

3177
02:03:28,013 --> 02:03:29,700
- Unreal Engine is not
just about simulating,

3178
02:03:29,700 --> 02:03:31,470
I mean I guess it is about simulating

3179
02:03:31,470 --> 02:03:32,340
the physics of the world,

3180
02:03:32,340 --> 02:03:35,340
it's also doing something with that.

3181
02:03:35,340 --> 02:03:36,329
- Yeah.

3182
02:03:36,329 --> 02:03:37,560
The graphics, the physics, and the agents

3183
02:03:37,560 --> 02:03:39,120
that you put into the
environment and stuff like that.

3184
02:03:39,120 --> 02:03:40,500
Yeah.

3185
02:03:40,500 --> 02:03:43,050
- I feel like you said that
it's not that important,

3186
02:03:43,050 --> 02:03:46,200
I guess, for the future of AI development.

3187
02:03:46,200 --> 02:03:47,738
Is that correct, to
interpret you that way?

3188
02:03:47,738 --> 02:03:50,887
- Well, I think, humans use simulators

3189
02:03:54,330 --> 02:03:56,040
and they find them useful and so computers

3190
02:03:56,040 --> 02:03:58,263
will use simulators and find them useful.

3191
02:03:59,160 --> 02:04:00,300
- Okay, so you're saying it's not,

3192
02:04:00,300 --> 02:04:02,130
I don't use simulators very often.

3193
02:04:02,130 --> 02:04:03,690
I play a video game every once in a while

3194
02:04:03,690 --> 02:04:05,880
but I don't think I derive any wisdom

3195
02:04:05,880 --> 02:04:09,300
about my own existence
from those video games.

3196
02:04:09,300 --> 02:04:11,730
It's a momentary escape from reality

3197
02:04:11,730 --> 02:04:14,880
versus a source of wisdom about reality.

3198
02:04:14,880 --> 02:04:17,700
So I think that's a very
polite way of saying

3199
02:04:17,700 --> 02:04:19,293
simulation is not that useful.

3200
02:04:20,190 --> 02:04:21,143
- Yeah.

3201
02:04:21,143 --> 02:04:21,976
Maybe not.

3202
02:04:21,976 --> 02:04:24,090
I don't see it as a fundamental,
really important part

3203
02:04:24,090 --> 02:04:26,970
of training neural nets currently.

3204
02:04:26,970 --> 02:04:29,880
But I think as neural nets
become more and more powerful,

3205
02:04:29,880 --> 02:04:32,430
I think you will need fewer examples

3206
02:04:32,430 --> 02:04:34,440
to train additional behaviors.

3207
02:04:34,440 --> 02:04:37,590
And simulation is, of
course, there's a domain gap

3208
02:04:37,590 --> 02:04:39,150
in a simulation that's not the real-world,

3209
02:04:39,150 --> 02:04:40,560
it's slightly something different.

3210
02:04:40,560 --> 02:04:42,960
But with a powerful enough neural net

3211
02:04:42,960 --> 02:04:45,780
you need the domain gap
can be bigger I think

3212
02:04:45,780 --> 02:04:47,400
because neural net will understand

3213
02:04:47,400 --> 02:04:48,660
that even though it's not the real-world,

3214
02:04:48,660 --> 02:04:50,520
it has all this high-level structure

3215
02:04:50,520 --> 02:04:52,320
that I'm supposed to learn from.

3216
02:04:52,320 --> 02:04:54,473
- So the neural net will actually, yeah,

3217
02:04:54,473 --> 02:04:58,740
it will be able to leverage
the synthetic data better?

3218
02:04:58,740 --> 02:04:59,573
- [Andrej] Yes.

3219
02:04:59,573 --> 02:05:01,770
- By closing the gap but understanding

3220
02:05:01,770 --> 02:05:04,260
in which ways this is not real data?

3221
02:05:04,260 --> 02:05:05,210
- [Andrej] Exactly.

3222
02:05:06,390 --> 02:05:08,070
- Reddit do better questions next time.

3223
02:05:08,070 --> 02:05:09,357
That was a question.

3224
02:05:09,357 --> 02:05:10,620
No, I'm just kidding.

3225
02:05:10,620 --> 02:05:15,620
All right, so is it
possible, do you think,

3226
02:05:16,320 --> 02:05:18,660
speaking of EMNIST to
construct neural nets

3227
02:05:18,660 --> 02:05:21,393
and training processes that
require very little data.

3228
02:05:23,310 --> 02:05:25,050
So we've been talking about huge data sets

3229
02:05:25,050 --> 02:05:25,951
like the internet for training.

3230
02:05:25,951 --> 02:05:26,784
- [Andrej] Yeah.

3231
02:05:26,784 --> 02:05:28,620
- I mean one way to say
that is like you said,

3232
02:05:28,620 --> 02:05:32,160
the querying itself is another
level of training I guess

3233
02:05:32,160 --> 02:05:33,883
and that requires a little data.

3234
02:05:33,883 --> 02:05:34,716
- [Andrej] Yeah.

3235
02:05:34,716 --> 02:05:38,940
- But do you see any
value in doing research

3236
02:05:38,940 --> 02:05:43,260
and going down the direction
of can we use very little data

3237
02:05:43,260 --> 02:05:45,420
to construct a knowledge base?

3238
02:05:45,420 --> 02:05:46,290
- A hundred percent.

3239
02:05:46,290 --> 02:05:49,050
I just think at some point
you need a massive data set

3240
02:05:49,050 --> 02:05:51,150
and then when you pre-train
your massive neural net

3241
02:05:51,150 --> 02:05:54,330
and get something that is
like a GPT or something,

3242
02:05:54,330 --> 02:05:56,760
then you're able to be very efficient

3243
02:05:56,760 --> 02:05:58,743
at training any arbitrary new task.

3244
02:05:59,610 --> 02:06:02,370
So a lot of these GPTs you can do tasks

3245
02:06:02,370 --> 02:06:04,920
like sentiment analysis,
or translation, or so on,

3246
02:06:04,920 --> 02:06:06,990
just by being prompted
with very few examples.

3247
02:06:06,990 --> 02:06:08,190
Here's the kind of thing I want you to do.

3248
02:06:08,190 --> 02:06:09,780
Like here's an input sentence,

3249
02:06:09,780 --> 02:06:11,250
here's the translation into German,

3250
02:06:11,250 --> 02:06:12,870
input sentence, translation to German,

3251
02:06:12,870 --> 02:06:15,360
input sentence, blank, and the neural net

3252
02:06:15,360 --> 02:06:16,770
will complete the translation to German

3253
02:06:16,770 --> 02:06:19,950
just by looking at the
example you've provided.

3254
02:06:19,950 --> 02:06:23,550
And so that's an example
of a very few shot learning

3255
02:06:23,550 --> 02:06:24,900
in the activations of the neural net,

3256
02:06:24,900 --> 02:06:26,460
instead of the weights of the neural net.

3257
02:06:26,460 --> 02:06:30,000
And so I think basically just like humans,

3258
02:06:30,000 --> 02:06:31,680
neural nets will become
very data efficient

3259
02:06:31,680 --> 02:06:33,780
at learning any other new task.

3260
02:06:33,780 --> 02:06:35,580
But at some point, you
need a massive data set

3261
02:06:35,580 --> 02:06:36,933
to pre-train your network.

3262
02:06:38,297 --> 02:06:39,130
- I do get that.

3263
02:06:39,130 --> 02:06:41,580
And probably, we humans
have something like that.

3264
02:06:41,580 --> 02:06:43,074
Do we have something like that?

3265
02:06:43,074 --> 02:06:47,130
Do we have a passive in the background,

3266
02:06:47,130 --> 02:06:50,283
background model constructing thing

3267
02:06:50,283 --> 02:06:53,010
that just runs all the time
in a self-supervised way?

3268
02:06:53,010 --> 02:06:54,240
We're not conscious of it?

3269
02:06:54,240 --> 02:06:57,108
- I think humans definitely,
I mean obviously,

3270
02:06:57,108 --> 02:06:59,850
we learn a lot during our lifespan,

3271
02:06:59,850 --> 02:07:02,130
but also we have a ton of hardware

3272
02:07:02,130 --> 02:07:06,240
that helps us, the initialization
coming from evolution.

3273
02:07:06,240 --> 02:07:08,880
And so I think that's also
a really big component.

3274
02:07:08,880 --> 02:07:09,810
A lot of people in the field,

3275
02:07:09,810 --> 02:07:13,070
I think they just talk about
the amounts of like seconds

3276
02:07:13,070 --> 02:07:14,040
that a person has lived,

3277
02:07:14,040 --> 02:07:16,650
pretending that this is a tabula rasa,

3278
02:07:16,650 --> 02:07:18,870
a zero initialization of a neural net.

3279
02:07:18,870 --> 02:07:20,190
And it's not.

3280
02:07:20,190 --> 02:07:22,680
You can look at a lot of
animals for example, zebras.

3281
02:07:22,680 --> 02:07:27,060
Zebras get born, and they
see, and they can run,

3282
02:07:27,060 --> 02:07:29,430
there's zero training
data in their lifespan.

3283
02:07:29,430 --> 02:07:30,630
They can just do that.

3284
02:07:30,630 --> 02:07:33,780
So somehow, I have no idea
how, evolution has found a way

3285
02:07:33,780 --> 02:07:35,250
to encode these algorithms

3286
02:07:35,250 --> 02:07:36,123
and these neural net initializations

3287
02:07:36,123 --> 02:07:39,021
that are extremely good into ATCGs.

3288
02:07:39,021 --> 02:07:40,110
And I have no idea how this works,

3289
02:07:40,110 --> 02:07:41,190
but apparently, it's possible

3290
02:07:41,190 --> 02:07:44,280
because here's proof by existence.

3291
02:07:44,280 --> 02:07:46,920
- There's something magical about

3292
02:07:46,920 --> 02:07:50,100
going from a single-cell
to an organism that is born

3293
02:07:50,100 --> 02:07:51,690
to the first few years of life.

3294
02:07:51,690 --> 02:07:54,450
I like the idea that the reason
we don't remember anything

3295
02:07:54,450 --> 02:07:57,090
about the first few years of our life

3296
02:07:57,090 --> 02:07:59,745
is that it's a really painful process.

3297
02:07:59,745 --> 02:08:03,459
Like it's a very difficult
challenging training process.

3298
02:08:03,459 --> 02:08:04,292
- [Andrej] Yeah.

3299
02:08:04,292 --> 02:08:08,250
- Like intellectually and maybe, yeah,

3300
02:08:08,250 --> 02:08:11,523
I mean I don't, why don't
we remember any of that?

3301
02:08:11,523 --> 02:08:14,550
That might be some crazy training going on

3302
02:08:14,550 --> 02:08:19,530
and maybe that's the
background model training

3303
02:08:19,530 --> 02:08:23,160
that is very painful.

3304
02:08:23,160 --> 02:08:24,308
- [Andrej] Yeah.

3305
02:08:24,308 --> 02:08:25,770
- And so it's best for the
system once it's trained

3306
02:08:25,770 --> 02:08:27,780
not to remember how it was constructed.

3307
02:08:27,780 --> 02:08:30,150
- I think it's just like the
hardware for long-term memory

3308
02:08:30,150 --> 02:08:31,680
is just not fully developed.

3309
02:08:31,680 --> 02:08:32,763
- [Lex] Sure.

3310
02:08:32,763 --> 02:08:35,760
- I feel like the first
few years of infants,

3311
02:08:35,760 --> 02:08:38,220
it's not actually like
learning, it's brain maturing.

3312
02:08:38,220 --> 02:08:39,420
- [Lex] Yeah.

3313
02:08:39,420 --> 02:08:42,510
- We're born premature
and there's a theory

3314
02:08:42,510 --> 02:08:43,920
along those lines because
of the birth canal

3315
02:08:43,920 --> 02:08:45,450
and the swelling of the brain.

3316
02:08:45,450 --> 02:08:47,670
And so we're born premature
and then the first few years

3317
02:08:47,670 --> 02:08:49,560
we're just, the brain's maturing

3318
02:08:49,560 --> 02:08:51,660
and then there's some learning eventually.

3319
02:08:52,770 --> 02:08:53,730
It's my current view on it.

3320
02:08:53,730 --> 02:08:56,730
- What do you think, do
you think neural nets

3321
02:08:56,730 --> 02:08:58,773
can have long-term memory?

3322
02:08:59,991 --> 02:09:02,040
Like that approach is
something like humans?

3323
02:09:02,040 --> 02:09:05,010
Do you think there needs to
be another meta-architecture

3324
02:09:05,010 --> 02:09:07,680
on top of it to add something
like a knowledge base

3325
02:09:07,680 --> 02:09:09,538
that learns facts about the world

3326
02:09:09,538 --> 02:09:10,590
and all that kind of stuff?

3327
02:09:10,590 --> 02:09:12,660
- Yes, but I don't know to what extent

3328
02:09:12,660 --> 02:09:14,763
it will be explicitly constructed.

3329
02:09:15,840 --> 02:09:17,550
It might take on intuitive forms

3330
02:09:17,550 --> 02:09:20,130
where you are telling the GPT,

3331
02:09:20,130 --> 02:09:22,890
hey, you have a declarative memory bank

3332
02:09:22,890 --> 02:09:25,140
to which you can store
and retrieve data from

3333
02:09:25,140 --> 02:09:26,940
and whenever you
encounter some information

3334
02:09:26,940 --> 02:09:29,700
that you find useful just
save it to your memory bank.

3335
02:09:29,700 --> 02:09:32,160
And here's an example of
something you have retrieved

3336
02:09:32,160 --> 02:09:34,257
and here's how you say it and
here's how you load from it.

3337
02:09:34,257 --> 02:09:37,950
You just say, load whatever,
you teach it in text,

3338
02:09:37,950 --> 02:09:40,170
in English and then it might learn

3339
02:09:40,170 --> 02:09:42,450
to use a memory bank from that.

3340
02:09:42,450 --> 02:09:45,390
- Oh, so the neural
net is the architecture

3341
02:09:45,390 --> 02:09:48,270
for the background model, the base thing

3342
02:09:48,270 --> 02:09:50,340
and then everything else
is just on top of it.

3343
02:09:50,340 --> 02:09:51,660
- It's not just a text, right?

3344
02:09:51,660 --> 02:09:52,950
You're giving it gadgets and gizmos.

3345
02:09:52,950 --> 02:09:55,950
So you're teaching it some
kind of a special language

3346
02:09:55,950 --> 02:09:58,200
by which it can save arbitrary information

3347
02:09:58,200 --> 02:09:59,490
and retrieve it at a later time.

3348
02:09:59,490 --> 02:10:00,323
- [Lex] Yeah.

3349
02:10:00,323 --> 02:10:01,740
- And you're telling
about these special tokens

3350
02:10:01,740 --> 02:10:04,830
and how to arrange them
to use these interfaces.

3351
02:10:04,830 --> 02:10:06,450
It's like, hey, you can use a calculator,

3352
02:10:06,450 --> 02:10:07,283
here's how you use it.

3353
02:10:07,283 --> 02:10:10,290
Just do five three plus four one equals

3354
02:10:10,290 --> 02:10:12,090
and when equals is there,

3355
02:10:12,090 --> 02:10:14,580
a calculator will actually
read out the answer

3356
02:10:14,580 --> 02:10:16,290
and you don't have to
calculate it yourself

3357
02:10:16,290 --> 02:10:17,640
and you just tell it in English,

3358
02:10:17,640 --> 02:10:19,650
this might actually work.

3359
02:10:19,650 --> 02:10:21,870
- Do you think in that
sense Gato is interesting,

3360
02:10:21,870 --> 02:10:24,810
the DeepMind system that
it's not just new language

3361
02:10:24,810 --> 02:10:28,770
but actually throws it
all in the same pile,

3362
02:10:28,770 --> 02:10:31,770
images, actions, all that kind of stuff.

3363
02:10:31,770 --> 02:10:34,200
That's basically what
we're moving towards.

3364
02:10:34,200 --> 02:10:35,033
- Yeah, I think so.

3365
02:10:35,033 --> 02:10:38,400
So Gato is very much a
kitchen sink of an approach

3366
02:10:38,400 --> 02:10:42,480
to reinforcement learning in
lots of different environments

3367
02:10:42,480 --> 02:10:44,910
with a single fixed transformer model.

3368
02:10:44,910 --> 02:10:45,743
Right?

3369
02:10:46,680 --> 02:10:49,950
I think it's a very early
result in that realm.

3370
02:10:49,950 --> 02:10:51,600
But I think, yeah, it's along the lines

3371
02:10:51,600 --> 02:10:53,520
of what I think things
will eventually look like.

3372
02:10:53,520 --> 02:10:54,353
- Right.

3373
02:10:54,353 --> 02:10:55,980
So this is the early days of a system

3374
02:10:55,980 --> 02:10:57,093
that eventually will look like this,

3375
02:10:57,093 --> 02:11:00,000
from a Rich Sutton perspective.

3376
02:11:00,000 --> 02:11:00,973
- Yeah.

3377
02:11:00,973 --> 02:11:02,400
I'm not a super huge fan of,
I think all these interfaces

3378
02:11:02,400 --> 02:11:04,920
that like look very different.

3379
02:11:04,920 --> 02:11:07,440
I would want everything to be
normalized into the same API.

3380
02:11:07,440 --> 02:11:10,230
So for example, screen
pixels, very same API

3381
02:11:10,230 --> 02:11:12,000
instead of having like
different world environments

3382
02:11:12,000 --> 02:11:14,160
that have very different
physics and joint configurations

3383
02:11:14,160 --> 02:11:15,660
and appearances and whatever.

3384
02:11:15,660 --> 02:11:17,250
And you're having some
kind of special tokens

3385
02:11:17,250 --> 02:11:19,620
for different games that you can plug.

3386
02:11:19,620 --> 02:11:22,590
I'd rather just normalize
everything to a single interface

3387
02:11:22,590 --> 02:11:24,920
so it looks the same to the
neural net if that makes sense.

3388
02:11:24,920 --> 02:11:27,660
- So it's all gonna be
pixel-based Pong in the end?

3389
02:11:27,660 --> 02:11:28,760
- [Andrej] I think so.

3390
02:11:30,780 --> 02:11:32,280
- Okay.

3391
02:11:32,280 --> 02:11:34,923
Let me ask you about
your own personal life.

3392
02:11:35,850 --> 02:11:37,980
A lot of people wanna know
you're one of the most productive

3393
02:11:37,980 --> 02:11:40,320
and brilliant people in the history of AI.

3394
02:11:40,320 --> 02:11:42,210
What is a productive day in the life

3395
02:11:42,210 --> 02:11:44,520
of Andrej Karpathy look like?

3396
02:11:44,520 --> 02:11:45,990
What time do you wake up?

3397
02:11:45,990 --> 02:11:49,380
'Cause imagine some kind of dance between

3398
02:11:49,380 --> 02:11:51,990
the average productive day
and a perfect productive day.

3399
02:11:51,990 --> 02:11:55,470
So the perfect productive day
is the thing we strive towards

3400
02:11:55,470 --> 02:11:58,200
and the average is kind
of what it converges to,

3401
02:11:58,200 --> 02:12:01,710
given all the mistakes and
human eventualities and so on.

3402
02:12:01,710 --> 02:12:02,648
- [Andrej] Yep.

3403
02:12:02,648 --> 02:12:03,481
- So what time did you wake up?

3404
02:12:03,481 --> 02:12:04,470
Are you a morning person?

3405
02:12:04,470 --> 02:12:05,610
- I'm not a morning person.

3406
02:12:05,610 --> 02:12:07,498
I'm a night owl, for sure.

3407
02:12:07,498 --> 02:12:09,006
- Is it stable or not?

3408
02:12:09,006 --> 02:12:11,280
- It's semi-stable like eight or nine

3409
02:12:11,280 --> 02:12:12,570
or something like that.

3410
02:12:12,570 --> 02:12:15,300
During my PhD, it was even
later, I used to go to sleep

3411
02:12:15,300 --> 02:12:19,950
usually at 3:00 AM, I think
the AM hours are precious

3412
02:12:19,950 --> 02:12:21,150
and a very interesting time to work

3413
02:12:21,150 --> 02:12:23,160
because everyone is asleep.

3414
02:12:23,160 --> 02:12:26,340
At 8:00 AM or 7:00 AM
the east coast is awake.

3415
02:12:26,340 --> 02:12:27,450
So there's already activity,

3416
02:12:27,450 --> 02:12:28,703
there's already some
text messages, whatever.

3417
02:12:28,703 --> 02:12:29,940
There's stuff happening.

3418
02:12:29,940 --> 02:12:31,920
You can go on some news website

3419
02:12:31,920 --> 02:12:34,200
and there's stuff
happening, it's distracting.

3420
02:12:34,200 --> 02:12:36,630
At 3:00 AM everything is totally quiet

3421
02:12:36,630 --> 02:12:37,830
and so you're not gonna be bothered

3422
02:12:37,830 --> 02:12:40,323
and you have solid chunks
of time to do work.

3423
02:12:42,180 --> 02:12:45,210
So I like those periods,
night owl by default.

3424
02:12:45,210 --> 02:12:46,650
And then I think productive time,

3425
02:12:46,650 --> 02:12:50,250
basically, what I like to
do is you need to build

3426
02:12:50,250 --> 02:12:53,610
some momentum on the problem
without too much distraction

3427
02:12:53,610 --> 02:12:58,610
and you need to load your
ram, your working memory

3428
02:12:58,890 --> 02:13:00,360
with that problem.

3429
02:13:00,360 --> 02:13:01,710
And then you need to be obsessed with it

3430
02:13:01,710 --> 02:13:04,020
when you're taking shower,
when you're falling asleep,

3431
02:13:04,020 --> 02:13:05,340
you need to be obsessed with the problem

3432
02:13:05,340 --> 02:13:06,540
and it's fully in your memory

3433
02:13:06,540 --> 02:13:08,880
and you're ready to wake up
and work on it right there.

3434
02:13:08,880 --> 02:13:13,380
- So is this in a scale
temporal scales of a single day

3435
02:13:13,380 --> 02:13:14,792
or a couple of days a week, a month?

3436
02:13:14,792 --> 02:13:15,625
- Yeah.

3437
02:13:15,625 --> 02:13:16,920
So I can't talk about one day, basically,

3438
02:13:16,920 --> 02:13:19,534
in isolation because it's a whole process.

3439
02:13:19,534 --> 02:13:21,510
When I wanna get
productive in the problem,

3440
02:13:21,510 --> 02:13:23,940
I feel like I need a span of a few days

3441
02:13:23,940 --> 02:13:26,217
where I can really get in on that problem

3442
02:13:26,217 --> 02:13:27,600
and I don't wanna be interrupted

3443
02:13:27,600 --> 02:13:30,150
and I'm going to just
be completely obsessed

3444
02:13:30,150 --> 02:13:31,200
with that problem.

3445
02:13:31,200 --> 02:13:34,110
And that's where I do most
of my good work, I would say.

3446
02:13:34,110 --> 02:13:35,280
- You've done a bunch of cool,

3447
02:13:35,280 --> 02:13:37,830
like little projects in a
very short amount of time,

3448
02:13:37,830 --> 02:13:40,890
very quickly, so that requires
you just focusing on it.

3449
02:13:40,890 --> 02:13:42,480
- Yeah, basically, I need
to load my working memory

3450
02:13:42,480 --> 02:13:44,430
with the problem and I
need to be productive

3451
02:13:44,430 --> 02:13:46,350
because there's always a huge fixed cost

3452
02:13:46,350 --> 02:13:47,733
to approaching any problem.

3453
02:13:49,500 --> 02:13:51,150
I was struggling with
this for example at Tesla

3454
02:13:51,150 --> 02:13:54,000
because I want to work on
small side project, but okay,

3455
02:13:54,000 --> 02:13:54,960
you first need to figure out,

3456
02:13:54,960 --> 02:13:56,550
oh, okay, I need to
associate into my cluster,

3457
02:13:56,550 --> 02:13:59,450
I need to bring up a VS Code
editor so I can work on this.

3458
02:14:00,360 --> 02:14:03,390
I ran into some stupid error
because of some reason.

3459
02:14:03,390 --> 02:14:04,223
You're not at a point

3460
02:14:04,223 --> 02:14:05,670
where you can be just
productive right away.

3461
02:14:05,670 --> 02:14:07,530
You are facing barriers.

3462
02:14:07,530 --> 02:14:11,490
And so it's about really
removing all of that barrier

3463
02:14:11,490 --> 02:14:12,900
and you're able to go into the problem

3464
02:14:12,900 --> 02:14:15,420
and you have the full problem
loaded in your memory.

3465
02:14:15,420 --> 02:14:17,672
- And somehow avoiding distractions
of all different forms.

3466
02:14:17,672 --> 02:14:18,505
- [Andrej] Yes.

3467
02:14:18,505 --> 02:14:23,250
- Like news stories, emails,
but also distractions

3468
02:14:23,250 --> 02:14:24,870
from other interesting projects

3469
02:14:24,870 --> 02:14:26,280
that you previously worked on

3470
02:14:26,280 --> 02:14:28,080
or currently working on and so on.

3471
02:14:28,080 --> 02:14:29,297
You just wanna really focus your mind.

3472
02:14:29,297 --> 02:14:30,130
- Yeah.

3473
02:14:30,130 --> 02:14:32,130
And I mean I can take some
time off for distractions

3474
02:14:32,130 --> 02:14:35,400
and in between, but I
think it can't be too much.

3475
02:14:35,400 --> 02:14:38,070
Most of your day is sort
of spent on that problem.

3476
02:14:38,070 --> 02:14:42,270
And then, I drink coffee,
I have my morning routine,

3477
02:14:42,270 --> 02:14:45,030
I look at some news, Twitter, Hacker News,

3478
02:14:45,030 --> 02:14:47,089
Wall Street Journal, et cetera.

3479
02:14:47,089 --> 02:14:47,922
It's great.

3480
02:14:47,922 --> 02:14:49,470
- So basically, you wake
up, you have some coffee,

3481
02:14:49,470 --> 02:14:51,420
are you trying to get to
work as quickly as possible?

3482
02:14:51,420 --> 02:14:55,410
Do you take in this diet of
what the hell's happening

3483
02:14:55,410 --> 02:14:57,060
in the world first?

3484
02:14:57,060 --> 02:14:59,700
- I do find it interesting
to know about the world.

3485
02:14:59,700 --> 02:15:01,980
I don't know that it's useful or good,

3486
02:15:01,980 --> 02:15:03,600
but it is part of my routine right now.

3487
02:15:03,600 --> 02:15:05,310
So I do read through a
bunch of news articles

3488
02:15:05,310 --> 02:15:09,960
and I wanna be informed
and I'm suspicious of it.

3489
02:15:09,960 --> 02:15:11,010
I'm suspicious of the practice,

3490
02:15:11,010 --> 02:15:12,360
but currently, that's where I am.

3491
02:15:12,360 --> 02:15:15,540
- Oh, you mean suspicious
about the positive effect?

3492
02:15:15,540 --> 02:15:16,373
- [Andrej] Yeah.

3493
02:15:16,373 --> 02:15:19,260
- Of that practice on your
productivity and your well-being?

3494
02:15:19,260 --> 02:15:21,120
- My well-being, psychologically, yeah.

3495
02:15:21,120 --> 02:15:23,640
- And also on your ability to
deeply understand the world

3496
02:15:23,640 --> 02:15:26,550
because there's a bunch
of sources of information

3497
02:15:26,550 --> 02:15:28,584
you're not really focused
on deeply integrating it.

3498
02:15:28,584 --> 02:15:30,413
- Yeah, it's a little bit distracting.

3499
02:15:30,413 --> 02:15:31,260
Yeah.

3500
02:15:31,260 --> 02:15:33,300
- In terms of a perfectly productive day

3501
02:15:33,300 --> 02:15:37,440
for how long of a stretch
of time in one session

3502
02:15:37,440 --> 02:15:39,310
do you try to work and focus on a thing?

3503
02:15:39,310 --> 02:15:42,060
Is it a couple hours, is it
one hour, is it 30 minutes?

3504
02:15:42,060 --> 02:15:43,590
Is it 10 minutes?

3505
02:15:43,590 --> 02:15:45,588
- I can probably go a small few hours

3506
02:15:45,588 --> 02:15:47,190
and then I need some breaks in between

3507
02:15:47,190 --> 02:15:50,610
for food and stuff and yeah.

3508
02:15:50,610 --> 02:15:53,610
But I think, it's still really
hard to accumulate hours.

3509
02:15:53,610 --> 02:15:55,800
I was using a tracker that
told me exactly how much time

3510
02:15:55,800 --> 02:15:57,030
I spent coding any one day.

3511
02:15:57,030 --> 02:15:58,710
And even on a very productive day,

3512
02:15:58,710 --> 02:16:00,930
I still spent only six or eight hours.

3513
02:16:00,930 --> 02:16:01,763
- [Lex] Yeah.

3514
02:16:01,763 --> 02:16:04,140
- And it's just because there's
so much padding, commute,

3515
02:16:04,140 --> 02:16:07,320
talking to people, food, et cetera.

3516
02:16:07,320 --> 02:16:11,040
There's a cost of life
just living and sustaining

3517
02:16:11,040 --> 02:16:13,650
and homeostasis and just
maintaining yourself

3518
02:16:13,650 --> 02:16:15,930
as a human is very high.

3519
02:16:15,930 --> 02:16:20,114
- And there seems to be a
desire within the human mind

3520
02:16:20,114 --> 02:16:23,646
to participate in society
that creates that padding.

3521
02:16:23,646 --> 02:16:24,479
- [Andrej] Yeah.

3522
02:16:24,479 --> 02:16:26,580
- 'Cause the most productive
days I've ever had

3523
02:16:26,580 --> 02:16:28,350
is just completely from start to finish

3524
02:16:28,350 --> 02:16:29,460
is tuning out everything.

3525
02:16:29,460 --> 02:16:30,293
- [Andrej] Yep.

3526
02:16:30,293 --> 02:16:32,790
- And just sitting there
and then you could do more

3527
02:16:32,790 --> 02:16:33,900
than six in eight hours.

3528
02:16:33,900 --> 02:16:34,733
- Yep.

3529
02:16:34,733 --> 02:16:36,510
- Is there some wisdom about
what gives you strength

3530
02:16:36,510 --> 02:16:39,693
to do tough days of long focus?

3531
02:16:40,980 --> 02:16:43,020
- Yeah, just whenever I get
obsessed about a problem,

3532
02:16:43,020 --> 02:16:44,070
something just needs to work.

3533
02:16:44,070 --> 02:16:45,270
Something just needs to exist.

3534
02:16:45,270 --> 02:16:47,040
- It needs to exist.

3535
02:16:47,040 --> 02:16:49,860
So you're able to deal with
bugs and programming issues

3536
02:16:49,860 --> 02:16:53,010
and technical issues and design decisions

3537
02:16:53,010 --> 02:16:54,240
that turn out to be the wrong ones.

3538
02:16:54,240 --> 02:16:55,923
You're able to think through all of that

3539
02:16:55,923 --> 02:16:57,840
given that you want a thing to exist.

3540
02:16:57,840 --> 02:16:58,673
- Yeah, it needs to exist.

3541
02:16:58,673 --> 02:17:01,230
And then I think to me
also a big factor is,

3542
02:17:01,230 --> 02:17:02,940
are other humans are
going to appreciate it?

3543
02:17:02,940 --> 02:17:04,230
Are they going to like it?

3544
02:17:04,230 --> 02:17:05,400
That's a big part of my motivation.

3545
02:17:05,400 --> 02:17:07,889
If I'm helping humans and they seem happy,

3546
02:17:07,889 --> 02:17:11,549
they say nice things, they
Tweet about it or whatever,

3547
02:17:11,549 --> 02:17:13,859
that gives me pleasure because
I'm doing something useful.

3548
02:17:13,860 --> 02:17:16,990
- So you do see yourself
sharing it with the world?

3549
02:17:16,990 --> 02:17:19,770
With GitHub, with the blog
post or through videos?

3550
02:17:19,770 --> 02:17:20,790
- Yeah, I was thinking about it like,

3551
02:17:20,790 --> 02:17:22,980
suppose I did all these
things but did not share them.

3552
02:17:22,980 --> 02:17:24,719
I don't think I would have
the same amount of motivation

3553
02:17:24,719 --> 02:17:25,552
that I can build up.

3554
02:17:25,553 --> 02:17:30,553
- You enjoy the feeling of
other people gaining value

3555
02:17:30,809 --> 02:17:33,119
and happiness from the
stuff you've created.

3556
02:17:33,120 --> 02:17:34,620
- [Andrej] Yeah.

3557
02:17:34,620 --> 02:17:35,613
- What about diet?

3558
02:17:36,719 --> 02:17:38,399
I saw you played with
intermittent fasting.

3559
02:17:38,400 --> 02:17:39,233
Do you fast?

3560
02:17:39,233 --> 02:17:40,234
Does that help?

3561
02:17:40,234 --> 02:17:41,760
- I play with everything.

3562
02:17:41,760 --> 02:17:44,730
- With the things you played,
what's been most beneficial

3563
02:17:44,730 --> 02:17:47,486
to your ability to
mentally focus on a thing

3564
02:17:47,486 --> 02:17:50,819
and just mental
productivity and happiness?

3565
02:17:50,820 --> 02:17:51,653
You still fast?

3566
02:17:51,653 --> 02:17:52,710
- Yeah.

3567
02:17:52,710 --> 02:17:54,209
I still fast but I do intermittent fasting

3568
02:17:54,209 --> 02:17:55,519
but really what it means
at the end of the day

3569
02:17:55,520 --> 02:17:56,610
is I skip breakfast.

3570
02:17:56,610 --> 02:17:57,442
- [Lex] Yeah.

3571
02:17:57,442 --> 02:17:59,759
- So I do 18/6 roughly by default

3572
02:17:59,760 --> 02:18:01,799
when I'm in my steady
state, if I'm traveling

3573
02:18:01,799 --> 02:18:03,569
or doing something else
I will break the rules.

3574
02:18:03,570 --> 02:18:06,059
But in my steady state, I do 18/6.

3575
02:18:06,059 --> 02:18:08,249
So I eat only from 12:00 to 6:00.

3576
02:18:08,250 --> 02:18:10,920
Not a hard rule and I break it
often, but that's my default.

3577
02:18:10,920 --> 02:18:13,920
And then, yeah, I've done a
bunch of random experiments

3578
02:18:13,920 --> 02:18:15,450
for the most part right now

3579
02:18:15,450 --> 02:18:17,850
where I've been for the last
year and a half I wanna say

3580
02:18:17,850 --> 02:18:20,969
is I'm plant-based or plant-forward.

3581
02:18:20,969 --> 02:18:22,320
I heard plant-forward, it sounds better.

3582
02:18:22,321 --> 02:18:23,154
- [Lex] What does that mean exactly?

3583
02:18:23,154 --> 02:18:24,150
- I don't actually know
what the difference is,

3584
02:18:24,150 --> 02:18:25,799
but it sounds better in my mind.

3585
02:18:25,799 --> 02:18:28,949
But it just means I prefer
plant-based food and-

3586
02:18:28,950 --> 02:18:30,690
- Raw or cooked.

3587
02:18:30,690 --> 02:18:33,059
- I prefer cooked and plant-based.

3588
02:18:33,059 --> 02:18:35,849
- So plant-based, forgive me,

3589
02:18:35,850 --> 02:18:38,910
I don't actually know
how wide the category

3590
02:18:38,910 --> 02:18:40,799
of plant entails.

3591
02:18:40,799 --> 02:18:41,669
- Well, plant-based just means

3592
02:18:41,670 --> 02:18:45,187
that you're not militant
about it and you can flex

3593
02:18:45,187 --> 02:18:49,049
and you just prefer to eat
plants and you're not making,

3594
02:18:49,049 --> 02:18:50,969
you're not trying to
influence other people.

3595
02:18:50,969 --> 02:18:52,979
And you come to someone's house party

3596
02:18:52,980 --> 02:18:54,870
and they serve you a steak
that they're really proud of,

3597
02:18:54,870 --> 02:18:55,702
you will eat it.

3598
02:18:55,702 --> 02:18:56,549
- Yes.

3599
02:18:56,549 --> 02:18:57,546
Right.

3600
02:18:57,547 --> 02:18:58,379
You're not judgmental.

3601
02:18:58,379 --> 02:18:59,212
That's beautiful.

3602
02:18:59,213 --> 02:19:00,750
I mean, I'm the flip side of that,

3603
02:19:00,750 --> 02:19:02,760
but I'm very sort of flexible.

3604
02:19:02,760 --> 02:19:05,040
Have you tried doing one meal a day?

3605
02:19:05,040 --> 02:19:08,520
- I have accidentally
but not consistently,

3606
02:19:08,520 --> 02:19:09,540
but I've accidentally had that.

3607
02:19:09,540 --> 02:19:10,680
I don't like it.

3608
02:19:10,680 --> 02:19:12,758
I think it makes me feel not good.

3609
02:19:12,758 --> 02:19:14,549
It's too much of a hit.

3610
02:19:14,549 --> 02:19:15,382
- [Lex] Yeah.

3611
02:19:15,383 --> 02:19:17,490
- And so currently I have
about two meals a day,

3612
02:19:17,490 --> 02:19:18,823
12:00 and 6:00, probably.

3613
02:19:18,823 --> 02:19:19,980
- I do that nonstop.

3614
02:19:19,980 --> 02:19:21,063
I'm doing it now.

3615
02:19:21,063 --> 02:19:22,678
I do one meal a day.

3616
02:19:22,678 --> 02:19:23,511
- [Andrej] Okay.

3617
02:19:23,511 --> 02:19:24,343
- It's interesting.

3618
02:19:24,343 --> 02:19:25,176
It's an interesting feeling.

3619
02:19:25,177 --> 02:19:26,610
Have you ever fasted longer than a day?

3620
02:19:26,610 --> 02:19:28,410
- Yeah, I've done a bunch of water fasts.

3621
02:19:28,410 --> 02:19:29,730
'Cause I was curious what happens.

3622
02:19:29,730 --> 02:19:32,473
- [Lex] What happens,
anything interesting?

3623
02:19:32,473 --> 02:19:33,306
- Yeah, I would say so.

3624
02:19:33,306 --> 02:19:35,010
I mean, what's interesting
is that you're hungry

3625
02:19:35,010 --> 02:19:37,799
for two days and then
starting day three or so,

3626
02:19:37,799 --> 02:19:39,479
you're not hungry.

3627
02:19:39,480 --> 02:19:40,530
It's such a weird feeling

3628
02:19:40,530 --> 02:19:41,760
because you haven't eaten in a few days

3629
02:19:41,760 --> 02:19:42,809
and you're not hungry.

3630
02:19:42,809 --> 02:19:43,832
- Yeah, isn't that weird?

3631
02:19:43,833 --> 02:19:44,665
- [Andrej] It's really weird.

3632
02:19:44,665 --> 02:19:47,159
- One of the many weird
things about human biology.

3633
02:19:47,160 --> 02:19:48,124
- [Andrej] Yeah.

3634
02:19:48,124 --> 02:19:48,957
- It figures something out,

3635
02:19:48,957 --> 02:19:51,270
it finds another source of
energy or something like that.

3636
02:19:51,270 --> 02:19:54,219
Or relaxes the system.

3637
02:19:54,219 --> 02:19:55,052
I don't know how it works.

3638
02:19:55,052 --> 02:19:56,160
- Yeah, the body is like,
"You're hungry, you're hungry."

3639
02:19:56,160 --> 02:19:57,090
And then it just gives up.

3640
02:19:57,090 --> 02:19:58,870
It's like, "Okay, I
guess we're fasting now."

3641
02:19:58,870 --> 02:20:00,242
There's nothing.

3642
02:20:00,242 --> 02:20:03,240
And then it's just focuses on
trying to make you not hungry

3643
02:20:03,240 --> 02:20:06,660
and not feel the damage of
that and trying to give you

3644
02:20:06,660 --> 02:20:08,883
some space to figure
out the food situation.

3645
02:20:09,810 --> 02:20:14,700
- So are you still to this
day most productive at night?

3646
02:20:14,700 --> 02:20:15,780
- I would say I am,

3647
02:20:15,780 --> 02:20:18,573
but it is really hard to
maintain my PhD schedule.

3648
02:20:19,740 --> 02:20:21,483
Especially, when I was say
working at Tesla and so on.

3649
02:20:21,483 --> 02:20:22,653
It's a non-starter.

3650
02:20:23,550 --> 02:20:27,960
But even now, people want
to meet for various events.

3651
02:20:27,960 --> 02:20:29,970
Society lives in a certain period of time.

3652
02:20:29,970 --> 02:20:30,937
- [Lex] Yeah.

3653
02:20:30,937 --> 02:20:32,187
- And you have to work with that.

3654
02:20:32,187 --> 02:20:34,350
- It's hard to do a social thing

3655
02:20:34,350 --> 02:20:36,510
and then after that return and do work.

3656
02:20:36,510 --> 02:20:37,343
- Yeah.

3657
02:20:37,343 --> 02:20:38,176
It's just really hard.

3658
02:20:40,110 --> 02:20:41,580
- That's why I try,
when I do social thing,

3659
02:20:41,580 --> 02:20:43,890
I try not to do too much drinking

3660
02:20:43,890 --> 02:20:46,260
so I can return and continue doing work.

3661
02:20:46,260 --> 02:20:47,820
- [Andrej] Yeah.

3662
02:20:47,820 --> 02:20:52,020
- But at Tesla is there
conversions, not Tesla,

3663
02:20:52,020 --> 02:20:56,070
but any company, is there a
convergence to always a schedule

3664
02:20:56,070 --> 02:21:00,690
or is that how humans behave
when they collaborate?

3665
02:21:00,690 --> 02:21:01,830
I need to learn about this?

3666
02:21:01,830 --> 02:21:02,828
- [Andrej] Yeah.

3667
02:21:02,828 --> 02:21:04,230
- Do they try to keep
a consistent schedule

3668
02:21:04,230 --> 02:21:05,167
where you're all awake at the same time?

3669
02:21:05,167 --> 02:21:07,410
- I mean, I do try to create a routine

3670
02:21:07,410 --> 02:21:09,090
and I try to create a steady state

3671
02:21:09,090 --> 02:21:11,490
in which I'm comfortable in.

3672
02:21:11,490 --> 02:21:13,380
So I have a morning routine,
I have a day routine.

3673
02:21:13,380 --> 02:21:15,570
I try to keep things to a steady state

3674
02:21:15,570 --> 02:21:17,940
and things are predictable

3675
02:21:17,940 --> 02:21:20,970
and then your body just sticks to that.

3676
02:21:20,970 --> 02:21:22,440
And if you try to stress
that a little too much,

3677
02:21:22,440 --> 02:21:24,300
it will create, when you're traveling

3678
02:21:24,300 --> 02:21:25,380
and you're dealing with jet lag,

3679
02:21:25,380 --> 02:21:29,790
you're not able to really
ascend to where you need to go.

3680
02:21:29,790 --> 02:21:30,623
- Yeah.

3681
02:21:30,623 --> 02:21:31,789
Yeah.

3682
02:21:31,789 --> 02:21:33,690
That's weird too about us humans
with the habits and stuff.

3683
02:21:33,690 --> 02:21:35,940
What are your thoughts
on work-life balance

3684
02:21:35,940 --> 02:21:37,473
throughout a human lifetime?

3685
02:21:38,670 --> 02:21:42,240
So Tesla in part was
known for pushing people

3686
02:21:42,240 --> 02:21:45,420
to their limits, in terms
of what they're able to do,

3687
02:21:45,420 --> 02:21:48,450
in terms of what they're trying to do,

3688
02:21:48,450 --> 02:21:50,760
in terms of how much they
work, all that kind of stuff.

3689
02:21:50,760 --> 02:21:52,470
- Yeah, I mean I will say Tesla

3690
02:21:52,470 --> 02:21:55,140
gets a little too much bad rep for this

3691
02:21:55,140 --> 02:21:56,117
because what's happening is Tesla,

3692
02:21:56,117 --> 02:21:58,110
it's a bursty environment.

3693
02:21:58,110 --> 02:22:00,330
So I would say the baseline,

3694
02:22:00,330 --> 02:22:02,130
my only point of reference is Google

3695
02:22:02,130 --> 02:22:03,150
where I've interned three times

3696
02:22:03,150 --> 02:22:05,943
and I saw what it's like
inside Google and DeepMind,

3697
02:22:06,870 --> 02:22:08,850
I would say the baseline
is higher than that.

3698
02:22:08,850 --> 02:22:10,740
But then there's a punctual equilibrium

3699
02:22:10,740 --> 02:22:12,510
where once in a while there's a fire

3700
02:22:12,510 --> 02:22:16,770
and people work really hard
and so it's spiky and bursty

3701
02:22:16,770 --> 02:22:18,360
and then all the stories get collected-

3702
02:22:18,360 --> 02:22:19,320
- [Lex] Above the bursts.

3703
02:22:19,320 --> 02:22:20,227
Yeah.

3704
02:22:20,227 --> 02:22:21,900
- And then it gives the
appearance of total insanity.

3705
02:22:21,900 --> 02:22:24,540
But actually, it's just a
bit more intense environment

3706
02:22:24,540 --> 02:22:26,970
and there are fires and sprints

3707
02:22:26,970 --> 02:22:30,900
and so I think definitely
though I would say

3708
02:22:30,900 --> 02:22:31,920
it's a more intense environment

3709
02:22:31,920 --> 02:22:32,907
than something you would get at Google.

3710
02:22:32,907 --> 02:22:34,920
- But in your personal,
forget all of that,

3711
02:22:34,920 --> 02:22:37,590
just in your own personal life,

3712
02:22:37,590 --> 02:22:40,980
what do you think about the
happiness of a human being?

3713
02:22:40,980 --> 02:22:43,890
A brilliant person like yourself.

3714
02:22:43,890 --> 02:22:46,740
about finding a balance
between work and life

3715
02:22:46,740 --> 02:22:50,793
or is such a thing not a
good thought experiment?

3716
02:22:51,870 --> 02:22:55,470
- Yeah, I think balance is good

3717
02:22:55,470 --> 02:22:58,710
but I also love to have sprints
that are out of distribution

3718
02:22:58,710 --> 02:23:03,710
and that's when I think I've
been pretty creative as well.

3719
02:23:04,260 --> 02:23:08,100
- So sprints out of distribution
means that most of the time

3720
02:23:08,100 --> 02:23:11,730
you have a quote-unquote balance.

3721
02:23:11,730 --> 02:23:14,070
- I have balance most of the
time and I like being obsessed

3722
02:23:14,070 --> 02:23:15,930
with something once in a while.

3723
02:23:15,930 --> 02:23:17,370
- Once in a while is what, once a week,

3724
02:23:17,370 --> 02:23:18,480
once a month, once a year?

3725
02:23:18,480 --> 02:23:19,378
- Yeah.

3726
02:23:19,378 --> 02:23:20,211
Probably I'd like say
once a month or something.

3727
02:23:20,211 --> 02:23:21,044
Yeah.

3728
02:23:21,044 --> 02:23:23,310
- And that's when we get and
you get Git-Repo for market.

3729
02:23:23,310 --> 02:23:24,143
- Yeah.

3730
02:23:24,143 --> 02:23:24,990
That's when you're like
really care about a problem,

3731
02:23:24,990 --> 02:23:25,823
it must exist.

3732
02:23:25,823 --> 02:23:26,970
This will be awesome.

3733
02:23:26,970 --> 02:23:29,280
You're obsessed with it and
now you can't just do it

3734
02:23:29,280 --> 02:23:31,110
on that day, you need
to pay the fixed cost

3735
02:23:31,110 --> 02:23:32,460
of getting into the groove.

3736
02:23:32,460 --> 02:23:33,594
- [Lex] Yeah.

3737
02:23:33,594 --> 02:23:34,427
- And then you need to
stay there for a while

3738
02:23:34,427 --> 02:23:35,610
and then society will come

3739
02:23:35,610 --> 02:23:36,960
and they will try to mess with you

3740
02:23:36,960 --> 02:23:38,348
and they will try to distract you.

3741
02:23:38,348 --> 02:23:39,181
- [Lex] Yeah.

3742
02:23:39,181 --> 02:23:40,014
- Yeah.

3743
02:23:40,014 --> 02:23:40,847
The worst thing is a person who's like,

3744
02:23:40,847 --> 02:23:41,700
"I just need five minutes of your time."

3745
02:23:41,700 --> 02:23:42,533
- [Lex] Yeah.

3746
02:23:43,604 --> 02:23:45,165
- The cost of that is not five minutes.

3747
02:23:45,165 --> 02:23:45,998
- [Lex] Yes.

3748
02:23:45,998 --> 02:23:48,540
- And society needs to
change how it thinks about

3749
02:23:48,540 --> 02:23:50,370
just five minutes of your time.

3750
02:23:50,370 --> 02:23:51,357
- Right.

3751
02:23:51,357 --> 02:23:53,790
It's never just one minute, just 30.

3752
02:23:53,790 --> 02:23:54,900
Just a quick thing.

3753
02:23:54,900 --> 02:23:55,733
- [Andrej] What's the big deal?

3754
02:23:55,733 --> 02:23:56,566
Why are you being, so?

3755
02:23:56,566 --> 02:23:57,399
- Yeah, no.

3756
02:23:58,860 --> 02:24:00,960
What's your computer setup?

3757
02:24:00,960 --> 02:24:02,970
What's like the perfect-

3758
02:24:02,970 --> 02:24:04,590
Are you somebody that's flexible

3759
02:24:04,590 --> 02:24:08,100
to no matter what laptop, four screens?

3760
02:24:08,100 --> 02:24:08,933
- [Andrej] Yeah.

3761
02:24:08,933 --> 02:24:11,670
- Or do you prefer a certain setup

3762
02:24:11,670 --> 02:24:13,710
that you're most productive with?

3763
02:24:13,710 --> 02:24:15,180
- I guess the one that I'm familiar with

3764
02:24:15,180 --> 02:24:20,180
is one large screen, 27-inch
and my laptop on the side.

3765
02:24:20,430 --> 02:24:21,673
- What operating system?

3766
02:24:21,673 --> 02:24:23,700
- I do MaX, that's my primary.

3767
02:24:23,700 --> 02:24:25,260
- For all tasks?

3768
02:24:25,260 --> 02:24:26,093
- I would say OS X.

3769
02:24:26,093 --> 02:24:26,970
But when you're working on deep-learning,

3770
02:24:26,970 --> 02:24:29,190
everything is Linux,
you're SSH into a cluster

3771
02:24:29,190 --> 02:24:30,870
and you're working remotely.

3772
02:24:30,870 --> 02:24:32,250
- But what about the actual development?

3773
02:24:32,250 --> 02:24:33,979
Like using the IDE?

3774
02:24:33,979 --> 02:24:34,812
- Yeah.

3775
02:24:34,812 --> 02:24:38,340
You would use, I think a good
way is you just run VS Code,

3776
02:24:38,340 --> 02:24:40,530
my favorite editor right now on your Mac.

3777
02:24:40,530 --> 02:24:43,480
But you are actually, you have
a remote folder through SSH.

3778
02:24:44,370 --> 02:24:46,410
So the actual files
that you're manipulating

3779
02:24:46,410 --> 02:24:47,490
are on a cluster somewhere else.

3780
02:24:47,490 --> 02:24:52,290
- So what's the best IDE, VS Code?

3781
02:24:52,290 --> 02:24:53,123
What else do people use?

3782
02:24:53,123 --> 02:24:55,380
So I use Emax, still.

3783
02:24:55,380 --> 02:24:56,600
- That's old school.

3784
02:24:57,540 --> 02:25:00,573
- It may be cool, I don't know
if it's maximum productivity.

3785
02:25:01,440 --> 02:25:04,290
So what do you recommend
in terms of editors?

3786
02:25:04,290 --> 02:25:06,150
You worked with a lot
of software engineers,

3787
02:25:06,150 --> 02:25:11,150
editors for Python, C++,
machine learning applications.

3788
02:25:11,340 --> 02:25:13,470
- I think the current answer is VS Code,

3789
02:25:13,470 --> 02:25:16,710
currently, I believe that's the best IDE.

3790
02:25:16,710 --> 02:25:18,330
It's got a huge amount of extensions.

3791
02:25:18,330 --> 02:25:22,080
It has GitHub Copilot integration,

3792
02:25:22,080 --> 02:25:23,370
which I think is very valuable.

3793
02:25:23,370 --> 02:25:26,670
- What do you think about
the Copilot integration?

3794
02:25:26,670 --> 02:25:28,800
I got to talk a bunch
with Guido van Rossum

3795
02:25:28,800 --> 02:25:32,144
who's the creator of Python
and he loves Copilot,

3796
02:25:32,144 --> 02:25:34,488
he programs a lot with it.

3797
02:25:34,488 --> 02:25:35,513
- [Andrej] Yeah.

3798
02:25:35,513 --> 02:25:36,743
- Do you?

3799
02:25:36,743 --> 02:25:37,576
- Yeah, I use Copilot.

3800
02:25:37,576 --> 02:25:38,409
I love it.

3801
02:25:38,409 --> 02:25:40,710
And it's free for me
but I would pay for it.

3802
02:25:40,710 --> 02:25:41,706
Yeah.

3803
02:25:41,706 --> 02:25:42,539
I think it's very good.

3804
02:25:42,539 --> 02:25:44,430
And the utility that I found with it was,

3805
02:25:44,430 --> 02:25:45,750
I would say there's a learning curve

3806
02:25:45,750 --> 02:25:48,600
and you need to figure
out when it's helpful

3807
02:25:48,600 --> 02:25:50,220
and when to pay attention to its outputs

3808
02:25:50,220 --> 02:25:51,360
and when it's not going to be helpful

3809
02:25:51,360 --> 02:25:53,040
where you should not pay attention to it.

3810
02:25:53,040 --> 02:25:55,020
Because if you're just reading
its suggestions all the time,

3811
02:25:55,020 --> 02:25:56,670
it's not a good way of
interacting with it.

3812
02:25:56,670 --> 02:25:58,920
But I think I was able to
sort of mold myself to it.

3813
02:25:58,920 --> 02:26:01,860
I find it's very helpful,
number one, in copy-paste

3814
02:26:01,860 --> 02:26:03,060
and replace some parts.

3815
02:26:03,060 --> 02:26:05,790
So when the pattern is clear,

3816
02:26:05,790 --> 02:26:07,680
it's really good at
completing the pattern.

3817
02:26:07,680 --> 02:26:10,020
And number two, sometimes it suggests APIs

3818
02:26:10,020 --> 02:26:11,550
that I'm not aware of.

3819
02:26:11,550 --> 02:26:14,460
So it tells you about
something that you didn't know.

3820
02:26:14,460 --> 02:26:16,140
- And that's an opportunity
to discover a new thing.

3821
02:26:16,140 --> 02:26:17,250
- It's an opportunity to.

3822
02:26:17,250 --> 02:26:19,560
So I would never take
Copilot code as given.

3823
02:26:19,560 --> 02:26:22,710
I almost always copy-paste
into a Google search

3824
02:26:22,710 --> 02:26:24,480
and you see what this function is doing

3825
02:26:24,480 --> 02:26:26,130
and then you're like, "Oh,
that's actually exactly

3826
02:26:26,130 --> 02:26:26,963
what I need."

3827
02:26:26,963 --> 02:26:27,796
Thank you, Copilot.

3828
02:26:27,796 --> 02:26:28,629
So you learn something.

3829
02:26:28,629 --> 02:26:29,840
- So it's in part of search engine,

3830
02:26:29,840 --> 02:26:33,900
in part maybe getting the
exact syntax correctly

3831
02:26:33,900 --> 02:26:37,350
that once you see it, it's that MPR thing.

3832
02:26:37,350 --> 02:26:38,970
Once you see it, you know it's correct.

3833
02:26:38,970 --> 02:26:40,230
- [Andrej] Yes, exactly.

3834
02:26:40,230 --> 02:26:41,063
Exactly.

3835
02:26:41,063 --> 02:26:42,057
- You, yourself can struggle.

3836
02:26:42,057 --> 02:26:42,890
- [Andrej] You can verify.

3837
02:26:42,890 --> 02:26:43,723
- You can verify efficiently

3838
02:26:43,723 --> 02:26:45,690
but you can't generate efficiently.

3839
02:26:45,690 --> 02:26:48,750
- And Copilot really, I mean
it's autopilot for programming.

3840
02:26:48,750 --> 02:26:49,590
Right?

3841
02:26:49,590 --> 02:26:51,570
And currently is doing the link following

3842
02:26:51,570 --> 02:26:54,570
which is the simple copy-paste
and sometimes suggest,

3843
02:26:54,570 --> 02:26:57,180
but over time it's going to
become more and more autonomous.

3844
02:26:57,180 --> 02:27:00,060
And so the same thing will
play out in not just coding

3845
02:27:00,060 --> 02:27:02,370
but actually across many, many
different things probably.

3846
02:27:02,370 --> 02:27:04,001
- But coding is an important one, right?

3847
02:27:04,001 --> 02:27:04,834
- [Andrej] Very.

3848
02:27:04,834 --> 02:27:05,667
- Like writing programs.

3849
02:27:05,667 --> 02:27:06,500
- [Andrej] Yeah.

3850
02:27:06,500 --> 02:27:08,580
- How do you see the
future of that developing

3851
02:27:08,580 --> 02:27:11,700
the program synthesis, like
being able to write programs

3852
02:27:11,700 --> 02:27:13,320
that are more and more complicated?

3853
02:27:13,320 --> 02:27:17,910
'Cause right now it's
human-supervised in interesting ways.

3854
02:27:17,910 --> 02:27:19,110
- [Andrej] Yes.

3855
02:27:19,110 --> 02:27:22,050
- It feels like the transition
will be very painful.

3856
02:27:22,050 --> 02:27:24,380
- My mental model for it is
the same thing will happen

3857
02:27:24,380 --> 02:27:26,220
as with the autopilot.

3858
02:27:26,220 --> 02:27:27,650
So currently, he's doing lane following,

3859
02:27:27,650 --> 02:27:30,180
he is doing some simple
stuff, and eventually,

3860
02:27:30,180 --> 02:27:31,290
we'll be doing autonomy

3861
02:27:31,290 --> 02:27:33,240
and people will have to
intervene less and less.

3862
02:27:33,240 --> 02:27:37,650
And those could be like
testing mechanisms.

3863
02:27:37,650 --> 02:27:38,730
If it writes a function

3864
02:27:38,730 --> 02:27:41,430
and that function looks
pretty damn correct.

3865
02:27:41,430 --> 02:27:43,140
But how do you know it's correct

3866
02:27:43,140 --> 02:27:44,940
'cause you're like
getting lazier and lazier

3867
02:27:44,940 --> 02:27:48,720
as a programmer, your ability
to, 'cause like little bugs

3868
02:27:48,720 --> 02:27:50,910
but I guess it won't make little mistakes.

3869
02:27:50,910 --> 02:27:53,520
- No it will, Copilot will make off

3870
02:27:53,520 --> 02:27:54,690
by one subtle bug.

3871
02:27:54,690 --> 02:27:55,830
It has done that to me.

3872
02:27:55,830 --> 02:27:57,390
- But do you think future systems will

3873
02:27:57,390 --> 02:28:00,330
or is it really the off by one

3874
02:28:00,330 --> 02:28:02,880
is actually a fundamental
challenge of programming.

3875
02:28:02,880 --> 02:28:04,740
- In that case it wasn't fundamental

3876
02:28:04,740 --> 02:28:07,140
and I think things can improve but yeah,

3877
02:28:07,140 --> 02:28:08,460
I think humans have to supervise.

3878
02:28:08,460 --> 02:28:11,160
I am nervous about people not
supervising what comes out

3879
02:28:11,160 --> 02:28:12,900
and what happens to, for example,

3880
02:28:12,900 --> 02:28:15,390
the proliferation of bugs
in all of our systems.

3881
02:28:15,390 --> 02:28:17,970
I'm nervous about that but I
think there will probably be

3882
02:28:17,970 --> 02:28:19,800
some other Copilots for bug finding

3883
02:28:19,800 --> 02:28:21,300
and stuff like that, at some point.

3884
02:28:21,300 --> 02:28:23,400
'Cause there'll be a
lot more automation for-

3885
02:28:23,400 --> 02:28:27,717
- Oh man, a Copilot that
generates a compiler,

3886
02:28:31,158 --> 02:28:32,417
one that does a linter.

3887
02:28:32,417 --> 02:28:33,250
- [Andrej] Yes.

3888
02:28:33,250 --> 02:28:35,460
- One that does like a type checker.

3889
02:28:35,460 --> 02:28:36,293
- Yeah.

3890
02:28:37,710 --> 02:28:40,410
It's a committee of a GPT sort of like-

3891
02:28:40,410 --> 02:28:42,300
- And then there'll be like
a manager for the committee.

3892
02:28:42,300 --> 02:28:43,133
- [Andrej] Yeah.

3893
02:28:43,133 --> 02:28:44,370
- And then there'll be somebody that says,

3894
02:28:44,370 --> 02:28:45,780
a new version of this is needed.

3895
02:28:45,780 --> 02:28:47,149
We need to regenerate it.

3896
02:28:47,149 --> 02:28:47,982
- Yeah.

3897
02:28:47,982 --> 02:28:48,960
There were 10 GPTs that were forwarded

3898
02:28:48,960 --> 02:28:50,250
and gave 50 suggestions.

3899
02:28:50,250 --> 02:28:52,020
Another one looked at it and picked a few

3900
02:28:52,020 --> 02:28:54,367
that they like, a bug one looked at it

3901
02:28:54,367 --> 02:28:56,370
and it was like, it's probably
a bug, they got re-ranked

3902
02:28:56,370 --> 02:28:59,867
by some other thing and then
a final ensemble GPT comes in

3903
02:28:59,867 --> 02:29:01,523
and is like, okay, given
everything you guys have told me,

3904
02:29:01,523 --> 02:29:03,153
this is probably the next token.

3905
02:29:04,170 --> 02:29:05,940
- You know the feeling is
the number of programmers

3906
02:29:05,940 --> 02:29:08,355
in the world has been growing
and growing very quickly.

3907
02:29:08,355 --> 02:29:09,188
- [Andrej] Yeah.

3908
02:29:09,188 --> 02:29:10,800
- Do you think it's possible
that it'll actually level out

3909
02:29:10,800 --> 02:29:14,520
and drop to a very low
number in this kind of world?

3910
02:29:14,520 --> 02:29:17,710
'Cause then you'll be doing
Software 2.0 programming

3911
02:29:19,170 --> 02:29:21,447
and you'll be doing this generation

3912
02:29:22,539 --> 02:29:25,200
of Copilot-type systems programming.

3913
02:29:25,200 --> 02:29:27,540
But you won't be doing the old school

3914
02:29:27,540 --> 02:29:29,910
Software 1.0 programming.

3915
02:29:29,910 --> 02:29:30,810
- I don't currently think

3916
02:29:30,810 --> 02:29:33,460
that they're just going to
replace human programmers.

3917
02:29:35,359 --> 02:29:37,169
I'm so hesitant saying
stuff like this, right?

3918
02:29:37,169 --> 02:29:38,002
- Yeah.

3919
02:29:38,002 --> 02:29:40,740
Because this is gonna be
replayed in five years and no,

3920
02:29:40,740 --> 02:29:43,583
it's going to show that like
this is where we thought,

3921
02:29:43,583 --> 02:29:45,210
because I agree with you

3922
02:29:45,210 --> 02:29:49,083
but I think we might be
very surprised, right?

3923
02:29:52,980 --> 02:29:55,290
What's your sense of where we
stand with language models?

3924
02:29:55,290 --> 02:29:57,960
Does it feel like the beginning,
or the middle, or the end?

3925
02:29:57,960 --> 02:29:59,460
- The beginning, a hundred percent.

3926
02:29:59,460 --> 02:30:00,810
I think the big question in my mind is

3927
02:30:00,810 --> 02:30:03,120
for sure GPT will be able
to program quite well,

3928
02:30:03,120 --> 02:30:04,084
competently and so on.

3929
02:30:04,084 --> 02:30:04,917
- [Lex] Yeah.

3930
02:30:04,917 --> 02:30:05,820
- How do you steer the system?

3931
02:30:05,820 --> 02:30:07,710
You still have to provide some guidance

3932
02:30:07,710 --> 02:30:09,300
to what you actually are looking for.

3933
02:30:09,300 --> 02:30:11,490
And so how do you steer
it and how do you say,

3934
02:30:11,490 --> 02:30:12,840
how do you talk to it?

3935
02:30:12,840 --> 02:30:15,900
How do you audit it and verify

3936
02:30:15,900 --> 02:30:16,860
that what it's done is correct

3937
02:30:16,860 --> 02:30:18,360
and how do you work with this?

3938
02:30:18,360 --> 02:30:20,460
And it's as much not just an AI problem

3939
02:30:20,460 --> 02:30:22,100
but a UI, UX problem.

3940
02:30:22,100 --> 02:30:23,460
- [Lex] Yeah.

3941
02:30:23,460 --> 02:30:27,390
- So beautiful fertile ground
for so much interesting work

3942
02:30:27,390 --> 02:30:30,006
for VS Code++ where you're not just,

3943
02:30:30,006 --> 02:30:31,170
it's not just human programming anymore.

3944
02:30:31,170 --> 02:30:32,003
It's amazing.

3945
02:30:32,003 --> 02:30:33,130
- Yeah.

3946
02:30:33,130 --> 02:30:33,963
So you're interacting with the system

3947
02:30:33,963 --> 02:30:37,290
so not just one prompt but
it's iterative prompting.

3948
02:30:37,290 --> 02:30:38,284
- [Andrej] Yeah.

3949
02:30:38,284 --> 02:30:39,420
- You're trying to figure
out, having a conversation

3950
02:30:39,420 --> 02:30:40,253
with the system.

3951
02:30:40,253 --> 02:30:41,086
- [Andrej] Yeah.

3952
02:30:41,086 --> 02:30:42,810
- That actually, I mean to
me that's super exciting

3953
02:30:42,810 --> 02:30:45,030
to have a conversation with
the program I'm writing.

3954
02:30:45,030 --> 02:30:45,870
- Yeah.

3955
02:30:45,870 --> 02:30:46,777
Yeah.

3956
02:30:46,777 --> 02:30:47,713
Maybe at some point you're
just conversing with it.

3957
02:30:47,713 --> 02:30:49,860
It's like, okay, here's what I wanna do,

3958
02:30:49,860 --> 02:30:51,780
actually this variable.

3959
02:30:51,780 --> 02:30:54,120
Maybe it's not even that
low-level as a variable, but.

3960
02:30:54,120 --> 02:30:57,000
- You can also imagine
like can you translate

3961
02:30:57,000 --> 02:30:59,280
this to C++ and back
to Python and back to?

3962
02:30:59,280 --> 02:31:00,233
- Yeah, that already kind
of exists in some way.

3963
02:31:00,233 --> 02:31:02,160
- No, but just like doing it as part

3964
02:31:02,160 --> 02:31:03,660
of the program experience.

3965
02:31:03,660 --> 02:31:07,740
Like I think I'd like to
write this function in C++

3966
02:31:07,740 --> 02:31:11,430
or you just keep changing
from different programs

3967
02:31:11,430 --> 02:31:12,957
'cause of the different syntax,

3968
02:31:12,957 --> 02:31:15,690
maybe I want to convert this
into a functional language.

3969
02:31:15,690 --> 02:31:16,523
- [Andrej] Yeah.

3970
02:31:16,523 --> 02:31:20,580
- And so you get to become
multilingual as a programmer

3971
02:31:20,580 --> 02:31:22,440
and dance back and forth efficiently.

3972
02:31:22,440 --> 02:31:23,273
- Yeah.

3973
02:31:23,273 --> 02:31:24,870
I mean I think the UI, UX of it though

3974
02:31:24,870 --> 02:31:26,640
is still very hard to think through.

3975
02:31:26,640 --> 02:31:27,634
- [Lex] Yeah.

3976
02:31:27,634 --> 02:31:29,550
- Because it's not just
about writing code on a page.

3977
02:31:29,550 --> 02:31:31,380
You have an entire developer environment,

3978
02:31:31,380 --> 02:31:33,150
you have a bunch of hardware on it,

3979
02:31:33,150 --> 02:31:34,560
you have some environmental variables,

3980
02:31:34,560 --> 02:31:36,630
you have some scripts that
are running in a Chrome job.

3981
02:31:36,630 --> 02:31:39,450
There's a lot going on
to working with computers

3982
02:31:39,450 --> 02:31:43,560
and how do these systems
set up environment flags,

3983
02:31:43,560 --> 02:31:45,180
and work across multiple machines,

3984
02:31:45,180 --> 02:31:46,230
and set up screen sessions,

3985
02:31:46,230 --> 02:31:47,880
and automate different processes.

3986
02:31:47,880 --> 02:31:51,060
like how all that works and is
auditable by humans and so on

3987
02:31:51,060 --> 02:31:53,340
is like massive question at the moment.

3988
02:31:53,340 --> 02:31:56,040
- You've built arxiv-sanity.

3989
02:31:56,040 --> 02:31:58,380
What is arxiv and what is the future

3990
02:31:58,380 --> 02:32:02,010
of academic research publishing
that you would like to see?

3991
02:32:02,010 --> 02:32:03,750
- So arxiv is this pre-print server.

3992
02:32:03,750 --> 02:32:06,600
So if you have a paper you
can submit it for publication

3993
02:32:06,600 --> 02:32:08,790
to journals or conferences
and then wait six months

3994
02:32:08,790 --> 02:32:10,920
and then maybe get a
decision pass or fail,

3995
02:32:10,920 --> 02:32:13,320
or you can just upload it to arxiv

3996
02:32:13,320 --> 02:32:15,930
and then people can Tweet
about it three minutes later.

3997
02:32:15,930 --> 02:32:17,217
And then everyone sees
it, everyone reads it,

3998
02:32:17,217 --> 02:32:20,536
and everyone can profit from
it in their own little ways.

3999
02:32:20,536 --> 02:32:23,980
- And you can cite it and it
has an official look to it.

4000
02:32:23,980 --> 02:32:27,300
It feels like a publication process.

4001
02:32:27,300 --> 02:32:28,280
- [Andrej] Yeah.

4002
02:32:28,280 --> 02:32:30,420
- It feels different than if
you just put in a blog post.

4003
02:32:30,420 --> 02:32:31,253
- Oh, yeah.

4004
02:32:31,253 --> 02:32:32,179
Yeah.

4005
02:32:32,179 --> 02:32:33,630
I mean it's a paper and
usually the bar is higher

4006
02:32:33,630 --> 02:32:36,090
for something that you
would expect on arxiv

4007
02:32:36,090 --> 02:32:38,070
as opposed to something you
would see in a blog post.

4008
02:32:38,070 --> 02:32:40,980
- Well, the culture created the bar

4009
02:32:40,980 --> 02:32:42,240
'cause you could probably post

4010
02:32:42,240 --> 02:32:44,190
a pretty crappy paper or an arxiv.

4011
02:32:44,190 --> 02:32:45,480
- [Andrej] Yes.

4012
02:32:45,480 --> 02:32:46,697
- So what's that make you feel like,

4013
02:32:46,697 --> 02:32:49,050
what's that make you
feel about peer review?

4014
02:32:49,050 --> 02:32:52,740
So rigorous peer review
by two, three experts

4015
02:32:52,740 --> 02:32:57,180
versus the peer review
of the community right

4016
02:32:57,180 --> 02:32:58,013
as it's written?

4017
02:32:58,013 --> 02:32:58,846
- Yeah.

4018
02:32:58,846 --> 02:33:00,660
Basically, I think the
community is very well able

4019
02:33:00,660 --> 02:33:03,960
to peer-review things
very quickly on Twitter.

4020
02:33:03,960 --> 02:33:05,670
And I think maybe it
just has to do something

4021
02:33:05,670 --> 02:33:07,980
with AI machine learning
field specifically though,

4022
02:33:07,980 --> 02:33:10,470
I feel like things are
more easily auditable

4023
02:33:10,470 --> 02:33:14,070
and the verification is easier potentially

4024
02:33:14,070 --> 02:33:15,660
than the verification somewhere else.

4025
02:33:15,660 --> 02:33:18,360
So it's like, you can think of these

4026
02:33:18,360 --> 02:33:20,220
scientific publications
as little block-chains

4027
02:33:20,220 --> 02:33:21,510
where everyone's building
on each other's work

4028
02:33:21,510 --> 02:33:23,670
and citing each other
and you sort of have AI,

4029
02:33:23,670 --> 02:33:27,120
which is this much faster
and loose blockchain,

4030
02:33:27,120 --> 02:33:30,180
but then you have, and
any one individual entry

4031
02:33:30,180 --> 02:33:32,460
is very cheap to make.

4032
02:33:32,460 --> 02:33:34,200
And then you have other
fields where maybe that model

4033
02:33:34,200 --> 02:33:35,550
doesn't make as much sense.

4034
02:33:36,690 --> 02:33:38,310
And so I think in AI at least

4035
02:33:38,310 --> 02:33:40,260
things are pretty easily verifiable.

4036
02:33:40,260 --> 02:33:41,907
And so that's why when
people upload papers,

4037
02:33:41,907 --> 02:33:43,410
they have a really good idea and so on.

4038
02:33:43,410 --> 02:33:45,810
People can try it out the next day

4039
02:33:45,810 --> 02:33:48,150
and they can be the final arbiter
of whether it works or not

4040
02:33:48,150 --> 02:33:49,080
on their problem.

4041
02:33:49,080 --> 02:33:51,540
And the whole thing just
moves significantly faster.

4042
02:33:51,540 --> 02:33:54,000
So I feel like academia still has a place,

4043
02:33:54,000 --> 02:33:56,490
sorry, this conference, journal
process still has a place,

4044
02:33:56,490 --> 02:33:59,760
but it's sort of it lags behind I think,

4045
02:33:59,760 --> 02:34:03,150
and it's a bit more maybe
higher quality process.

4046
02:34:03,150 --> 02:34:05,850
But it's not the place
where you will discover

4047
02:34:05,850 --> 02:34:07,110
cutting-edge work anymore.

4048
02:34:07,110 --> 02:34:07,943
- [Lex] Yeah.

4049
02:34:07,943 --> 02:34:09,090
- It used to be the case
when I was starting my PhD

4050
02:34:09,090 --> 02:34:10,920
that you go to conferences and journals

4051
02:34:10,920 --> 02:34:12,540
and you discuss all the latest research.

4052
02:34:12,540 --> 02:34:14,460
Now when you go to a
conference or a journal,

4053
02:34:14,460 --> 02:34:15,990
no one discusses anything that's there

4054
02:34:15,990 --> 02:34:19,290
because it's already like three
generations ago, irrelevant.

4055
02:34:19,290 --> 02:34:20,123
- Yes.

4056
02:34:20,123 --> 02:34:22,410
Which makes me sad about
like DeepMind for example,

4057
02:34:22,410 --> 02:34:25,020
where they still publish in nature

4058
02:34:25,020 --> 02:34:27,930
and these big prestigious,
I mean there's still value,

4059
02:34:27,930 --> 02:34:29,976
I suppose to the prestige that
comes with these big venues.

4060
02:34:29,976 --> 02:34:30,810
- [Andrej] Yeah.

4061
02:34:30,810 --> 02:34:34,170
- But the result is that they'll announce

4062
02:34:34,170 --> 02:34:36,030
some breakthrough performance

4063
02:34:36,030 --> 02:34:39,660
and it'll take like a year to
actually publish the details.

4064
02:34:39,660 --> 02:34:42,660
I mean, and those details,

4065
02:34:42,660 --> 02:34:44,340
if they were published
immediately would inspire

4066
02:34:44,340 --> 02:34:46,920
the community to move in
certain directions, would they?

4067
02:34:46,920 --> 02:34:47,872
- Yeah.

4068
02:34:47,872 --> 02:34:48,705
It would speed up the
rest of the community,

4069
02:34:48,705 --> 02:34:50,820
but I don't know to what
extent that's part of their

4070
02:34:50,820 --> 02:34:52,230
objective function also.

4071
02:34:52,230 --> 02:34:53,063
- That's true.

4072
02:34:53,063 --> 02:34:54,312
So it's not just the prestige.

4073
02:34:54,312 --> 02:34:57,000
A little bit of the delay is part.

4074
02:34:57,000 --> 02:34:58,800
- Yeah, they certainly,
DeepMind specifically,

4075
02:34:58,800 --> 02:35:00,960
has been working in the regime

4076
02:35:00,960 --> 02:35:03,960
of having slightly higher
quality basically process

4077
02:35:03,960 --> 02:35:07,230
and latency and publishing
those papers that way.

4078
02:35:07,230 --> 02:35:09,120
- Another question from Reddit.

4079
02:35:09,120 --> 02:35:12,390
Do you or have you suffered
from imposter syndrome,

4080
02:35:12,390 --> 02:35:16,980
being the director of AI
Tesla, being this person

4081
02:35:16,980 --> 02:35:19,470
when you're at Stanford
where the world looks at you

4082
02:35:19,470 --> 02:35:23,760
as the expert in AI to teach the world

4083
02:35:23,760 --> 02:35:25,500
about machine learning.

4084
02:35:25,500 --> 02:35:27,240
- When I was leaving
Tesla after five years,

4085
02:35:27,240 --> 02:35:29,910
I spent a ton of time in meeting rooms

4086
02:35:29,910 --> 02:35:31,830
and I would read papers.

4087
02:35:31,830 --> 02:35:33,780
In the beginning, when I joined
Tesla, I was writing code

4088
02:35:33,780 --> 02:35:34,857
and then I was writing less and less code,

4089
02:35:34,857 --> 02:35:36,570
and I was reading code
and then I was reading

4090
02:35:36,570 --> 02:35:37,710
less and less code.

4091
02:35:37,710 --> 02:35:39,120
And so this is just a natural progression

4092
02:35:39,120 --> 02:35:40,170
that happens I think.

4093
02:35:40,170 --> 02:35:42,720
And definitely, I would
say near the tail end,

4094
02:35:42,720 --> 02:35:44,400
that's when it starts
to hit you a bit more.

4095
02:35:44,400 --> 02:35:45,420
That you're supposed to be an expert

4096
02:35:45,420 --> 02:35:48,060
but actually, the source
of truth is the code

4097
02:35:48,060 --> 02:35:49,200
that people are writing, the GitHub

4098
02:35:49,200 --> 02:35:52,020
and the actual code itself.

4099
02:35:52,020 --> 02:35:54,420
And you're not as familiar
with that as you used to be.

4100
02:35:54,420 --> 02:35:57,240
And so I would say maybe
there's some insecurity there.

4101
02:35:57,240 --> 02:35:59,070
- Yeah, that's actually pretty profound,

4102
02:35:59,070 --> 02:36:00,660
that a lot of the insecurity has to do

4103
02:36:00,660 --> 02:36:01,710
with not writing the code

4104
02:36:01,710 --> 02:36:05,003
in the computer science space
'cause that is the truth.

4105
02:36:05,003 --> 02:36:06,117
That right there.

4106
02:36:06,117 --> 02:36:06,960
- The code is the source of truth.

4107
02:36:06,960 --> 02:36:09,900
The papers and everything else,
it's a high-level summary.

4108
02:36:09,900 --> 02:36:12,420
I don't, yeah, just a high-level summary,

4109
02:36:12,420 --> 02:36:13,770
but at the end of the day
you have to read code.

4110
02:36:13,770 --> 02:36:15,270
It's impossible to translate all that code

4111
02:36:15,270 --> 02:36:18,570
into actual paper form.

4112
02:36:18,570 --> 02:36:20,310
So when things come out,

4113
02:36:20,310 --> 02:36:21,630
especially when they have
a source code available,

4114
02:36:21,630 --> 02:36:23,160
that's my favorite place to go.

4115
02:36:23,160 --> 02:36:25,560
- So like I said, you're
one of the greatest teachers

4116
02:36:25,560 --> 02:36:30,560
of machine learning AI
ever from CS231n to today,

4117
02:36:31,710 --> 02:36:33,720
what advice would you give to beginners

4118
02:36:33,720 --> 02:36:36,540
interested in getting
into machine learning?

4119
02:36:36,540 --> 02:36:40,530
- Beginners are often
focused on what to do

4120
02:36:40,530 --> 02:36:43,440
and I think the focus should
be more how much you do.

4121
02:36:43,440 --> 02:36:45,390
So I am a believer on the high-level,

4122
02:36:45,390 --> 02:36:48,990
in this 10,000 hours concept
where you just have to

4123
02:36:48,990 --> 02:36:50,550
just pick the things
where you can spend time

4124
02:36:50,550 --> 02:36:52,350
and you care about and
you're interested in.

4125
02:36:52,350 --> 02:36:55,140
You literally have to put
in 10,000 hours of work.

4126
02:36:55,140 --> 02:36:57,690
It doesn't even matter
as much where you put it,

4127
02:36:57,690 --> 02:36:59,490
you'll iterate and you'll improve

4128
02:36:59,490 --> 02:37:00,600
and you'll waste some time.

4129
02:37:00,600 --> 02:37:01,620
I dunno if there's a better way.

4130
02:37:01,620 --> 02:37:03,600
You need to put in 10,000 hours.

4131
02:37:03,600 --> 02:37:04,560
But I think it's actually really nice

4132
02:37:04,560 --> 02:37:06,420
'cause I feel like there's
some sense of determinism

4133
02:37:06,420 --> 02:37:08,610
about being an expert at a thing

4134
02:37:08,610 --> 02:37:10,020
if you spend 10,000 hours.

4135
02:37:10,020 --> 02:37:12,570
you can literally pick an arbitrary thing

4136
02:37:12,570 --> 02:37:14,250
and I think if you spend 10,000 hours

4137
02:37:14,250 --> 02:37:15,630
of deliberate effort and work,

4138
02:37:15,630 --> 02:37:17,760
you actually will become an expert at it.

4139
02:37:17,760 --> 02:37:20,583
And so I think it's like a nice thought.

4140
02:37:21,510 --> 02:37:24,480
And so basically I would focus more on

4141
02:37:24,480 --> 02:37:26,310
are you spending 10,000 hours?

4142
02:37:26,310 --> 02:37:27,660
That's what I would focus on.

4143
02:37:27,660 --> 02:37:30,720
- And then thinking about what
kind of mechanisms maximize

4144
02:37:30,720 --> 02:37:32,150
your likelihood of
getting to 10,000 hours.

4145
02:37:32,150 --> 02:37:33,250
- [Andrej] Yes, exactly.

4146
02:37:33,250 --> 02:37:36,810
- Which for us silly humans
means probably forming

4147
02:37:36,810 --> 02:37:40,260
a daily habit of every single
day actually doing thing.

4148
02:37:40,260 --> 02:37:41,190
- Whatever helps you.

4149
02:37:41,190 --> 02:37:42,300
So I do think to a large extent

4150
02:37:42,300 --> 02:37:44,040
it's a psychological problem for yourself.

4151
02:37:44,040 --> 02:37:44,873
- [Lex] Yeah.

4152
02:37:44,873 --> 02:37:46,950
- One other thing that I think is helpful

4153
02:37:46,950 --> 02:37:48,120
for the psychology of it,

4154
02:37:48,120 --> 02:37:49,710
is many times people compare themselves

4155
02:37:49,710 --> 02:37:52,290
to others in the area, I
think this very harmful.

4156
02:37:52,290 --> 02:37:55,020
Only compare yourself to
you from some time ago.

4157
02:37:55,020 --> 02:37:58,020
Like say a year ago, are you
better than you a year ago?

4158
02:37:58,020 --> 02:38:00,050
This is the only way to think.

4159
02:38:00,050 --> 02:38:02,220
And I think then you can see your progress

4160
02:38:02,220 --> 02:38:03,480
and it's very motivating.

4161
02:38:03,480 --> 02:38:04,350
- That's so interesting.

4162
02:38:04,350 --> 02:38:07,490
That focus on the quantity of hours.

4163
02:38:07,490 --> 02:38:10,110
'Cause I think a lot of
people in the beginner stage

4164
02:38:10,110 --> 02:38:15,110
but actually throughout get
paralyzed by the choice.

4165
02:38:15,480 --> 02:38:16,700
- [Andrej] Yeah.

4166
02:38:16,700 --> 02:38:19,140
- Like which one do I pick
this path or this path?

4167
02:38:19,140 --> 02:38:19,973
- [Andrej] Yeah.

4168
02:38:19,973 --> 02:38:22,620
- They'll literally get
paralyzed by which IDE to use?

4169
02:38:22,620 --> 02:38:23,460
- Well, they're worried, yeah,

4170
02:38:23,460 --> 02:38:24,720
they'll worried about all these things.

4171
02:38:24,720 --> 02:38:28,260
But the thing is, you will waste
time doing something wrong.

4172
02:38:28,260 --> 02:38:29,093
- [Lex] Yes.

4173
02:38:29,093 --> 02:38:29,940
- You will eventually
figure out it's not right.

4174
02:38:29,940 --> 02:38:31,650
You will accumulate scar tissue

4175
02:38:31,650 --> 02:38:33,420
and next time you'll grow stronger

4176
02:38:33,420 --> 02:38:35,100
because next time you'll
have the scar tissue,

4177
02:38:35,100 --> 02:38:36,660
and next time you'll learn from it.

4178
02:38:36,660 --> 02:38:39,030
And now next time you come
to a similar situation

4179
02:38:39,030 --> 02:38:41,610
you'll be like, oh, I messed up.

4180
02:38:41,610 --> 02:38:43,470
I've spent a lot of time working on things

4181
02:38:43,470 --> 02:38:45,270
that never materialized into anything

4182
02:38:45,270 --> 02:38:46,350
and I have all that scar tissue

4183
02:38:46,350 --> 02:38:48,420
and I have some intuitions
about what was useful,

4184
02:38:48,420 --> 02:38:50,640
what wasn't useful, how things turned out.

4185
02:38:50,640 --> 02:38:54,000
So all those mistakes were not dead work.

4186
02:38:54,000 --> 02:38:56,610
So I just think they should
just focus on working.

4187
02:38:56,610 --> 02:38:59,060
What have you done, what
have you done last week?

4188
02:39:00,720 --> 02:39:01,650
- That's a good question actually

4189
02:39:01,650 --> 02:39:04,683
to ask for a lot of things,
not just machine learning.

4190
02:39:05,640 --> 02:39:09,570
It's a good way to cut the,
I forgot the term we use,

4191
02:39:09,570 --> 02:39:10,770
but the fluff, the blubber,

4192
02:39:10,770 --> 02:39:15,000
whatever the inefficiencies in life.

4193
02:39:15,000 --> 02:39:17,130
What do you love about teaching?

4194
02:39:17,130 --> 02:39:21,690
You seem to find yourself often
in the, drawn to teaching.

4195
02:39:21,690 --> 02:39:23,135
You're very good at it but
you're also drawn to it.

4196
02:39:23,135 --> 02:39:25,260
- Yeah, I mean I don't
think I love teaching.

4197
02:39:25,260 --> 02:39:29,970
I love happy humans and happy
humans like when I teach.

4198
02:39:29,970 --> 02:39:31,140
- Yes.

4199
02:39:31,140 --> 02:39:33,120
- I wouldn't say I hate
teaching, I tolerate teaching.

4200
02:39:33,120 --> 02:39:34,211
- [Lex] Yes.

4201
02:39:34,211 --> 02:39:35,520
- But it's not like the act
of teaching that I like,

4202
02:39:35,520 --> 02:39:40,520
it's that I have something,
I'm actually okay at it.

4203
02:39:40,890 --> 02:39:41,723
- [Lex] Yes.

4204
02:39:41,723 --> 02:39:43,590
- I'm okay at teaching and
people appreciate it a lot.

4205
02:39:43,590 --> 02:39:44,423
- [Lex] Yeah.

4206
02:39:44,423 --> 02:39:47,190
- And so I'm just happy
to try to be helpful

4207
02:39:47,190 --> 02:39:50,197
and teaching itself is not like the most,

4208
02:39:50,197 --> 02:39:52,650
I mean it can be really
annoying, frustrating.

4209
02:39:52,650 --> 02:39:54,570
I was working on a bunch
of lectures just now.

4210
02:39:54,570 --> 02:39:56,970
I was reminded back to my days of 231n

4211
02:39:56,970 --> 02:39:59,430
just how much work it is to
create some of these materials

4212
02:39:59,430 --> 02:40:00,420
and make them good.

4213
02:40:00,420 --> 02:40:01,710
The amount of iteration and thought

4214
02:40:01,710 --> 02:40:03,120
and you go down blind alleys

4215
02:40:03,120 --> 02:40:04,830
and just how much you change it.

4216
02:40:04,830 --> 02:40:08,580
So creating something good
in terms of educational value

4217
02:40:08,580 --> 02:40:11,880
is really hard and it's not fun.

4218
02:40:11,880 --> 02:40:12,713
- It's difficult.

4219
02:40:12,713 --> 02:40:13,830
So people should definitely

4220
02:40:13,830 --> 02:40:16,500
go watch your new stuff you put out.

4221
02:40:16,500 --> 02:40:18,435
There are lectures where you're
actually building the thing,

4222
02:40:18,435 --> 02:40:20,820
like you said, "The code is truth."

4223
02:40:20,820 --> 02:40:24,210
So discussing back-propagation
by building it,

4224
02:40:24,210 --> 02:40:25,087
by looking through and
just the whole thing.

4225
02:40:25,087 --> 02:40:26,190
- [Andrej] Yeah.

4226
02:40:26,190 --> 02:40:27,323
- So how difficult is that to prepare for?

4227
02:40:27,323 --> 02:40:30,420
I think that's a really
powerful way to teach.

4228
02:40:30,420 --> 02:40:31,680
Did you have to prepare for that

4229
02:40:31,680 --> 02:40:34,470
or are you just live thinking through it?

4230
02:40:34,470 --> 02:40:36,570
- I will typically do like say three takes

4231
02:40:36,570 --> 02:40:38,730
and then I take the better take.

4232
02:40:38,730 --> 02:40:40,950
So I do multiple takes and I
take some of the better takes

4233
02:40:40,950 --> 02:40:42,990
and then I just build
out a lecture that way.

4234
02:40:42,990 --> 02:40:45,240
Sometimes I have to delete
30 minutes of content.

4235
02:40:45,240 --> 02:40:46,272
- [Lex] Yeah.

4236
02:40:46,272 --> 02:40:47,105
- Because it just went down the alley

4237
02:40:47,105 --> 02:40:47,938
that I didn't like too much.

4238
02:40:47,938 --> 02:40:49,680
So there's about a bunch of iteration

4239
02:40:49,680 --> 02:40:52,470
and it probably takes me
somewhere around 10 hours

4240
02:40:52,470 --> 02:40:53,520
to create one hour of content.

4241
02:40:53,520 --> 02:40:54,780
- To get one hour.

4242
02:40:54,780 --> 02:40:55,613
It's interesting.

4243
02:40:55,613 --> 02:40:58,950
I mean is it difficult
to go back to the basics?

4244
02:40:58,950 --> 02:41:02,230
Do you draw a lot of wisdom
from going back to the basics?

4245
02:41:02,230 --> 02:41:04,380
- Yeah, going back to
backropagation, loss functions,

4246
02:41:04,380 --> 02:41:05,220
where they come from.

4247
02:41:05,220 --> 02:41:07,740
And one thing I like about
teaching a lot honestly is

4248
02:41:07,740 --> 02:41:10,380
it definitely strengthens
your understanding.

4249
02:41:10,380 --> 02:41:12,660
So it's not a purely altruistic activity,

4250
02:41:12,660 --> 02:41:13,800
it's a way to learn.

4251
02:41:13,800 --> 02:41:16,740
If you have to explain
something to someone,

4252
02:41:16,740 --> 02:41:19,470
you realize you have gaps in knowledge.

4253
02:41:19,470 --> 02:41:22,690
And so I even surprised
myself in those lectures like,

4254
02:41:22,690 --> 02:41:24,690
well, the result will
obviously look like this

4255
02:41:24,690 --> 02:41:25,860
and then the result doesn't look like it.

4256
02:41:25,860 --> 02:41:27,870
And I'm like, okay, I
thought I understood this.

4257
02:41:27,870 --> 02:41:28,703
- Yeah.

4258
02:41:30,162 --> 02:41:32,610
Well, that's why it's really
cool, they literally code,

4259
02:41:32,610 --> 02:41:35,220
you run it in the notebook
and it gives you a result

4260
02:41:35,220 --> 02:41:36,630
and you're like, oh, wow.

4261
02:41:36,630 --> 02:41:37,626
- [Andrej] Yes.

4262
02:41:37,626 --> 02:41:39,814
- And like actual numbers,
actual input, actual code.

4263
02:41:39,814 --> 02:41:40,647
- Yeah.

4264
02:41:40,647 --> 02:41:41,580
It's not mathematical symbols, et cetera.

4265
02:41:41,580 --> 02:41:43,020
The source of truth is the code.

4266
02:41:43,020 --> 02:41:45,990
It's not slides, it's
just like let's build it.

4267
02:41:45,990 --> 02:41:46,823
- It's beautiful.

4268
02:41:46,823 --> 02:41:48,780
You're a rare human in that sense.

4269
02:41:48,780 --> 02:41:51,840
What advice would you give to researchers

4270
02:41:51,840 --> 02:41:54,420
trying to develop and publish an idea

4271
02:41:54,420 --> 02:41:56,850
that have a big impact in the world of AI?

4272
02:41:56,850 --> 02:42:01,650
So maybe undergrads, maybe
early-graduate students.

4273
02:42:01,650 --> 02:42:02,769
- Yeah.

4274
02:42:02,769 --> 02:42:04,547
I mean I would say they
definitely have to be

4275
02:42:04,547 --> 02:42:06,690
a little bit more
strategic than I had to be

4276
02:42:06,690 --> 02:42:09,720
as a PhD student because
of the way AI is evolving,

4277
02:42:09,720 --> 02:42:11,430
it's going the way of physics.

4278
02:42:11,430 --> 02:42:13,920
Where in physics you used
to be able to do experiments

4279
02:42:13,920 --> 02:42:15,420
on your bench-top and everything was great

4280
02:42:15,420 --> 02:42:17,813
and you could make progress
and now you have to work

4281
02:42:17,813 --> 02:42:21,690
in like LHC or like CERN and so AI

4282
02:42:21,690 --> 02:42:23,820
is going in that direction as well.

4283
02:42:23,820 --> 02:42:25,740
So there's certain kinds of things

4284
02:42:25,740 --> 02:42:28,230
that's just not possible to
do on the bench-top anymore.

4285
02:42:28,230 --> 02:42:32,510
And I think that didn't used
to be the case at the time.

4286
02:42:32,510 --> 02:42:37,170
- Do you still think that
there's like GAN type papers

4287
02:42:37,170 --> 02:42:39,662
to be written where like very simple idea.

4288
02:42:39,662 --> 02:42:40,912
- [Andrej] Yes.

4289
02:42:41,820 --> 02:42:43,230
- That requires just one computer

4290
02:42:43,230 --> 02:42:44,430
to illustrate a simple example?

4291
02:42:44,430 --> 02:42:46,740
- I mean one example that's
been very influential recently

4292
02:42:46,740 --> 02:42:49,320
is diffusion models,
diffusion models are amazing.

4293
02:42:49,320 --> 02:42:51,780
Diffusion models are six years old.

4294
02:42:51,780 --> 02:42:53,880
For the longest time,
people were ignoring them

4295
02:42:53,880 --> 02:42:55,020
as far as I can tell.

4296
02:42:55,020 --> 02:42:57,210
And they're an amazing generative model,

4297
02:42:57,210 --> 02:43:00,270
especially in images and so
stable diffusion and so on,

4298
02:43:00,270 --> 02:43:01,770
it's all diffusion-based.

4299
02:43:01,770 --> 02:43:05,010
Diffusion is new, it was
not there and it came from,

4300
02:43:05,010 --> 02:43:06,720
well, it came from Google but a researcher

4301
02:43:06,720 --> 02:43:07,553
could have come up with it.

4302
02:43:07,553 --> 02:43:09,900
In fact, some of the first, actually no,

4303
02:43:09,900 --> 02:43:11,820
those came from Google as well.

4304
02:43:11,820 --> 02:43:13,260
But a researcher could come up with that

4305
02:43:13,260 --> 02:43:15,240
in an academic institution.

4306
02:43:15,240 --> 02:43:16,230
- Yeah.

4307
02:43:16,230 --> 02:43:17,880
What do you find most fascinating
about diffusion models?

4308
02:43:17,880 --> 02:43:22,650
So from the societal impact
of the technical architecture.

4309
02:43:22,650 --> 02:43:25,580
- What I like about diffusion
is it works so well.

4310
02:43:25,580 --> 02:43:26,880
- Is that surprising to you?

4311
02:43:26,880 --> 02:43:30,120
The amount of the variety,
almost the novelty

4312
02:43:30,120 --> 02:43:32,760
of the synthetic data it's generating?

4313
02:43:32,760 --> 02:43:36,210
- Yeah, so the stable diffusion
images are incredible.

4314
02:43:36,210 --> 02:43:39,390
It's the speed of improvement
in generating images

4315
02:43:39,390 --> 02:43:40,950
has been insane.

4316
02:43:40,950 --> 02:43:43,470
We went very quickly from
generating tiny digits

4317
02:43:43,470 --> 02:43:45,540
to tiny faces and it all looked messed up.

4318
02:43:45,540 --> 02:43:46,770
And now we have stable diffusion

4319
02:43:46,770 --> 02:43:48,120
and that happened very quickly.

4320
02:43:48,120 --> 02:43:50,400
There's a lot that academia
can still contribute.

4321
02:43:50,400 --> 02:43:54,270
For example, FlashAttention
is a very efficient kernel

4322
02:43:54,270 --> 02:43:57,390
for running the attention
operation inside the transformer

4323
02:43:57,390 --> 02:43:59,670
that came from academic environment.

4324
02:43:59,670 --> 02:44:01,943
It's a very clever way
to structure the kernel,

4325
02:44:02,820 --> 02:44:03,810
that's the calculation.

4326
02:44:03,810 --> 02:44:06,160
So it doesn't materialize
the attention matrix.

4327
02:44:07,110 --> 02:44:08,760
And so, I think there's
still like lots of things

4328
02:44:08,760 --> 02:44:11,130
to contribute but you have
to be just more strategic.

4329
02:44:11,130 --> 02:44:13,733
- Do you think neural networks
could be made to reason?

4330
02:44:14,700 --> 02:44:16,080
- Yes.

4331
02:44:16,080 --> 02:44:17,640
- Do you think they already reason?

4332
02:44:17,640 --> 02:44:18,473
- Yes.

4333
02:44:18,473 --> 02:44:20,123
- [Lex] What's your
definition of reasoning?

4334
02:44:21,090 --> 02:44:22,340
- Information processing.

4335
02:44:24,690 --> 02:44:26,850
- So in the way that humans
think through a problem

4336
02:44:26,850 --> 02:44:31,850
and come up with novel ideas,
it feels like a reasoning.

4337
02:44:33,540 --> 02:44:34,373
- Yeah.

4338
02:44:34,373 --> 02:44:38,250
- So the novelty, I don't wanna say

4339
02:44:38,250 --> 02:44:43,250
but auto-distribution ideas,
you think it's possible?

4340
02:44:43,380 --> 02:44:44,213
- Yes.

4341
02:44:44,213 --> 02:44:45,240
And I think we're seeing that already

4342
02:44:45,240 --> 02:44:46,440
in the current neural nets.

4343
02:44:46,440 --> 02:44:48,960
You're able to remix the
training set information

4344
02:44:48,960 --> 02:44:51,060
into true generalization in some sense.

4345
02:44:51,060 --> 02:44:52,500
- That doesn't appear-

4346
02:44:52,500 --> 02:44:54,840
- It doesn't appear verbatim
in the training set.

4347
02:44:54,840 --> 02:44:56,460
You're doing something
interesting algorithmically,

4348
02:44:56,460 --> 02:44:59,100
you're manipulating some symbols

4349
02:44:59,100 --> 02:45:03,240
and you're coming up with
some correct unique answer

4350
02:45:03,240 --> 02:45:04,770
in a new setting.

4351
02:45:04,770 --> 02:45:08,460
- What would illustrate to you, holy shit,

4352
02:45:08,460 --> 02:45:10,160
this thing is definitely thinking?

4353
02:45:11,220 --> 02:45:12,780
- To me thinking or reasoning

4354
02:45:12,780 --> 02:45:15,270
is just information
processing and generalization.

4355
02:45:15,270 --> 02:45:18,000
And I think the neural
nets already do that today.

4356
02:45:18,000 --> 02:45:19,770
- So being able to perceive the world

4357
02:45:19,770 --> 02:45:22,620
or perceive the, whatever the inputs are

4358
02:45:22,620 --> 02:45:27,060
and to make predictions based on that

4359
02:45:27,060 --> 02:45:29,040
or actions based on that's reasoning?

4360
02:45:29,040 --> 02:45:29,983
- Yeah.

4361
02:45:29,983 --> 02:45:32,333
You're giving correct
answers in novel settings

4362
02:45:33,240 --> 02:45:34,860
by manipulating information.

4363
02:45:34,860 --> 02:45:36,570
You've learned the correct algorithm,

4364
02:45:36,570 --> 02:45:38,190
you're not doing just some
kind of a lookup table

4365
02:45:38,190 --> 02:45:39,570
and nearest neighbor search.

4366
02:45:39,570 --> 02:45:40,590
Something like that.

4367
02:45:40,590 --> 02:45:42,030
- Let me ask you about AGI.

4368
02:45:42,030 --> 02:45:43,770
What are some moonshot ideas

4369
02:45:43,770 --> 02:45:47,667
you think might make
significant progress towards AGI

4370
02:45:47,667 --> 02:45:49,350
and maybe another way is,

4371
02:45:49,350 --> 02:45:52,380
what are the big blockers
that we're missing now?

4372
02:45:52,380 --> 02:45:54,810
- So basically, I am fairly
bullish on our ability

4373
02:45:54,810 --> 02:45:59,460
to build AGIs, basically automated systems

4374
02:45:59,460 --> 02:46:02,370
that we can interact with
that are very human-like

4375
02:46:02,370 --> 02:46:03,203
and we can interact with them

4376
02:46:03,203 --> 02:46:05,550
in a digital realm or a physical realm.

4377
02:46:05,550 --> 02:46:07,893
Currently, it seems most of the models

4378
02:46:07,893 --> 02:46:11,103
that do these magical
tasks are in a text realm.

4379
02:46:12,900 --> 02:46:16,860
I think, as I mentioned, I'm
suspicious that text realm

4380
02:46:16,860 --> 02:46:19,460
is not enough to actually
build full understanding

4381
02:46:19,460 --> 02:46:20,460
of the world.

4382
02:46:20,460 --> 02:46:22,230
I do actually think you
need to go into pixels

4383
02:46:22,230 --> 02:46:24,930
and understand the physical
world and how it works.

4384
02:46:24,930 --> 02:46:26,730
So I do think that we need
to extend these models

4385
02:46:26,730 --> 02:46:30,120
to consume images and videos
and train on a lot more data

4386
02:46:30,120 --> 02:46:31,863
that is multimodal in that way.

4387
02:46:32,801 --> 02:46:33,960
- Do you think you need to touch the world

4388
02:46:33,960 --> 02:46:35,010
to understand it also?

4389
02:46:35,010 --> 02:46:36,180
- Well, that's the big open question

4390
02:46:36,180 --> 02:46:38,400
I would say in my mind,
is if you also require

4391
02:46:38,400 --> 02:46:42,510
the embodiment and the ability
to interact with the world,

4392
02:46:42,510 --> 02:46:45,540
run experiments and have
a data of that form,

4393
02:46:45,540 --> 02:46:47,691
then you need to go to Optimus
or something like that.

4394
02:46:47,691 --> 02:46:48,600
- [Lex] Yeah.

4395
02:46:48,600 --> 02:46:51,400
- And so I would say Optimus
in some way is like a hedge

4396
02:46:52,770 --> 02:46:57,360
in AGI because it seems
to me that it's possible

4397
02:46:57,360 --> 02:47:00,300
that just having data from
the internet is not enough.

4398
02:47:00,300 --> 02:47:04,290
If that is the case, then
Optimus may lead to AGI.

4399
02:47:04,290 --> 02:47:06,510
Because Optimus would, to me,

4400
02:47:06,510 --> 02:47:07,920
there's nothing beyond Optimus.

4401
02:47:07,920 --> 02:47:09,420
You have like this humanoid form factor

4402
02:47:09,420 --> 02:47:11,370
that can actually do stuff in the world.

4403
02:47:11,370 --> 02:47:12,330
You can have millions of them

4404
02:47:12,330 --> 02:47:14,550
interacting with humans and so on.

4405
02:47:14,550 --> 02:47:18,270
And if that doesn't give a
rise to AGI at some point,

4406
02:47:18,270 --> 02:47:20,130
I'm not sure what will.

4407
02:47:20,130 --> 02:47:21,870
So from a completeness perspective,

4408
02:47:21,870 --> 02:47:24,810
I think that's a really good platform

4409
02:47:24,810 --> 02:47:26,430
but it's a much more harder platform

4410
02:47:26,430 --> 02:47:28,680
because you are dealing with atoms

4411
02:47:28,680 --> 02:47:30,510
and you need to actually
build these things

4412
02:47:30,510 --> 02:47:32,730
and integrate them into society.

4413
02:47:32,730 --> 02:47:34,980
So I think that path takes longer

4414
02:47:34,980 --> 02:47:36,630
but it's much more certain.

4415
02:47:36,630 --> 02:47:38,250
And then there's a path of the internet

4416
02:47:38,250 --> 02:47:41,550
and just training these
compression models effectively

4417
02:47:41,550 --> 02:47:44,783
on trying to compress all the internet.

4418
02:47:44,783 --> 02:47:48,180
And that might also give
these agents as well.

4419
02:47:48,180 --> 02:47:51,600
- Compress the internet but
also interact with the internet.

4420
02:47:51,600 --> 02:47:52,590
- [Andrej] Yeah.

4421
02:47:52,590 --> 02:47:54,180
- So it's not obvious to me.

4422
02:47:54,180 --> 02:47:56,790
In fact, I suspect you can reach AGI

4423
02:47:56,790 --> 02:47:59,493
without ever entering the physical world,

4424
02:48:00,780 --> 02:48:03,870
which is a little bit more concerning

4425
02:48:03,870 --> 02:48:08,870
because that results
in it happening faster.

4426
02:48:08,940 --> 02:48:11,910
So it just feels like
we're in boiling water.

4427
02:48:11,910 --> 02:48:14,250
We won't know as it's happening.

4428
02:48:14,250 --> 02:48:17,880
I would like to, I'm not afraid of AGI,

4429
02:48:17,880 --> 02:48:19,170
I'm excited about it.

4430
02:48:19,170 --> 02:48:20,430
There's always concerns

4431
02:48:20,430 --> 02:48:22,803
but I would like to know when it happens.

4432
02:48:23,970 --> 02:48:24,803
- [Andrej] Yeah.

4433
02:48:24,803 --> 02:48:26,850
- And have like hints
about when it happens,

4434
02:48:26,850 --> 02:48:29,910
like a year from now it will
happen, that kind of thing.

4435
02:48:29,910 --> 02:48:30,914
- [Andrej] Yeah.

4436
02:48:30,914 --> 02:48:31,747
- I just feel like in the digital realm

4437
02:48:31,747 --> 02:48:32,700
it just might happen.

4438
02:48:32,700 --> 02:48:33,533
- Yeah.

4439
02:48:33,533 --> 02:48:34,680
I think all we have available to us

4440
02:48:34,680 --> 02:48:36,870
because no one has built AGI again,

4441
02:48:36,870 --> 02:48:38,943
so all we have available to us is,

4442
02:48:39,810 --> 02:48:42,510
is there enough fertile
ground on the periphery?

4443
02:48:42,510 --> 02:48:43,343
I would say, yes.

4444
02:48:43,343 --> 02:48:44,640
And we have the progress so far,

4445
02:48:44,640 --> 02:48:47,850
which has been very rapid
and there are next steps

4446
02:48:47,850 --> 02:48:48,683
that are available.

4447
02:48:48,683 --> 02:48:51,720
And so I would say,
yeah, it's quite likely

4448
02:48:51,720 --> 02:48:54,330
that we'll be interacting
with digital entities.

4449
02:48:54,330 --> 02:48:57,401
- How will you know that
somebody has built AGI?

4450
02:48:57,401 --> 02:48:59,733
- I think it's going to be a
slow incremental transition.

4451
02:48:59,733 --> 02:49:01,680
It's going to be
product-based and focused.

4452
02:49:01,680 --> 02:49:03,780
It's going to be GitHub
Copilot going better.

4453
02:49:03,780 --> 02:49:07,740
And then GPTs helping you
write and then these oracles

4454
02:49:07,740 --> 02:49:09,720
that you can go to with
mathematical problems.

4455
02:49:09,720 --> 02:49:12,000
I think we're on a verge of being able

4456
02:49:12,000 --> 02:49:15,750
to ask very complex questions
in chemistry, physics,

4457
02:49:15,750 --> 02:49:19,830
math of these oracles and
have them complete solutions.

4458
02:49:19,830 --> 02:49:22,680
- So AGI to use primarily
focused on intelligence

4459
02:49:22,680 --> 02:49:26,163
so consciousness doesn't enter into it.

4460
02:49:27,918 --> 02:49:30,540
- So in my mind, consciousness
is not a special thing

4461
02:49:30,540 --> 02:49:32,190
you will figure out and bolt on.

4462
02:49:32,190 --> 02:49:34,920
I think it's an emergent
phenomenon of a large enough

4463
02:49:34,920 --> 02:49:38,400
and complex enough
generative model sort of.

4464
02:49:38,400 --> 02:49:42,600
So if you have a complex
enough world model

4465
02:49:42,600 --> 02:49:45,270
that understands the world,
then it also understands

4466
02:49:45,270 --> 02:49:48,630
its predicament in the world
as being a language model,

4467
02:49:48,630 --> 02:49:52,020
which to me is a form of
consciousness or self-awareness.

4468
02:49:52,020 --> 02:49:53,940
- So in order to
understand the world deeply

4469
02:49:53,940 --> 02:49:56,220
you probably have to integrate
yourself into the world.

4470
02:49:56,220 --> 02:49:57,053
- [Andrej] Yeah.

4471
02:49:57,053 --> 02:49:58,560
- And in order to interact with humans

4472
02:49:58,560 --> 02:50:00,360
and other living beings,

4473
02:50:00,360 --> 02:50:02,414
consciousness is a very useful tool.

4474
02:50:02,414 --> 02:50:03,247
- Yeah.

4475
02:50:03,247 --> 02:50:05,850
I think consciousness is
like a modeling insight.

4476
02:50:05,850 --> 02:50:07,380
- Modeling insight.

4477
02:50:07,380 --> 02:50:08,213
- Yeah.

4478
02:50:08,213 --> 02:50:10,298
You have a powerful enough model

4479
02:50:10,298 --> 02:50:11,940
of understanding the world
that you actually understand

4480
02:50:11,940 --> 02:50:13,350
that you are an entity in it.

4481
02:50:13,350 --> 02:50:14,302
- Yeah.

4482
02:50:14,302 --> 02:50:16,620
But there's also this,
perhaps just a narrative

4483
02:50:16,620 --> 02:50:19,380
we tell ourselves, it feels like something

4484
02:50:19,380 --> 02:50:22,410
to experience the world, the
hard problem of consciousness.

4485
02:50:22,410 --> 02:50:23,243
- [Andrej] Yeah.

4486
02:50:23,243 --> 02:50:24,900
- But that could be just a
narrative that we tell ourselves.

4487
02:50:24,900 --> 02:50:25,864
- Yeah.

4488
02:50:25,864 --> 02:50:27,210
I don't think we'll, yeah,
I think it will emerge.

4489
02:50:27,210 --> 02:50:29,558
I think it's going to be
something very boring.

4490
02:50:29,558 --> 02:50:31,920
We'll be talking to these digital AIs,

4491
02:50:31,920 --> 02:50:33,360
they will claim they're conscious,

4492
02:50:33,360 --> 02:50:35,910
they will appear conscious,
they will do all the things

4493
02:50:35,910 --> 02:50:37,500
that you would expect of other humans

4494
02:50:37,500 --> 02:50:40,522
and it's going to just be a stalemate.

4495
02:50:40,522 --> 02:50:41,400
- I think there will be a lot

4496
02:50:41,400 --> 02:50:44,700
of actual fascinating ethical questions,

4497
02:50:44,700 --> 02:50:47,640
like supreme court level questions

4498
02:50:47,640 --> 02:50:51,810
of whether you're allowed
to turn off a conscious AI,

4499
02:50:51,810 --> 02:50:54,573
if you're allowed to build a conscious AI,

4500
02:50:55,680 --> 02:50:58,680
maybe there would have to
be the same kind of debates

4501
02:50:58,680 --> 02:51:03,120
that you have around, sorry
to bring up a political topic,

4502
02:51:03,120 --> 02:51:07,990
but abortion, which is the
deeper question with abortion

4503
02:51:09,210 --> 02:51:11,379
is what is life?

4504
02:51:11,379 --> 02:51:14,280
And the deep question with AI is also,

4505
02:51:14,280 --> 02:51:16,470
what is life and what is conscious?

4506
02:51:16,470 --> 02:51:17,402
- [Andrej] Right.

4507
02:51:17,402 --> 02:51:19,920
- And I think that'll be very fascinating

4508
02:51:19,920 --> 02:51:23,610
to bring up, it might become
illegal to build systems

4509
02:51:23,610 --> 02:51:28,610
that are capable of such
level of intelligence

4510
02:51:28,620 --> 02:51:29,910
that consciousness would emerge

4511
02:51:29,910 --> 02:51:32,100
and therefore the capacity
to suffer would emerge.

4512
02:51:32,100 --> 02:51:36,210
And a system that says,
no, please don't kill me.

4513
02:51:36,210 --> 02:51:40,290
- Well, that's what the
LaMDA chatbot already told

4514
02:51:40,290 --> 02:51:41,398
this Google engineer, right?

4515
02:51:41,398 --> 02:51:44,910
It was talking about not
wanting to die or so on.

4516
02:51:44,910 --> 02:51:47,280
- So that might become illegal to do that.

4517
02:51:47,280 --> 02:51:48,130
- [Andrej] Right.

4518
02:51:49,680 --> 02:51:52,590
- 'Cause otherwise, you
might have a lot of creatures

4519
02:51:52,590 --> 02:51:54,753
that don't want to die and they will-

4520
02:51:55,668 --> 02:51:57,983
- [Andrej] You can just spawn
infinity of them on a cluster.

4521
02:51:59,280 --> 02:52:01,740
- And then that might lead
to horrible consequences.

4522
02:52:01,740 --> 02:52:03,630
'Cause then there might be a lot of people

4523
02:52:03,630 --> 02:52:05,100
that secretly love murder

4524
02:52:05,100 --> 02:52:07,650
and they'll start practicing
murder on those systems.

4525
02:52:07,650 --> 02:52:11,280
I mean there's just, to me
all of this stuff just brings

4526
02:52:11,280 --> 02:52:14,100
a beautiful mirror to the human condition

4527
02:52:14,100 --> 02:52:15,810
and human nature and we get to explore it.

4528
02:52:15,810 --> 02:52:16,710
- [Andrej] Yes.

4529
02:52:16,710 --> 02:52:19,650
- And that's what like the
best of the supreme court

4530
02:52:19,650 --> 02:52:22,290
of all the different
debates we have about ideas

4531
02:52:22,290 --> 02:52:23,460
of what it means to be human,

4532
02:52:23,460 --> 02:52:25,350
we get to those deep questions

4533
02:52:25,350 --> 02:52:27,420
that we've been asking
throughout human history.

4534
02:52:27,420 --> 02:52:30,123
There's always been the
other in human history.

4535
02:52:31,050 --> 02:52:33,240
We're the good guys
and that's the bad guys

4536
02:52:33,240 --> 02:52:36,060
and we're going to
throughout human history,

4537
02:52:36,060 --> 02:52:37,890
let's murder the bad guys.

4538
02:52:37,890 --> 02:52:40,140
And the same will probably
happen with robots.

4539
02:52:40,140 --> 02:52:41,760
It'll be the other at first.

4540
02:52:41,760 --> 02:52:42,593
And then we'll get to ask questions,

4541
02:52:42,593 --> 02:52:44,580
that what does it mean to be alive?

4542
02:52:44,580 --> 02:52:45,930
What does it mean to be conscious?

4543
02:52:45,930 --> 02:52:46,763
- Yep.

4544
02:52:46,763 --> 02:52:48,330
And I think there's some
canary in the coal mines

4545
02:52:48,330 --> 02:52:50,100
even with what we have today.

4546
02:52:50,100 --> 02:52:52,920
And for example, there's these waifus

4547
02:52:52,920 --> 02:52:53,757
that you can work with

4548
02:52:53,757 --> 02:52:55,560
and some people are trying to,

4549
02:52:55,560 --> 02:52:56,700
this company's going to shut down,

4550
02:52:56,700 --> 02:52:59,560
but this person really loved their waifu

4551
02:52:59,560 --> 02:53:01,620
and is trying to like
port it somewhere else.

4552
02:53:01,620 --> 02:53:03,180
And it's not possible.

4553
02:53:03,180 --> 02:53:06,900
And I think definitely
people will have feelings

4554
02:53:06,900 --> 02:53:11,460
towards these systems
because in some sense

4555
02:53:11,460 --> 02:53:13,470
they are like a mirror of humanity

4556
02:53:13,470 --> 02:53:16,980
because they are like a
big average of humanity.

4557
02:53:16,980 --> 02:53:17,836
- [Lex] Yeah.

4558
02:53:17,836 --> 02:53:18,669
- In a way that it's trained.

4559
02:53:18,669 --> 02:53:22,350
- But that average, we can actually watch.

4560
02:53:22,350 --> 02:53:23,670
It's nice to be able to interact

4561
02:53:23,670 --> 02:53:25,268
with the big average of humanity.

4562
02:53:25,268 --> 02:53:26,101
- [Andrej] Yeah.

4563
02:53:26,101 --> 02:53:27,330
- And do a search query on it.

4564
02:53:27,330 --> 02:53:28,306
- Yeah.

4565
02:53:28,306 --> 02:53:29,139
Yeah.

4566
02:53:29,139 --> 02:53:29,972
It's very fascinating.

4567
02:53:29,972 --> 02:53:31,950
And we can also of course, also shape it.

4568
02:53:31,950 --> 02:53:33,000
It's not just a pure average.

4569
02:53:33,000 --> 02:53:34,650
We can mess with the training data,

4570
02:53:34,650 --> 02:53:35,670
we can mess with the objective,

4571
02:53:35,670 --> 02:53:37,680
we can fine-tune them in various ways.

4572
02:53:37,680 --> 02:53:42,570
So we have some impact on
what those systems look like.

4573
02:53:42,570 --> 02:53:44,650
- If you want to achieve AGI

4574
02:53:45,780 --> 02:53:48,090
and you could have a conversation with her

4575
02:53:48,090 --> 02:53:51,840
and ask her, talk about anything,
maybe ask her a question.

4576
02:53:51,840 --> 02:53:54,240
What kind of stuff would you ask?

4577
02:53:54,240 --> 02:53:55,890
- I would've some practical
questions in my mind

4578
02:53:55,890 --> 02:54:00,090
like do I or my loved
ones really have to die?

4579
02:54:00,090 --> 02:54:01,390
What can we do about that?

4580
02:54:02,940 --> 02:54:04,590
- Do you think it will answer clearly

4581
02:54:04,590 --> 02:54:06,303
or would it answer poetically?

4582
02:54:07,410 --> 02:54:09,090
- I would expect it to give solutions.

4583
02:54:09,090 --> 02:54:10,770
I would expect it to be
like, well, I've read

4584
02:54:10,770 --> 02:54:12,840
all of these textbooks and
I know all these things

4585
02:54:12,840 --> 02:54:14,070
that you've produced and it seems to me

4586
02:54:14,070 --> 02:54:15,000
like here are the experiments

4587
02:54:15,000 --> 02:54:17,580
that I think it would
be useful to run next.

4588
02:54:17,580 --> 02:54:18,630
And here are some gene therapies

4589
02:54:18,630 --> 02:54:19,890
that I think would be helpful,

4590
02:54:19,890 --> 02:54:22,350
and here are the kinds of
experiments that you should run.

4591
02:54:22,350 --> 02:54:24,300
- Okay, let's go with
this thought experiment.

4592
02:54:24,300 --> 02:54:25,320
Okay.

4593
02:54:25,320 --> 02:54:29,430
Imagine that mortality is actually

4594
02:54:29,430 --> 02:54:33,030
a prerequisite for happiness.

4595
02:54:33,030 --> 02:54:34,770
So if we become immortal,

4596
02:54:34,770 --> 02:54:36,810
we'll actually become deeply unhappy

4597
02:54:36,810 --> 02:54:39,660
and the model is able to know that.

4598
02:54:39,660 --> 02:54:41,160
So what is it supposed to tell you?

4599
02:54:41,160 --> 02:54:42,540
A stupid human about it?

4600
02:54:42,540 --> 02:54:43,860
Yes, you can become a mortal

4601
02:54:43,860 --> 02:54:46,158
but you'll become deeply unhappy.

4602
02:54:46,158 --> 02:54:51,158
If the AGI system is trying
to empathize with you human,

4603
02:54:52,050 --> 02:54:53,550
what is it supposed to tell you.

4604
02:54:53,550 --> 02:54:55,860
That yes, you don't have to die

4605
02:54:55,860 --> 02:54:58,020
but you're really not gonna like it?

4606
02:54:58,020 --> 02:54:59,730
Is it gonna be deeply honest?

4607
02:54:59,730 --> 02:55:01,650
There's an "Interstellar",

4608
02:55:01,650 --> 02:55:05,703
what is it the AI says like
humans want 90% honesty.

4609
02:55:08,040 --> 02:55:09,307
So you have to pick how honest

4610
02:55:09,307 --> 02:55:11,790
do I want to answer these
practical questions?

4611
02:55:11,790 --> 02:55:12,623
- Yeah.

4612
02:55:12,623 --> 02:55:14,190
I love AI "Interstellar" by the way.

4613
02:55:14,190 --> 02:55:16,740
I think it's like such a
sidekick to the entire story

4614
02:55:16,740 --> 02:55:19,740
but at the same time,
it's really interesting.

4615
02:55:19,740 --> 02:55:22,350
- It's kind of limited
in certain ways, right?

4616
02:55:22,350 --> 02:55:24,240
- Yeah, it's limited and I
think that's totally fine

4617
02:55:24,240 --> 02:55:25,073
by the way.

4618
02:55:25,073 --> 02:55:29,040
I think it's fine and
plausible to have a limited

4619
02:55:29,040 --> 02:55:30,033
and imperfect AGIs.

4620
02:55:32,340 --> 02:55:34,020
- Is that a feature almost?

4621
02:55:34,020 --> 02:55:36,930
- As an example, it has
a fixed amount of compute

4622
02:55:36,930 --> 02:55:38,250
on its physical body.

4623
02:55:38,250 --> 02:55:40,680
And it might just be that
even though you can have

4624
02:55:40,680 --> 02:55:43,980
a super amazing mega brain,
super-intelligent AI,

4625
02:55:43,980 --> 02:55:46,203
you also can have less intelligent AI

4626
02:55:46,203 --> 02:55:49,530
that you can deploy in
a power-efficient way.

4627
02:55:49,530 --> 02:55:51,450
And then they're not perfect,
they might make mistakes.

4628
02:55:51,450 --> 02:55:55,320
- No, I meant more like say
you had infinite compute

4629
02:55:55,320 --> 02:55:58,140
and it's still good to
make mistakes sometimes.

4630
02:55:58,140 --> 02:55:59,610
In order to integrate yourself.

4631
02:55:59,610 --> 02:56:01,710
Like, what is it?

4632
02:56:01,710 --> 02:56:05,130
Going back to "Goodwill Hunting",
Robin Williams character

4633
02:56:05,130 --> 02:56:08,883
says the human imperfections,
that's good stuff, right?

4634
02:56:11,220 --> 02:56:12,480
We don't want perfect,

4635
02:56:12,480 --> 02:56:17,480
we want flaws in part to form
connections with each other.

4636
02:56:17,610 --> 02:56:19,230
'Cause it feels like
something you can attach

4637
02:56:19,230 --> 02:56:22,770
your feelings to, the flaws.

4638
02:56:22,770 --> 02:56:26,209
In that same way you
want an AI that's flawed.

4639
02:56:26,209 --> 02:56:27,042
I don't know.

4640
02:56:27,042 --> 02:56:28,744
I feel like perfection is cold.

4641
02:56:28,744 --> 02:56:29,913
- [Andrej] Okay, yeah.

4642
02:56:29,913 --> 02:56:30,960
- But that's not AGI.

4643
02:56:30,960 --> 02:56:33,960
But see AGI would need
to be intelligent enough

4644
02:56:33,960 --> 02:56:36,700
to give answers to humans
that humans don't understand.

4645
02:56:36,700 --> 02:56:40,170
And I think perfect is something
humans can't understand

4646
02:56:40,170 --> 02:56:42,600
because even science doesn't
give perfect answers.

4647
02:56:42,600 --> 02:56:46,023
There's always gaps and
mysteries and I don't know,

4648
02:56:47,040 --> 02:56:50,100
I don't know if humans want perfect.

4649
02:56:50,100 --> 02:56:52,770
- Yeah, I could imagine
just having a conversation

4650
02:56:52,770 --> 02:56:55,800
with this oracle entity
as you'd imagine them

4651
02:56:55,800 --> 02:56:58,233
and yeah, maybe it can tell you about,

4652
02:56:59,160 --> 02:57:01,160
based on my analysis of human condition,

4653
02:57:02,100 --> 02:57:03,030
you might not want this

4654
02:57:03,030 --> 02:57:05,190
and here are some of
the things that might-

4655
02:57:05,190 --> 02:57:08,790
- But every dumb human will
say, yeah, yeah, yeah, yeah,

4656
02:57:08,790 --> 02:57:12,360
trust me, give me the
truth, I can handle it.

4657
02:57:12,360 --> 02:57:15,000
- But that's the beauty,
like people can choose.

4658
02:57:15,000 --> 02:57:19,410
- But then, it's the old marshmallow test

4659
02:57:19,410 --> 02:57:22,010
with the kids and so on, I
feel like too many people

4660
02:57:24,090 --> 02:57:27,240
can't handle the truth,
probably including myself.

4661
02:57:27,240 --> 02:57:29,010
Deep truth to the human condition.

4662
02:57:29,010 --> 02:57:31,410
I don't know if I can handle it.

4663
02:57:31,410 --> 02:57:32,883
What if there's some dark stuff?

4664
02:57:32,883 --> 02:57:35,760
What if we are an alien science experiment

4665
02:57:35,760 --> 02:57:36,990
and it realizes that.

4666
02:57:36,990 --> 02:57:38,910
What if it hacked, I mean?

4667
02:57:38,910 --> 02:57:41,206
- I mean, this is "The
Matrix" all over again.

4668
02:57:41,206 --> 02:57:46,020
- "The Matrix", I don't know,
what would I talk about?

4669
02:57:46,020 --> 02:57:50,310
I don't even, yeah, probably I will go

4670
02:57:50,310 --> 02:57:52,620
with the safer scientific
questions at first

4671
02:57:52,620 --> 02:57:55,650
that have nothing to do
with my own personal life.

4672
02:57:55,650 --> 02:57:56,609
- [Andrej] Yeah.

4673
02:57:56,609 --> 02:57:59,460
- Immortality just like
about physics and so on.

4674
02:57:59,460 --> 02:58:00,630
- [Andrej] Yeah.

4675
02:58:00,630 --> 02:58:02,670
- To build up see where it's at

4676
02:58:02,670 --> 02:58:04,560
or maybe see if it has a sense of humor.

4677
02:58:04,560 --> 02:58:05,793
That's another question.

4678
02:58:07,110 --> 02:58:10,140
Presumably in order to, if
it understands humans deeply,

4679
02:58:10,140 --> 02:58:15,140
would it able to generate humor.

4680
02:58:15,420 --> 02:58:16,253
- Yeah.

4681
02:58:16,253 --> 02:58:18,210
I think that's actually a
wonderful benchmark almost,

4682
02:58:18,210 --> 02:58:19,740
like is it able, I think that's

4683
02:58:19,740 --> 02:58:21,330
a really good point, basically.

4684
02:58:21,330 --> 02:58:22,320
- [Lex] To make you laugh.

4685
02:58:22,320 --> 02:58:23,153
- Yeah.

4686
02:58:23,153 --> 02:58:24,930
If it's able to be a very
effective standup comedian

4687
02:58:24,930 --> 02:58:26,970
that is doing something very
interesting computationally.

4688
02:58:26,970 --> 02:58:28,950
I think being funny is extremely hard.

4689
02:58:28,950 --> 02:58:33,950
- Yeah, because it's hard in
a way like a touring test,

4690
02:58:35,580 --> 02:58:38,580
the original intent of
the touring test is hard

4691
02:58:38,580 --> 02:58:40,230
because you have to convince humans

4692
02:58:40,230 --> 02:58:45,230
and that's why comedians talk about this,

4693
02:58:45,360 --> 02:58:48,000
like this is deeply honest.

4694
02:58:48,000 --> 02:58:49,920
'Cause if people can't help but laugh

4695
02:58:49,920 --> 02:58:51,840
and if they don't laugh
that means you're not funny,

4696
02:58:51,840 --> 02:58:52,920
if they laugh, it's funny.

4697
02:58:52,920 --> 02:58:53,773
- Yeah.

4698
02:58:53,773 --> 02:58:54,921
And you're showing, you
need a lot of knowledge

4699
02:58:54,921 --> 02:58:57,690
to create humor about like you mentioned

4700
02:58:57,690 --> 02:58:58,650
human condition and so on.

4701
02:58:58,650 --> 02:59:01,230
And then you need to be clever with it.

4702
02:59:01,230 --> 02:59:02,400
- You mentioned a few movies,

4703
02:59:02,400 --> 02:59:05,250
you Tweeted, "Movies that
I've seen five-plus times

4704
02:59:05,250 --> 02:59:08,520
but am ready and willing to keep watching:

4705
02:59:08,520 --> 02:59:11,130
'Interstellar', 'Gladiator',
'Contact', 'Goodwill Hunting',

4706
02:59:11,130 --> 02:59:14,040
'The Matrix', 'Lord of
the Rings', all three,

4707
02:59:14,040 --> 02:59:15,720
'Avatar', 'Fifth Element'," and so on,

4708
02:59:15,720 --> 02:59:17,377
it goes on, "'Terminator 2'."

4709
02:59:17,377 --> 02:59:19,677
"Mean Girls" I'm not
gonna ask about that one.

4710
02:59:20,584 --> 02:59:22,584
- "Mean Girls" is great.

4711
02:59:23,610 --> 02:59:25,070
- What are some that jump out to you

4712
02:59:25,070 --> 02:59:28,920
in your memory that you love and why?

4713
02:59:28,920 --> 02:59:32,070
You mentioned "The Matrix"
as a computer person,

4714
02:59:32,070 --> 02:59:33,520
why do you love "The Matrix"?

4715
02:59:34,440 --> 02:59:35,400
- There's so many properties

4716
02:59:35,400 --> 02:59:36,660
that make it beautiful and interesting.

4717
02:59:36,660 --> 02:59:39,120
So there's all these
philosophical questions

4718
02:59:39,120 --> 02:59:42,150
but then there's also AGIs,
and there's simulation,

4719
02:59:42,150 --> 02:59:45,033
and it's cool, and there's the black.

4720
02:59:46,320 --> 02:59:47,613
- [Lex] The look of it, the feel of it.

4721
02:59:47,613 --> 02:59:48,446
- Yeah.

4722
02:59:48,446 --> 02:59:50,010
The look of it, the feel of it,
the action, the bullet time.

4723
02:59:50,010 --> 02:59:52,353
It was just like
innovating in so many ways.

4724
02:59:53,460 --> 02:59:56,040
- And then "Goodwill Hunting".

4725
02:59:56,040 --> 02:59:57,510
Why do you like that one?

4726
02:59:57,510 --> 03:00:02,160
- Yeah, I really like this
tortured genius character

4727
03:00:02,160 --> 03:00:04,500
who's grappling with whether or not

4728
03:00:04,500 --> 03:00:07,230
he has any responsibility or what to do

4729
03:00:07,230 --> 03:00:08,760
with this gift that he was given

4730
03:00:08,760 --> 03:00:11,912
or how to think about the whole thing and-

4731
03:00:11,912 --> 03:00:13,500
- But there's also a
dance between the genius

4732
03:00:13,500 --> 03:00:16,620
and the personal, like what it means

4733
03:00:16,620 --> 03:00:18,013
to love another human being.

4734
03:00:18,013 --> 03:00:18,846
- Yeah.

4735
03:00:18,846 --> 03:00:19,679
There's a lot of themes there.

4736
03:00:19,679 --> 03:00:20,512
It's just a beautiful movie.

4737
03:00:20,512 --> 03:00:21,540
- And then the fatherly figure,

4738
03:00:21,540 --> 03:00:24,300
the mentor and the psychiatrist.

4739
03:00:24,300 --> 03:00:27,270
- It really messes with you.

4740
03:00:27,270 --> 03:00:30,210
There's some movies that just
like really mess with you

4741
03:00:30,210 --> 03:00:31,050
on a deep level.

4742
03:00:31,050 --> 03:00:33,240
- Do you relate to that movie at all?

4743
03:00:33,240 --> 03:00:34,590
- No.

4744
03:00:34,590 --> 03:00:36,937
- It's not your fault Andrej, as I said.

4745
03:00:36,937 --> 03:00:40,087
"Lord of the Rings",
that's self-explanatory.

4746
03:00:40,087 --> 03:00:42,810
"Terminator 2", which is interesting,

4747
03:00:42,810 --> 03:00:44,220
you rewatch that a lot.

4748
03:00:44,220 --> 03:00:46,140
Is that better than Terminator one?

4749
03:00:46,140 --> 03:00:48,029
You don't like Arnold as he comes back?

4750
03:00:48,029 --> 03:00:50,100
- I do like Terminator one as well.

4751
03:00:50,100 --> 03:00:51,720
I like "Terminator 2" a little bit more.

4752
03:00:51,720 --> 03:00:53,673
But in terms of its surface properties.

4753
03:00:55,920 --> 03:00:58,590
- Do you think Skynet
is at all a possibility?

4754
03:00:58,590 --> 03:01:00,120
- Yes.

4755
03:01:00,120 --> 03:01:04,890
- Like the actual autonomous
weapon system kind of thing?

4756
03:01:04,890 --> 03:01:06,870
Do you worry about that stuff?

4757
03:01:06,870 --> 03:01:09,480
So AI being used for war?

4758
03:01:09,480 --> 03:01:10,560
- I a hundred percent worry about it.

4759
03:01:10,560 --> 03:01:14,277
And so the, I mean, some
of these fears of AGIs

4760
03:01:14,277 --> 03:01:16,230
and how this will plan
out, I mean these will be

4761
03:01:16,230 --> 03:01:17,757
very powerful entities
probably at some point.

4762
03:01:17,757 --> 03:01:20,790
And so for a long time,
there are going to be tools

4763
03:01:20,790 --> 03:01:22,320
in the hands of humans.

4764
03:01:22,320 --> 03:01:24,057
People talk about alignment of AGIs

4765
03:01:24,057 --> 03:01:27,750
and how to make, the problem
is even humans are not aligned.

4766
03:01:27,750 --> 03:01:31,500
So how this will be used and
what this is gonna look like

4767
03:01:31,500 --> 03:01:33,543
is, yeah, it's troubling.

4768
03:01:34,530 --> 03:01:36,630
- Do you think it'll happen slowly enough

4769
03:01:36,630 --> 03:01:40,500
that we'll be able to
as a human civilization

4770
03:01:40,500 --> 03:01:41,820
think through the problems?

4771
03:01:41,820 --> 03:01:44,070
- Yes, that's my hope, is
that it happens slowly enough

4772
03:01:44,070 --> 03:01:46,920
and in an open enough way
where a lot of people can see

4773
03:01:46,920 --> 03:01:48,150
and participate in it.

4774
03:01:48,150 --> 03:01:50,790
Just to figure out how to
deal with this transition,

4775
03:01:50,790 --> 03:01:52,320
I think, which is gonna be interesting.

4776
03:01:52,320 --> 03:01:54,810
- I draw a lot of inspiration
from nuclear weapons

4777
03:01:54,810 --> 03:01:57,990
'cause I sure thought it would be fucked

4778
03:01:57,990 --> 03:02:00,300
once they develop nuclear weapons.

4779
03:02:00,300 --> 03:02:05,300
But it's almost like when the
systems are not so dangerous

4780
03:02:06,480 --> 03:02:07,860
they destroy human civilization.

4781
03:02:07,860 --> 03:02:11,790
We deploy them and learn the
lessons and then we quickly,

4782
03:02:11,790 --> 03:02:13,740
if it's too dangerous we quickly, quickly,

4783
03:02:13,740 --> 03:02:17,070
we might still deploy it
but you very quickly learn

4784
03:02:17,070 --> 03:02:17,903
not to use them.

4785
03:02:17,903 --> 03:02:19,650
And so there'll be like
this balance achieved,

4786
03:02:19,650 --> 03:02:21,960
humans are very clever as a species.

4787
03:02:21,960 --> 03:02:25,590
It's interesting, we exploit
the resources as much as we can

4788
03:02:25,590 --> 03:02:28,530
but we avoid destroying
ourselves it seems like.

4789
03:02:28,530 --> 03:02:29,363
- Yeah.

4790
03:02:29,363 --> 03:02:30,840
Well, I dunno about that actually.

4791
03:02:30,840 --> 03:02:32,043
- I hope it continues.

4792
03:02:33,690 --> 03:02:35,460
- I mean I'm definitely like concerned

4793
03:02:35,460 --> 03:02:36,810
about nuclear weapons and so on,

4794
03:02:36,810 --> 03:02:38,880
not just as a result
of the recent conflict,

4795
03:02:38,880 --> 03:02:40,440
even before that.

4796
03:02:40,440 --> 03:02:43,470
That's probably like my number
one concern for humanity.

4797
03:02:43,470 --> 03:02:48,470
- So if humanity destroys
itself or destroys 90% of people

4798
03:02:50,520 --> 03:02:52,530
that would be because of nukes?

4799
03:02:52,530 --> 03:02:53,877
- Yeah, I think so.

4800
03:02:53,877 --> 03:02:55,830
And it's not even about full destruction,

4801
03:02:55,830 --> 03:02:58,128
to me, it's bad enough
if we reset society,

4802
03:02:58,128 --> 03:02:59,580
that would be terrible.

4803
03:02:59,580 --> 03:03:00,413
That would be really bad.

4804
03:03:00,413 --> 03:03:03,630
And I can't believe we're so close to it.

4805
03:03:03,630 --> 03:03:04,463
- [Lex] Yeah.

4806
03:03:04,463 --> 03:03:05,296
- It's like so crazy to me.

4807
03:03:05,296 --> 03:03:07,140
- It feels like we might
be a few Tweets away

4808
03:03:07,140 --> 03:03:08,460
from something like that.

4809
03:03:08,460 --> 03:03:09,293
- Yep.

4810
03:03:09,293 --> 03:03:11,940
Basically, it's extremely unnerving

4811
03:03:11,940 --> 03:03:14,250
and has been for me for a long time.

4812
03:03:14,250 --> 03:03:18,570
- It seems unstable that world leaders

4813
03:03:18,570 --> 03:03:23,570
just having a bad mood can take one step

4814
03:03:23,910 --> 03:03:26,640
towards a bad direction
and then it escalates.

4815
03:03:26,640 --> 03:03:27,473
- Yeah.

4816
03:03:27,473 --> 03:03:30,420
- And because of a
collection of bad moods,

4817
03:03:30,420 --> 03:03:33,783
it can escalate without
being able to stop.

4818
03:03:34,650 --> 03:03:35,483
- Yeah.

4819
03:03:35,483 --> 03:03:37,230
It's a huge amount of power.

4820
03:03:37,230 --> 03:03:39,600
And then also with the proliferation

4821
03:03:39,600 --> 03:03:42,260
and basically, I don't
actually really see,

4822
03:03:42,260 --> 03:03:44,123
I don't actually know what
the good outcomes are here,

4823
03:03:45,000 --> 03:03:46,680
so I'm definitely
worried about that a lot.

4824
03:03:46,680 --> 03:03:48,450
And then AGI is not currently there

4825
03:03:48,450 --> 03:03:52,290
but I think at some point
it will more and more become

4826
03:03:52,290 --> 03:03:53,340
something like it.

4827
03:03:53,340 --> 03:03:55,530
The danger with AGI even is that

4828
03:03:55,530 --> 03:03:56,910
I think it's even slightly worse

4829
03:03:56,910 --> 03:04:00,810
in the sense that there
are good outcomes of AGI

4830
03:04:00,810 --> 03:04:03,990
and then the bad outcomes
are an epsilon way,

4831
03:04:03,990 --> 03:04:05,310
like a tiny run away.

4832
03:04:05,310 --> 03:04:08,280
And so I think capitalism
and humanity, and so on

4833
03:04:08,280 --> 03:04:11,970
will drive for the positive
ways of using that technology.

4834
03:04:11,970 --> 03:04:13,950
But then if bad outcomes
are just like a tiny,

4835
03:04:13,950 --> 03:04:16,590
like flip a minus sign away,

4836
03:04:16,590 --> 03:04:18,493
that's a really bad position to be in.

4837
03:04:18,493 --> 03:04:20,370
- A tiny perturbation of the system

4838
03:04:20,370 --> 03:04:23,318
results in the destruction
of the human species.

4839
03:04:23,318 --> 03:04:24,151
- [Andrej] Yeah.

4840
03:04:24,151 --> 03:04:25,230
- It's a weird line to walk.

4841
03:04:25,230 --> 03:04:26,640
- Yeah, I think in general
what's really weird

4842
03:04:26,640 --> 03:04:27,990
about the dynamics of humanity

4843
03:04:27,990 --> 03:04:29,190
and this explosion we talked about

4844
03:04:29,190 --> 03:04:32,520
is just the insane coupling
afforded by technology.

4845
03:04:32,520 --> 03:04:33,353
- [Lex] Yeah.

4846
03:04:33,353 --> 03:04:36,390
- And just the instability of
the whole dynamical system.

4847
03:04:36,390 --> 03:04:39,210
I think it doesn't look good, honestly.

4848
03:04:39,210 --> 03:04:40,257
- Yes.

4849
03:04:40,257 --> 03:04:41,580
That explosion could be
destructive or constructive

4850
03:04:41,580 --> 03:04:45,120
and the probabilities are
non-zero in both ends of it.

4851
03:04:45,120 --> 03:04:47,160
- I'm gonna have to, I
do feel like I have to

4852
03:04:47,160 --> 03:04:50,010
try to be optimistic and so on
and I think even in this case

4853
03:04:50,010 --> 03:04:53,730
I still am predominantly
optimistic but there's definitely-

4854
03:04:53,730 --> 03:04:54,780
- Me too.

4855
03:04:54,780 --> 03:04:57,380
Do you think we'll become
a multi-planetary species?

4856
03:04:58,680 --> 03:04:59,513
- Probably, yes.

4857
03:04:59,513 --> 03:05:01,200
But I don't know if it's dominant feature

4858
03:05:01,200 --> 03:05:04,140
of future humanity.

4859
03:05:04,140 --> 03:05:06,900
There might be some people
on some planets and so on

4860
03:05:06,900 --> 03:05:08,880
but I'm not sure if it's like, yeah,

4861
03:05:08,880 --> 03:05:12,120
if it's like a major player
in our culture and so on.

4862
03:05:12,120 --> 03:05:14,430
- We still have to solve the drivers

4863
03:05:14,430 --> 03:05:16,830
of self-destruction here on earth.

4864
03:05:16,830 --> 03:05:18,420
So just having a backup on Mars

4865
03:05:18,420 --> 03:05:19,920
is not gonna solve the problem.

4866
03:05:19,920 --> 03:05:21,870
- So by the way, I love
the backup on Mars.

4867
03:05:21,870 --> 03:05:22,800
I think that's amazing.

4868
03:05:22,800 --> 03:05:23,760
We should absolutely do that.

4869
03:05:23,760 --> 03:05:24,840
- [Lex] Yes.

4870
03:05:24,840 --> 03:05:25,940
- And I'm so thankful.

4871
03:05:27,048 --> 03:05:28,710
- Would you go to Mars?

4872
03:05:28,710 --> 03:05:31,380
- Personally, no, I do
like earth quite a lot.

4873
03:05:31,380 --> 03:05:32,213
- Okay.

4874
03:05:32,213 --> 03:05:33,046
I'll go to Mars.

4875
03:05:33,046 --> 03:05:33,990
I'll go for you.

4876
03:05:33,990 --> 03:05:35,370
I'll Tweet at you from there.

4877
03:05:35,370 --> 03:05:37,590
- Maybe eventually I would,
once it's safe enough.

4878
03:05:37,590 --> 03:05:40,350
But I don't actually know
if it's on my lifetime scale

4879
03:05:40,350 --> 03:05:41,950
unless I can extend it by a lot.

4880
03:05:43,050 --> 03:05:45,030
I do think that for example, a
lot of people might disappear

4881
03:05:45,030 --> 03:05:47,885
into virtual realities and stuff like that

4882
03:05:47,885 --> 03:05:49,426
and I think that could be the major thrust

4883
03:05:49,426 --> 03:05:53,880
of the cultural development
of humanity if it survives.

4884
03:05:53,880 --> 03:05:56,220
So it might not be, it's
just really hard to work

4885
03:05:56,220 --> 03:05:58,500
in physical realm and go out there

4886
03:05:58,500 --> 03:06:00,180
and I think ultimately
all your experiences

4887
03:06:00,180 --> 03:06:01,800
are in your brain.

4888
03:06:01,800 --> 03:06:02,633
- [Lex] Yeah.

4889
03:06:02,633 --> 03:06:05,730
- And so it's much easier to
disappear into digital realm

4890
03:06:05,730 --> 03:06:08,310
and I think people will find
them more compelling, easier,

4891
03:06:08,310 --> 03:06:10,650
safer, more interesting.

4892
03:06:10,650 --> 03:06:12,930
- So you're a little bit
captivated by virtual reality,

4893
03:06:12,930 --> 03:06:15,270
by the possible worlds,
whether it's the metaverse

4894
03:06:15,270 --> 03:06:16,860
or some other manifestation of that?

4895
03:06:16,860 --> 03:06:18,270
- [Andrej] Yeah.

4896
03:06:18,270 --> 03:06:19,325
- Yeah.

4897
03:06:19,325 --> 03:06:20,158
It's really interesting.

4898
03:06:21,720 --> 03:06:24,960
I'm interested, just
talking a lot to Carmack,

4899
03:06:24,960 --> 03:06:29,490
where's the thing that's
currently preventing that?

4900
03:06:29,490 --> 03:06:31,530
- Yeah, I mean to be clear,
I think what's interesting

4901
03:06:31,530 --> 03:06:36,530
about the future is it's
not that, I feel like

4902
03:06:36,990 --> 03:06:39,120
the variance in the human condition grows.

4903
03:06:39,120 --> 03:06:40,440
That's the primary thing that's changing.

4904
03:06:40,440 --> 03:06:42,573
It's not as much the
mean of the distribution,

4905
03:06:42,573 --> 03:06:43,980
it's like the variance of it.

4906
03:06:43,980 --> 03:06:45,390
So there will probably be people on Mars

4907
03:06:45,390 --> 03:06:46,710
and there will be people in VR,

4908
03:06:46,710 --> 03:06:47,853
and there will people here on earth.

4909
03:06:47,853 --> 03:06:51,000
It's just like there will be
so many more ways of being.

4910
03:06:51,000 --> 03:06:53,220
And so feel like, I see
it as like a spreading out

4911
03:06:53,220 --> 03:06:54,660
of a human experience.

4912
03:06:54,660 --> 03:06:56,010
- There's something about the internet

4913
03:06:56,010 --> 03:06:57,930
that allows you to discover
those little groups

4914
03:06:57,930 --> 03:07:01,080
and you gravitate to,
something about your biology

4915
03:07:01,080 --> 03:07:03,043
likes that kind of world
and you find each other.

4916
03:07:03,043 --> 03:07:03,876
- Yeah.

4917
03:07:03,876 --> 03:07:05,760
And we'll have trans-humanists
and then we'll have the Amish

4918
03:07:05,760 --> 03:07:07,710
and everything is just gonna coexist.

4919
03:07:07,710 --> 03:07:08,543
- Yeah.

4920
03:07:08,543 --> 03:07:09,660
The cool thing about it
'cause I've interacted

4921
03:07:09,660 --> 03:07:11,610
with a bunch of internet communities

4922
03:07:11,610 --> 03:07:15,510
is they don't know about each other.

4923
03:07:15,510 --> 03:07:17,850
Like you can have a very happy existence

4924
03:07:17,850 --> 03:07:19,860
just having a very close-knit community

4925
03:07:19,860 --> 03:07:21,270
and not knowing about each other.

4926
03:07:21,270 --> 03:07:23,130
I mean even you even sense this,

4927
03:07:23,130 --> 03:07:24,783
just having traveled to Ukraine,

4928
03:07:26,100 --> 03:07:29,100
they don't know so many
things about America.

4929
03:07:29,100 --> 03:07:30,330
- [Andrej] Yeah.

4930
03:07:30,330 --> 03:07:31,363
- When you travel across the world

4931
03:07:31,363 --> 03:07:33,060
I think you experience this too.

4932
03:07:33,060 --> 03:07:34,740
There are certain cultures that are like,

4933
03:07:34,740 --> 03:07:36,570
they have their own thing
going on, they don't.

4934
03:07:36,570 --> 03:07:39,810
And so you can see that
happening more and more and more

4935
03:07:39,810 --> 03:07:40,920
and more in the future.

4936
03:07:40,920 --> 03:07:42,120
We have little communities.

4937
03:07:42,120 --> 03:07:42,953
- Yeah.

4938
03:07:42,953 --> 03:07:43,902
Yeah.

4939
03:07:43,902 --> 03:07:44,735
I think so.

4940
03:07:44,735 --> 03:07:46,467
That seems to be how it's going right now.

4941
03:07:46,467 --> 03:07:48,870
And I don't see that
trend really reversing.

4942
03:07:48,870 --> 03:07:50,460
I think people are
diverse and they're able

4943
03:07:50,460 --> 03:07:54,363
to choose their own path in
existence and I celebrate that.

4944
03:07:55,440 --> 03:07:56,273
And so-

4945
03:07:56,273 --> 03:07:58,110
- Will you spend some,
much time in the metaverse,

4946
03:07:58,110 --> 03:07:59,880
in the virtual reality?

4947
03:07:59,880 --> 03:08:01,500
Or which community are you,

4948
03:08:01,500 --> 03:08:06,500
are you the physicalist,
the physical reality enjoyer

4949
03:08:07,170 --> 03:08:11,520
or do you see drawing a lot
of pleasure and fulfillment

4950
03:08:11,520 --> 03:08:12,663
in the digital world?

4951
03:08:13,500 --> 03:08:15,660
- Yeah, I think, well
currently, the virtual reality

4952
03:08:15,660 --> 03:08:17,158
is not that compelling.

4953
03:08:17,158 --> 03:08:17,991
- [Lex] Yes.

4954
03:08:17,991 --> 03:08:18,870
- I do think it can improve a lot

4955
03:08:18,870 --> 03:08:21,540
but I don't really know to what extent.

4956
03:08:21,540 --> 03:08:23,820
Maybe there's actually
even more exotic things

4957
03:08:23,820 --> 03:08:26,460
you can think about with neural
links or stuff like that.

4958
03:08:26,460 --> 03:08:31,050
So currently, I kind of see
myself as mostly a team,

4959
03:08:31,050 --> 03:08:32,940
human person, I love nature.

4960
03:08:32,940 --> 03:08:33,773
- [Lex] Yeah.

4961
03:08:33,773 --> 03:08:36,210
- I love harmony, I love
people, I love humanity.

4962
03:08:36,210 --> 03:08:41,210
I love emotions of humanity
and I just want to be

4963
03:08:41,280 --> 03:08:43,170
in this solar punk little utopia.

4964
03:08:43,170 --> 03:08:44,160
That's my happy place.

4965
03:08:44,160 --> 03:08:44,993
- [Lex] Yes.

4966
03:08:44,993 --> 03:08:47,130
- My happy place is people I love,

4967
03:08:47,130 --> 03:08:49,410
thinking about cool problems,
surrounded by a lush,

4968
03:08:49,410 --> 03:08:50,970
beautiful dynamic nature.

4969
03:08:50,970 --> 03:08:51,803
- [Lex] Yeah.

4970
03:08:51,803 --> 03:08:54,360
- And secretly high-tech
in places that count.

4971
03:08:54,360 --> 03:08:55,230
- Places that count.

4972
03:08:55,230 --> 03:08:58,110
So you use technology to empower that love

4973
03:08:58,110 --> 03:09:00,540
for other humans and nature.

4974
03:09:00,540 --> 03:09:03,330
- Yeah, I think a technology
used very sparingly.

4975
03:09:03,330 --> 03:09:05,730
I don't love when it gets
in the way of humanity

4976
03:09:05,730 --> 03:09:06,603
in many ways.

4977
03:09:07,440 --> 03:09:09,360
I like just people being humans

4978
03:09:09,360 --> 03:09:12,300
in a way, we slightly
evolved and prefer I think

4979
03:09:12,300 --> 03:09:13,290
just by default.

4980
03:09:13,290 --> 03:09:16,170
- People kept asking me 'cause
they know you love reading.

4981
03:09:16,170 --> 03:09:19,710
Are there particular
books that you enjoyed

4982
03:09:19,710 --> 03:09:22,710
that had an impact on you for silly

4983
03:09:22,710 --> 03:09:26,133
or for profound reasons
that you would recommend?

4984
03:09:27,090 --> 03:09:29,370
You mentioned "The Vital Question".

4985
03:09:29,370 --> 03:09:30,210
- Many, of course.

4986
03:09:30,210 --> 03:09:31,627
I think in biology as an example,

4987
03:09:31,627 --> 03:09:32,940
"The Vital Question" is a good one.

4988
03:09:32,940 --> 03:09:36,030
Anything by Nick Lane
really, "Life Ascending"

4989
03:09:36,030 --> 03:09:39,720
I would say is a bit more
potentially representative

4990
03:09:39,720 --> 03:09:42,690
as like a summary of a lot of the things

4991
03:09:42,690 --> 03:09:44,310
he's been talking about.

4992
03:09:44,310 --> 03:09:46,290
I was very impacted by "The Selfish Gene".

4993
03:09:46,290 --> 03:09:47,730
I thought that was a really good book,

4994
03:09:47,730 --> 03:09:50,010
it helped me understand
altruism as an example

4995
03:09:50,010 --> 03:09:50,843
and where it comes from.

4996
03:09:50,843 --> 03:09:52,500
And just realizing that the selection

4997
03:09:52,500 --> 03:09:53,460
and the levels of genes

4998
03:09:53,460 --> 03:09:55,140
was a huge insight for me at the time

4999
03:09:55,140 --> 03:09:57,210
and it cleared up a lot of things for me.

5000
03:09:57,210 --> 03:09:59,910
- What do you think about the idea

5001
03:09:59,910 --> 03:10:01,920
that ideas are the organisms, the memes?

5002
03:10:01,920 --> 03:10:02,895
- Yeah.

5003
03:10:02,895 --> 03:10:03,728
Love it.

5004
03:10:03,728 --> 03:10:05,940
A hundred percent.

5005
03:10:05,940 --> 03:10:08,940
- Are you able to walk around
with that notion for a while?

5006
03:10:08,940 --> 03:10:12,450
That there's an
evolutionary kind of process

5007
03:10:12,450 --> 03:10:13,380
with ideas as well?

5008
03:10:13,380 --> 03:10:14,213
- There absolutely is.

5009
03:10:14,213 --> 03:10:16,620
There's memes just like
genes and they compete

5010
03:10:16,620 --> 03:10:18,420
and they live in our brains.

5011
03:10:18,420 --> 03:10:19,410
It's beautiful.

5012
03:10:19,410 --> 03:10:22,110
- Are we silly humans thinking
that we are the organisms?

5013
03:10:22,110 --> 03:10:26,253
Is it possible that the primary
organisms are the ideas?

5014
03:10:27,240 --> 03:10:29,070
- Yeah, I would say like
the ideas kind of live

5015
03:10:29,070 --> 03:10:33,090
in the software of our
civilization in the minds

5016
03:10:33,090 --> 03:10:33,923
and so on.

5017
03:10:33,923 --> 03:10:36,120
We think as humans that the hardware

5018
03:10:36,120 --> 03:10:37,890
is the fundamental thing.

5019
03:10:37,890 --> 03:10:40,500
I human is a hardware entity.

5020
03:10:40,500 --> 03:10:41,333
- [Andrej] Yeah.

5021
03:10:41,333 --> 03:10:43,080
- But it could be the software, right?

5022
03:10:43,080 --> 03:10:43,913
- Yeah.

5023
03:10:44,760 --> 03:10:45,593
Yeah.

5024
03:10:45,593 --> 03:10:46,770
I would say there needs
to be some grounding

5025
03:10:46,770 --> 03:10:48,870
at some point to a physical reality.

5026
03:10:48,870 --> 03:10:50,570
- Yeah, but if we clone an Andrej,

5027
03:10:52,470 --> 03:10:57,330
the software is a thing that
makes that thing special.

5028
03:10:57,330 --> 03:10:58,163
Right?

5029
03:10:58,163 --> 03:10:59,040
- Yeah.

5030
03:10:59,040 --> 03:10:59,873
I guess you're right.

5031
03:10:59,873 --> 03:11:01,620
- But then cloning might
be exceptionally difficult.

5032
03:11:01,620 --> 03:11:02,940
There might be a deep integration

5033
03:11:02,940 --> 03:11:04,560
between the software and the hardware

5034
03:11:04,560 --> 03:11:06,330
in ways we don't quite yet understand.

5035
03:11:06,330 --> 03:11:07,470
- Well, from the altruism point of view,

5036
03:11:07,470 --> 03:11:10,770
what makes me special is
more the gang of genes

5037
03:11:10,770 --> 03:11:13,050
that are riding in my
chromosomes I suppose.

5038
03:11:13,050 --> 03:11:13,883
Right?

5039
03:11:13,883 --> 03:11:16,080
They're the replicating unit I suppose-

5040
03:11:16,080 --> 03:11:17,400
- No, but that's just the compute,

5041
03:11:17,400 --> 03:11:20,070
the thing that makes you special, sure.

5042
03:11:20,070 --> 03:11:25,070
Well, the reality is
what makes you special

5043
03:11:25,110 --> 03:11:29,790
is your ability to survive
based on the software

5044
03:11:29,790 --> 03:11:33,123
that runs on the hardware
that was built by the genes.

5045
03:11:34,020 --> 03:11:35,940
So the software is the thing
that makes you survive.

5046
03:11:35,940 --> 03:11:37,650
Not the hardware or-

5047
03:11:37,650 --> 03:11:38,483
- It's a little bit of both.

5048
03:11:38,483 --> 03:11:40,380
It's just like a second layer.

5049
03:11:40,380 --> 03:11:41,460
It's a new second layer

5050
03:11:41,460 --> 03:11:42,870
that hasn't been there before the brain.

5051
03:11:42,870 --> 03:11:44,160
They both coexist.

5052
03:11:44,160 --> 03:11:46,050
- But there's also layers of the software.

5053
03:11:46,050 --> 03:11:51,050
I mean it's an abstraction
on top of abstractions.

5054
03:11:52,200 --> 03:11:53,033
Okay, "Selfish Gene".

5055
03:11:53,033 --> 03:11:55,590
- So "Selfish Gene", Nick Lane.

5056
03:11:55,590 --> 03:11:58,530
I would say sometimes
books are not sufficient.

5057
03:11:58,530 --> 03:12:00,530
I like to reach for textbooks sometimes.

5058
03:12:01,560 --> 03:12:03,570
I feel like books are for too much

5059
03:12:03,570 --> 03:12:05,130
of a general consumption sometime

5060
03:12:05,130 --> 03:12:07,920
and they're too high up in the level

5061
03:12:07,920 --> 03:12:09,540
of abstraction and it's not good enough.

5062
03:12:09,540 --> 03:12:10,540
- [Lex] Yeah.

5063
03:12:10,540 --> 03:12:12,150
- So I like textbooks, I like "The Cell".

5064
03:12:12,150 --> 03:12:13,900
I think "The Cell" was pretty cool.

5065
03:12:14,760 --> 03:12:17,910
That's why also I like
the writing of Nick Lane

5066
03:12:17,910 --> 03:12:21,240
is because he's pretty
willing to step one level down

5067
03:12:21,240 --> 03:12:24,850
and he doesn't, yeah,
he's willing to go there

5068
03:12:25,950 --> 03:12:27,870
but he's also willing to
be throughout the stack.

5069
03:12:27,870 --> 03:12:29,220
So he'll go down to a lot of detail

5070
03:12:29,220 --> 03:12:32,670
but then he will come back
up and I think he has a,

5071
03:12:32,670 --> 03:12:34,740
yeah, basically, I really appreciate that.

5072
03:12:34,740 --> 03:12:36,720
- That's why I love
college, early college,

5073
03:12:36,720 --> 03:12:39,717
even high school, just
textbooks on the basics

5074
03:12:39,717 --> 03:12:41,910
of computer science, of mathematics,

5075
03:12:41,910 --> 03:12:44,190
of biology, of chemistry.

5076
03:12:44,190 --> 03:12:45,023
- [Andrej] Yes.

5077
03:12:45,023 --> 03:12:49,770
- Those are, they condense
down, it's sufficient in general

5078
03:12:49,770 --> 03:12:51,207
that you can understand
both the philosophy

5079
03:12:51,207 --> 03:12:54,837
and the details but also
you get homework problems

5080
03:12:54,837 --> 03:12:57,000
and you get to play with
it as much as you would

5081
03:12:57,000 --> 03:12:59,761
if you were in programming stuff.

5082
03:12:59,761 --> 03:13:00,594
- Yeah.

5083
03:13:00,594 --> 03:13:01,980
And then I'm also suspicious
of textbooks honestly

5084
03:13:01,980 --> 03:13:04,290
because as an example in deep-learning

5085
03:13:04,290 --> 03:13:05,517
there's no amazing textbooks

5086
03:13:05,517 --> 03:13:07,260
and the field is changing very quickly.

5087
03:13:07,260 --> 03:13:10,590
I imagine the same is true
in say synthetic biology

5088
03:13:10,590 --> 03:13:13,470
and so on, these books like
"The Cell" are kind of outdated.

5089
03:13:13,470 --> 03:13:14,550
They're still high-level,

5090
03:13:14,550 --> 03:13:16,440
like what is the actual
real source of truth?

5091
03:13:16,440 --> 03:13:18,990
It's people in wet labs
working with cells.

5092
03:13:18,990 --> 03:13:19,823
- Yeah.

5093
03:13:19,823 --> 03:13:24,720
- Sequencing genomes and,
yeah, actually working with it.

5094
03:13:24,720 --> 03:13:27,150
And I don't have that
much exposure to that

5095
03:13:27,150 --> 03:13:27,983
or what that looks like.

5096
03:13:27,983 --> 03:13:30,180
So I still don't fully, I'm
reading through the cell

5097
03:13:30,180 --> 03:13:31,440
and it's kind of
interesting, and I'm learning

5098
03:13:31,440 --> 03:13:33,300
but it's still not sufficient I would say

5099
03:13:33,300 --> 03:13:34,740
in terms of understanding.

5100
03:13:34,740 --> 03:13:36,560
- Well, it's a clean summarization

5101
03:13:36,560 --> 03:13:38,820
of the mainstream narrative.

5102
03:13:38,820 --> 03:13:39,653
- [Andrej] Yeah.

5103
03:13:39,653 --> 03:13:41,760
- But you have to learn
that before you break out.

5104
03:13:41,760 --> 03:13:42,690
- [Andrej] Yeah,

5105
03:13:42,690 --> 03:13:43,770
- Towards the cutting edge.

5106
03:13:43,770 --> 03:13:44,752
- Yeah.

5107
03:13:44,752 --> 03:13:45,960
But what is the actual process
of working with these cells

5108
03:13:45,960 --> 03:13:47,820
and growing them and incubating them

5109
03:13:47,820 --> 03:13:50,130
and it's like a massive cooking recipe.

5110
03:13:50,130 --> 03:13:52,290
So making sure your cell
slows and proliferate

5111
03:13:52,290 --> 03:13:54,060
and then you're sequencing
them, running experiments

5112
03:13:54,060 --> 03:13:57,193
and just how that works, I
think is the source of truth of

5113
03:13:57,193 --> 03:13:59,550
at the end of the day what's really useful

5114
03:13:59,550 --> 03:14:01,980
in terms of creating therapies and so on.

5115
03:14:01,980 --> 03:14:02,900
- Yeah.

5116
03:14:02,900 --> 03:14:04,986
I wonder what in the
future AI textbooks will be

5117
03:14:04,986 --> 03:14:06,930
'cause you know there's
"Artificial Intelligence:

5118
03:14:06,930 --> 03:14:07,920
A Modern Approach".

5119
03:14:07,920 --> 03:14:09,900
I actually haven't read, if it's come out,

5120
03:14:09,900 --> 03:14:13,410
the recent version, there's
been a recent edition.

5121
03:14:13,410 --> 03:14:15,900
I also saw there's a science
of deep learning book.

5122
03:14:15,900 --> 03:14:17,940
I'm waiting for textbooks
that are worth recommending,

5123
03:14:17,940 --> 03:14:18,780
worth reading.

5124
03:14:18,780 --> 03:14:19,745
- [Andrej] Yeah.

5125
03:14:19,745 --> 03:14:23,640
- It's tricky 'cause it's like
papers and code, code, code.

5126
03:14:23,640 --> 03:14:25,770
- Honestly, I find papers are quite good.

5127
03:14:25,770 --> 03:14:28,650
I especially like the
appendix of any paper as well.

5128
03:14:28,650 --> 03:14:31,023
It's like the most detail you can have.

5129
03:14:33,180 --> 03:14:35,010
- It doesn't have to be cohesive,

5130
03:14:35,010 --> 03:14:35,843
connected to anything else.

5131
03:14:35,843 --> 03:14:37,340
You just described me a very specific way

5132
03:14:37,340 --> 03:14:39,060
you saw the particular thing.

5133
03:14:39,060 --> 03:14:40,027
Yeah.

5134
03:14:40,027 --> 03:14:41,280
- Yeah, many times papers can
be actually quite readable,

5135
03:14:41,280 --> 03:14:43,740
not always, but sometimes the
introduction and the abstract

5136
03:14:43,740 --> 03:14:46,167
is readable even for someone
outside of the field.

5137
03:14:46,167 --> 03:14:48,660
This is not always true
and sometimes I think,

5138
03:14:48,660 --> 03:14:51,030
unfortunately, scientists
use complex terms

5139
03:14:51,030 --> 03:14:52,620
even when it's not necessary.

5140
03:14:52,620 --> 03:14:54,030
I think that's harmful.

5141
03:14:54,030 --> 03:14:55,860
I think there's no reason for that.

5142
03:14:55,860 --> 03:14:58,980
- And papers sometimes are
longer than they need to be

5143
03:14:58,980 --> 03:15:01,620
in the parts that don't matter.

5144
03:15:01,620 --> 03:15:02,453
- [Andrej] Yeah.

5145
03:15:02,453 --> 03:15:03,330
- The appendix should be long

5146
03:15:03,330 --> 03:15:05,790
but then the papers
itself, look at Einstein,

5147
03:15:05,790 --> 03:15:07,110
make it simple.

5148
03:15:07,110 --> 03:15:08,117
- Yeah.

5149
03:15:08,117 --> 03:15:08,950
But certainly, I've come
across papers I would say,

5150
03:15:08,950 --> 03:15:10,590
say like synthetic biology or something

5151
03:15:10,590 --> 03:15:12,510
that I thought were quite
readable for the abstract

5152
03:15:12,510 --> 03:15:14,727
and the introduction and then
you're reading the rest of it

5153
03:15:14,727 --> 03:15:15,930
and you don't fully understand

5154
03:15:15,930 --> 03:15:18,593
but you kind of are getting
a gist and I think it's cool.

5155
03:15:21,900 --> 03:15:24,480
- You give advice to folks
interested in machine learning

5156
03:15:24,480 --> 03:15:26,910
and research but in general life advice

5157
03:15:26,910 --> 03:15:30,690
to a young person, high
school, early college

5158
03:15:30,690 --> 03:15:32,883
about how to have a career
they can be proud of

5159
03:15:32,883 --> 03:15:34,563
or a life they can be proud of?

5160
03:15:35,670 --> 03:15:38,040
- Yeah, I think I'm very
hesitant to give general advice.

5161
03:15:38,040 --> 03:15:38,910
I think it's really hard.

5162
03:15:38,910 --> 03:15:40,470
I've mentioned, some of
the stuff I've mentioned

5163
03:15:40,470 --> 03:15:41,760
is fairly general, I think.

5164
03:15:41,760 --> 03:15:44,130
Like focus on just the amount
of work you're spending

5165
03:15:44,130 --> 03:15:45,750
on like a thing.

5166
03:15:45,750 --> 03:15:48,120
Compare yourself only to
yourself, not to others.

5167
03:15:48,120 --> 03:15:48,953
- [Lex] That's good.

5168
03:15:48,953 --> 03:15:49,920
- [Andrej] I think those
are fairly general.

5169
03:15:49,920 --> 03:15:51,320
- How do you pick the thing?

5170
03:15:52,200 --> 03:15:55,350
- You just have like a
deep interest in something

5171
03:15:55,350 --> 03:15:58,260
or try to find the argmax over the things

5172
03:15:58,260 --> 03:15:59,093
that you're interested in.

5173
03:15:59,093 --> 03:16:01,290
- Argmax at that moment and stick with it.

5174
03:16:01,290 --> 03:16:02,123
- [Lex] Yeah.

5175
03:16:02,123 --> 03:16:05,190
- How do you not get distracted
and switch to another thing?

5176
03:16:05,190 --> 03:16:06,483
- You can if you like.

5177
03:16:07,860 --> 03:16:10,920
- Well, if you do an argmax
repeatedly every week,

5178
03:16:10,920 --> 03:16:11,956
every month.

5179
03:16:11,956 --> 03:16:12,789
- [Andrej] Yeah, it doesn't converge.

5180
03:16:12,789 --> 03:16:13,622
- It's a problem.

5181
03:16:13,622 --> 03:16:14,455
- Yeah.

5182
03:16:14,455 --> 03:16:15,330
You can like low-pass filter yourself

5183
03:16:15,330 --> 03:16:18,093
in terms of what has
consistently been true for you.

5184
03:16:19,417 --> 03:16:22,230
But yeah, I definitely
see how it can be hard

5185
03:16:22,230 --> 03:16:24,060
but I would say you're
going to work the hardest

5186
03:16:24,060 --> 03:16:26,070
on the thing that you care about the most.

5187
03:16:26,070 --> 03:16:29,040
So low pass filter yourself
and really introspect

5188
03:16:29,040 --> 03:16:31,230
in your past, what are the
things that gave you energy

5189
03:16:31,230 --> 03:16:33,420
and what are the things that
took energy away from you?

5190
03:16:33,420 --> 03:16:34,620
Concrete examples.

5191
03:16:34,620 --> 03:16:36,930
And usually, from those concrete examples,

5192
03:16:36,930 --> 03:16:38,610
sometimes patterns can merge.

5193
03:16:38,610 --> 03:16:40,595
I like it when things look like this

5194
03:16:40,595 --> 03:16:41,430
when I'm in these positions.

5195
03:16:41,430 --> 03:16:42,780
- So that's not necessarily the field

5196
03:16:42,780 --> 03:16:44,850
but the kind of stuff you're
doing in a particular field.

5197
03:16:44,850 --> 03:16:47,520
So for you, it seems
like you were energized

5198
03:16:47,520 --> 03:16:50,670
by implementing stuff,
building actual things.

5199
03:16:50,670 --> 03:16:52,650
- Yeah, being low-level, learning

5200
03:16:52,650 --> 03:16:56,040
and then also communicating
so that others can go through

5201
03:16:56,040 --> 03:16:58,340
the same realizations
and shortening that gap.

5202
03:16:59,250 --> 03:17:00,870
Because I usually have
to do way too much work

5203
03:17:00,870 --> 03:17:02,520
to understand a thing
and then I'm like, okay,

5204
03:17:02,520 --> 03:17:04,260
this is actually like,
okay, I think I get it

5205
03:17:04,260 --> 03:17:05,943
and like why was it so much work?

5206
03:17:07,061 --> 03:17:08,940
It should have been much less work.

5207
03:17:08,940 --> 03:17:10,680
And that gives me a lot of frustration

5208
03:17:10,680 --> 03:17:12,570
and that's why I sometimes go teach.

5209
03:17:12,570 --> 03:17:15,420
- So aside from the
teaching you're doing now,

5210
03:17:15,420 --> 03:17:20,420
putting out videos, aside from
a potential "Godfather II"

5211
03:17:21,990 --> 03:17:24,690
would the AGI at Tesla and beyond,

5212
03:17:24,690 --> 03:17:26,940
what does the future for
Andrej Karpathy hold,

5213
03:17:26,940 --> 03:17:28,950
have you figured that out yet or no?

5214
03:17:28,950 --> 03:17:32,610
I mean as you see through the fog of war

5215
03:17:32,610 --> 03:17:37,530
that is all of our future, do
you start seeing silhouettes

5216
03:17:37,530 --> 03:17:39,780
of what that possible
future could look like?

5217
03:17:41,010 --> 03:17:42,780
- The consistent thing I've
been always interested in,

5218
03:17:42,780 --> 03:17:44,550
for me at least is Ai.

5219
03:17:44,550 --> 03:17:49,320
And that's probably what I'm
spending the rest of my life on

5220
03:17:49,320 --> 03:17:50,880
because I just care about it a lot.

5221
03:17:50,880 --> 03:17:53,430
And I actually care about
many other problems as well.

5222
03:17:53,430 --> 03:17:56,310
Like say aging, which I
basically view as disease

5223
03:17:56,310 --> 03:17:59,010
and I care about that as well.

5224
03:17:59,010 --> 03:18:00,480
But I don't think it's a good idea

5225
03:18:00,480 --> 03:18:02,310
to go after it, specifically.

5226
03:18:02,310 --> 03:18:05,040
I don't actually think
that humans will be able to

5227
03:18:05,040 --> 03:18:06,210
come up with the answer.

5228
03:18:06,210 --> 03:18:08,820
I think the correct thing to
do is to ignore those problems

5229
03:18:08,820 --> 03:18:11,730
and you solve AI and then use
that to solve everything else.

5230
03:18:11,730 --> 03:18:13,433
And I think there's a
chance that this will work.

5231
03:18:13,433 --> 03:18:14,790
I think it's a very high chance

5232
03:18:14,790 --> 03:18:18,450
and that's the way I'm betting at least.

5233
03:18:18,450 --> 03:18:20,070
- So when you think about AI,

5234
03:18:20,070 --> 03:18:23,430
are you interested in all
kinds of applications?

5235
03:18:23,430 --> 03:18:24,454
- [Andrej] Yes.

5236
03:18:24,454 --> 03:18:25,287
- All kinds of domains.

5237
03:18:25,287 --> 03:18:27,530
And any domain you focus on will allow you

5238
03:18:27,530 --> 03:18:30,030
to get insights to the big problem of AGI?

5239
03:18:30,030 --> 03:18:31,890
- Yeah, for me it's the
ultimate meta-problem.

5240
03:18:31,890 --> 03:18:33,540
I don't wanna work on
any one specific problem,

5241
03:18:33,540 --> 03:18:34,440
there's too many problems.

5242
03:18:34,440 --> 03:18:36,600
So how can you work on all
problems simultaneously?

5243
03:18:36,600 --> 03:18:39,450
You solve the meta-problem,
which to me is just intelligence

5244
03:18:39,450 --> 03:18:42,360
and how do you automate it?

5245
03:18:42,360 --> 03:18:45,630
- Is there cool small
projects like arxiv-sanity

5246
03:18:45,630 --> 03:18:48,530
and so on that you're thinking about

5247
03:18:48,530 --> 03:18:53,160
that ML world can anticipate.

5248
03:18:53,160 --> 03:18:54,960
- There's always some fun side projects.

5249
03:18:54,960 --> 03:18:55,793
- [Lex] Yeah.

5250
03:18:55,793 --> 03:18:57,690
- arxiv-sanity is one, yeah, basically,

5251
03:18:57,690 --> 03:18:58,890
there's way too many archive papers,

5252
03:18:58,890 --> 03:19:02,460
how can I organize it and
recommend papers and so on.

5253
03:19:02,460 --> 03:19:04,950
I transcribed all of your podcasts.

5254
03:19:04,950 --> 03:19:07,410
- What did you learn from that experience,

5255
03:19:07,410 --> 03:19:10,080
from transcribing the process of like

5256
03:19:10,080 --> 03:19:13,080
you consuming audiobooks
and podcasts and so on?

5257
03:19:13,080 --> 03:19:14,035
- [Andrej] Yeah.

5258
03:19:14,035 --> 03:19:16,440
- And here's a process that achieves

5259
03:19:16,440 --> 03:19:19,290
closer to human-level
performance on annotation.

5260
03:19:19,290 --> 03:19:21,240
- Yeah, well, I definitely was surprised

5261
03:19:21,240 --> 03:19:24,030
that transcription with OpenAI Whisper

5262
03:19:24,030 --> 03:19:26,430
was working so well compared
to what I'm familiar with,

5263
03:19:26,430 --> 03:19:30,480
from Siri and a few other systems
I guess, it works so well.

5264
03:19:30,480 --> 03:19:34,240
And that's what gave me
some energy to try it out.

5265
03:19:34,240 --> 03:19:36,720
And I thought it could be
fun to run on podcasts.

5266
03:19:36,720 --> 03:19:40,590
It's not obvious to me why
Whisper is so much better

5267
03:19:40,590 --> 03:19:41,970
compared to anything
else because I feel like

5268
03:19:41,970 --> 03:19:43,020
there should be a lot of incentive

5269
03:19:43,020 --> 03:19:45,090
for a lot of companies to
produce transcription systems

5270
03:19:45,090 --> 03:19:46,740
and that they've done so over a long time.

5271
03:19:46,740 --> 03:19:50,280
Whisper is not a super exotic
model, it's a transformer,

5272
03:19:50,280 --> 03:19:51,720
it takes mel spectrograms

5273
03:19:51,720 --> 03:19:54,240
and it just outputs tokens of text.

5274
03:19:54,240 --> 03:19:55,800
It's not crazy.

5275
03:19:55,800 --> 03:19:58,500
The model and everything has
been around for a long time.

5276
03:19:58,500 --> 03:19:59,473
I'm not actually a hundred
percent sure why this came about.

5277
03:19:59,473 --> 03:20:01,830
- Yeah, it's not obvious to me either.

5278
03:20:01,830 --> 03:20:04,470
It makes me feel like I'm
missing something for the middle.

5279
03:20:04,470 --> 03:20:05,303
- [Andrej] I'm missing something.

5280
03:20:05,303 --> 03:20:08,580
- Yeah, because there is a
huge, even Google and so on,

5281
03:20:08,580 --> 03:20:10,598
YouTube transcription.

5282
03:20:10,598 --> 03:20:11,431
- [Andrej] Yeah.

5283
03:20:11,431 --> 03:20:12,264
- Yeah.

5284
03:20:12,264 --> 03:20:13,097
It's unclear.

5285
03:20:13,097 --> 03:20:15,870
But some of it is also
integrating into a bigger system.

5286
03:20:15,870 --> 03:20:17,250
- [Andrej] Yeah.

5287
03:20:17,250 --> 03:20:19,020
- So the user interface, how it's deployed

5288
03:20:19,020 --> 03:20:19,853
and all that kind of stuff.

5289
03:20:19,853 --> 03:20:24,090
Maybe running it as an
independent thing is much easier,

5290
03:20:24,090 --> 03:20:25,380
like an order of magnitude easier

5291
03:20:25,380 --> 03:20:27,750
than deploying it to a
large integrated system

5292
03:20:27,750 --> 03:20:31,170
like YouTube transcription, or anything,

5293
03:20:31,170 --> 03:20:34,890
like meetings, like
Zoom has transcription,

5294
03:20:34,890 --> 03:20:36,510
that's kind of crappy.

5295
03:20:36,510 --> 03:20:38,550
But creating interface where it detects

5296
03:20:38,550 --> 03:20:40,560
the different individual speakers,

5297
03:20:40,560 --> 03:20:45,560
it's able to display
it in compelling ways,

5298
03:20:45,720 --> 03:20:47,580
run it real-time, all that kind of stuff.

5299
03:20:47,580 --> 03:20:48,783
Maybe that's difficult.

5300
03:20:49,980 --> 03:20:51,210
That's the only explanation I have

5301
03:20:51,210 --> 03:20:55,410
because I'm currently paying quite a bit

5302
03:20:55,410 --> 03:20:58,260
for human transcription, human caption.

5303
03:20:58,260 --> 03:20:59,093
- [Andrej] Right.

5304
03:20:59,093 --> 03:20:59,926
- Annotation.

5305
03:20:59,926 --> 03:21:03,060
And like it seems like
there's a huge incentive

5306
03:21:03,060 --> 03:21:04,095
to automate that.

5307
03:21:04,095 --> 03:21:04,928
- [Andrej] Yeah.

5308
03:21:04,928 --> 03:21:05,761
- It's very confusing.

5309
03:21:05,761 --> 03:21:06,594
- And I think, I mean,
I dunno if you looked at

5310
03:21:06,594 --> 03:21:07,650
some of the Whisper transcripts,

5311
03:21:07,650 --> 03:21:09,090
but they're quite good.

5312
03:21:09,090 --> 03:21:10,170
- They're good.

5313
03:21:10,170 --> 03:21:12,120
And especially in tricky cases.

5314
03:21:12,120 --> 03:21:12,953
- [Andrej] Yeah.

5315
03:21:12,953 --> 03:21:16,947
- I've seen Whisper's
performance on super tricky cases

5316
03:21:16,947 --> 03:21:18,548
and it does incredibly well.

5317
03:21:18,548 --> 03:21:20,940
So I don't know, a
podcast is pretty simple.

5318
03:21:20,940 --> 03:21:23,460
It's like high-quality audio

5319
03:21:23,460 --> 03:21:26,540
and you're speaking
usually pretty clearly.

5320
03:21:26,540 --> 03:21:27,373
- [Andrej] Yeah.

5321
03:21:27,373 --> 03:21:28,773
- And so I don't know,

5322
03:21:28,773 --> 03:21:31,890
I don't know what
OpenAIs plans are either.

5323
03:21:31,890 --> 03:21:34,620
- But yeah, there's always
fun projects basically.

5324
03:21:34,620 --> 03:21:37,200
And stable diffusion also
is opening up a huge amount

5325
03:21:37,200 --> 03:21:39,210
of experimentation I would
say in the visual realm

5326
03:21:39,210 --> 03:21:42,630
and generating images, and
videos, and movies ultimately.

5327
03:21:42,630 --> 03:21:43,527
- [Lex] Yeah, videos now.

5328
03:21:43,527 --> 03:21:45,518
- And so that's going to be pretty crazy.

5329
03:21:45,518 --> 03:21:48,330
That's going to almost certainly work

5330
03:21:48,330 --> 03:21:49,680
and it's going to be really interesting

5331
03:21:49,680 --> 03:21:52,980
when the cost of content creation
is going to fall to zero.

5332
03:21:52,980 --> 03:21:55,560
You used to need a painter for
a few months to paint a thing

5333
03:21:55,560 --> 03:21:57,570
and now it's going to
be speak to your phone

5334
03:21:57,570 --> 03:21:59,220
to get your video.

5335
03:21:59,220 --> 03:22:02,120
- So if Hollywood will start
using that to generate scenes

5336
03:22:04,320 --> 03:22:05,700
which completely opens up.

5337
03:22:05,700 --> 03:22:06,533
Yeah.

5338
03:22:06,533 --> 03:22:10,530
So you can make a movie
like "Avatar" eventually

5339
03:22:10,530 --> 03:22:12,390
for under a million dollars.

5340
03:22:12,390 --> 03:22:14,333
- Much less maybe just
by talking to your phone.

5341
03:22:14,333 --> 03:22:16,283
I mean, I know it sounds kind of crazy.

5342
03:22:17,700 --> 03:22:20,730
- And then there'd be
some voting mechanism,

5343
03:22:20,730 --> 03:22:22,530
would there be a show on Netflix

5344
03:22:22,530 --> 03:22:24,763
that's generated completely automatedly?

5345
03:22:25,597 --> 03:22:26,430
Semi-automatedly?

5346
03:22:26,430 --> 03:22:27,263
- Yeah, potentially.

5347
03:22:27,263 --> 03:22:28,096
Yeah.

5348
03:22:28,096 --> 03:22:28,929
And what does it look like also

5349
03:22:28,929 --> 03:22:30,696
when you can just generate it on demand

5350
03:22:30,696 --> 03:22:33,990
and there's infinity of it?

5351
03:22:33,990 --> 03:22:34,823
- Yeah.

5352
03:22:36,270 --> 03:22:37,173
Oh, man.

5353
03:22:38,190 --> 03:22:39,240
All the synthetic content.

5354
03:22:39,240 --> 03:22:42,510
I mean it's humbling because
we treat ourselves as special

5355
03:22:42,510 --> 03:22:43,920
for being able to generate art,

5356
03:22:43,920 --> 03:22:46,350
and ideas, and all that kind of stuff.

5357
03:22:46,350 --> 03:22:49,950
If that can be done in
an automated way by Ai.

5358
03:22:49,950 --> 03:22:50,970
- Yeah.

5359
03:22:50,970 --> 03:22:52,527
I think it's fascinating to me how these,

5360
03:22:52,527 --> 03:22:54,904
the predictions of AI and
what it's going to look like

5361
03:22:54,904 --> 03:22:55,920
and what it's going to be capable of

5362
03:22:55,920 --> 03:22:57,630
are completely inverted and wrong.

5363
03:22:57,630 --> 03:22:59,490
And sci-fi of fifties and sixties,

5364
03:22:59,490 --> 03:23:01,350
were just totally not right.

5365
03:23:01,350 --> 03:23:03,870
They imagine AI is like super calculating,

5366
03:23:03,870 --> 03:23:05,580
theorem provers, and we're getting things

5367
03:23:05,580 --> 03:23:07,110
that can talk to you about emotions.

5368
03:23:07,110 --> 03:23:10,320
They can do art, it's just weird.

5369
03:23:10,320 --> 03:23:11,850
- Are you excited about that future?

5370
03:23:11,850 --> 03:23:16,800
just AI's, like hybrid
systems, heterogeneous systems

5371
03:23:16,800 --> 03:23:19,470
of humans and AIs talking about emotions,

5372
03:23:19,470 --> 03:23:22,239
Netflix and chill with an AI system.

5373
03:23:22,239 --> 03:23:24,989
Or the Netflix thing you
watch is also generated by AI?

5374
03:23:26,070 --> 03:23:28,420
- I think it's going to
be interesting for sure

5375
03:23:29,340 --> 03:23:31,380
and I think I'm cautiously optimistic

5376
03:23:31,380 --> 03:23:33,690
but it's not obvious.

5377
03:23:33,690 --> 03:23:37,350
- Well, the sad thing is
your brain and mine developed

5378
03:23:37,350 --> 03:23:42,350
in a time before Twitter,
before the internet.

5379
03:23:42,810 --> 03:23:45,270
So I wonder people that
are born inside of it

5380
03:23:45,270 --> 03:23:47,730
might have a different experience.

5381
03:23:47,730 --> 03:23:51,040
Like I, and maybe you will still resist it

5382
03:23:52,184 --> 03:23:54,750
and the people born now will not.

5383
03:23:54,750 --> 03:23:56,820
- Well, I do feel like humans
are extremely malleable.

5384
03:23:56,820 --> 03:23:57,653
- [Lex] Yeah.

5385
03:23:58,586 --> 03:24:00,063
- And you're probably right.

5386
03:24:00,990 --> 03:24:02,890
- What is the meaning of life, Andrej?

5387
03:24:04,890 --> 03:24:08,070
We talked about the universe

5388
03:24:08,070 --> 03:24:10,740
having a conversation with us humans

5389
03:24:10,740 --> 03:24:14,105
or with the systems we
create to try to answer.

5390
03:24:14,105 --> 03:24:17,580
For the creator of the
universe to notice us,

5391
03:24:17,580 --> 03:24:19,500
we're trying to create systems

5392
03:24:19,500 --> 03:24:23,760
that are loud enough to answer back.

5393
03:24:23,760 --> 03:24:24,990
- I dunno if that's the meaning of life.

5394
03:24:24,990 --> 03:24:26,970
That's like meaning of
life for some people.

5395
03:24:26,970 --> 03:24:28,260
The first level answer I would say

5396
03:24:28,260 --> 03:24:30,300
is anyone can choose
their own meaning of life

5397
03:24:30,300 --> 03:24:31,920
because we are a conscious entity

5398
03:24:31,920 --> 03:24:34,170
and it's beautiful, number one.

5399
03:24:34,170 --> 03:24:37,260
But I do think that a
deeper meaning of life

5400
03:24:37,260 --> 03:24:40,200
if someone is interested
is along the lines of like,

5401
03:24:40,200 --> 03:24:42,210
what the hell is all this?

5402
03:24:42,210 --> 03:24:43,557
And like why?

5403
03:24:43,557 --> 03:24:46,170
And if you look into fundamental physics

5404
03:24:46,170 --> 03:24:48,180
and the quantum field theory
and the standard model,

5405
03:24:48,180 --> 03:24:50,220
they're very complicated.

5406
03:24:50,220 --> 03:24:55,220
And there's this 19 free
parameters of our universe

5407
03:24:55,470 --> 03:24:58,650
and what's going on with all
this stuff and why is it here?

5408
03:24:58,650 --> 03:24:59,670
And can I hack it?

5409
03:24:59,670 --> 03:25:00,503
Can I work with it?

5410
03:25:00,503 --> 03:25:01,410
Is there a message for me?

5411
03:25:01,410 --> 03:25:03,390
Am I supposed to create a message?

5412
03:25:03,390 --> 03:25:05,760
And so I think there's some
fundamental answers there

5413
03:25:05,760 --> 03:25:07,650
but I think there's actually even like,

5414
03:25:07,650 --> 03:25:10,020
you can't actually
really make dent in those

5415
03:25:10,020 --> 03:25:11,037
without more time.

5416
03:25:11,037 --> 03:25:13,830
And so to me also there's
a big question around

5417
03:25:13,830 --> 03:25:15,990
just getting more time, honestly.

5418
03:25:15,990 --> 03:25:16,823
Yeah.

5419
03:25:16,823 --> 03:25:18,120
That's what I think about
quite a bit as well.

5420
03:25:18,120 --> 03:25:20,373
- So kind of the ultimate,

5421
03:25:21,300 --> 03:25:25,050
or at least first way to
sneak up to the why question

5422
03:25:25,050 --> 03:25:30,050
is to try to escape the
system, the universe?

5423
03:25:30,097 --> 03:25:30,930
- [Andrej] Yeah.

5424
03:25:30,930 --> 03:25:34,050
- And then for that you backtrack

5425
03:25:34,050 --> 03:25:36,750
and say, okay, for that, that's
gonna take a very long time.

5426
03:25:36,750 --> 03:25:38,820
So the why question boils down

5427
03:25:38,820 --> 03:25:41,310
from an engineering perspective
to how do we extend?

5428
03:25:41,310 --> 03:25:42,278
- Yeah.

5429
03:25:42,278 --> 03:25:43,111
I think that's the question number one,

5430
03:25:43,111 --> 03:25:45,840
practically speaking, because
you're not gonna calculate

5431
03:25:45,840 --> 03:25:48,540
the answer to the deeper
questions in the time you have.

5432
03:25:48,540 --> 03:25:50,880
- And that could be
extending your own lifetime

5433
03:25:50,880 --> 03:25:53,670
or extending just the lifetime
of human civilization.

5434
03:25:53,670 --> 03:25:56,970
- Of whoever wants to, many
people might not want that.

5435
03:25:56,970 --> 03:25:58,060
- [Lex] Yeah.

5436
03:25:58,060 --> 03:25:59,148
- But I think people who do want that,

5437
03:25:59,148 --> 03:26:02,400
I think it's probably possible,

5438
03:26:02,400 --> 03:26:05,460
and I don't know that
people fully realize this.

5439
03:26:05,460 --> 03:26:08,880
I feel like people think of
death as an inevitability

5440
03:26:08,880 --> 03:26:11,160
but at the end of the day,
this is a physical system.

5441
03:26:11,160 --> 03:26:13,050
Some things go wrong.

5442
03:26:13,050 --> 03:26:15,240
It makes sense why
things like this happen,

5443
03:26:15,240 --> 03:26:16,830
evolutionarily speaking,

5444
03:26:16,830 --> 03:26:21,450
and there's most certainly
interventions that mitigate it.

5445
03:26:21,450 --> 03:26:24,027
- That would be interesting if
death is eventually looked at

5446
03:26:24,027 --> 03:26:28,523
as a fascinating thing that
used to happen to humans.

5447
03:26:28,523 --> 03:26:30,000
- I don't think it's unlikely.

5448
03:26:30,000 --> 03:26:32,043
I think it's likely.

5449
03:26:33,360 --> 03:26:37,080
- And it's up to our
imagination to try to predict

5450
03:26:37,080 --> 03:26:39,750
what the world without death looks like.

5451
03:26:39,750 --> 03:26:40,653
- [Andrej] Yeah.

5452
03:26:40,653 --> 03:26:43,553
- It's hard to, I think the
values will completely change.

5453
03:26:44,490 --> 03:26:47,700
- Could be, I don't really
buy all these ideas that,

5454
03:26:47,700 --> 03:26:52,380
oh, without death, there's no
meaning, there's nothingness.

5455
03:26:52,380 --> 03:26:54,960
I don't intuitively buy
all those arguments.

5456
03:26:54,960 --> 03:26:56,220
I think there's plenty of meaning,

5457
03:26:56,220 --> 03:26:57,390
plenty of things to learn.

5458
03:26:57,390 --> 03:26:58,350
They're interesting, exciting.

5459
03:26:58,350 --> 03:27:00,453
I want to know, I want to calculate,

5460
03:27:00,453 --> 03:27:04,050
I want to improve the
condition of all the humans

5461
03:27:04,050 --> 03:27:05,700
and organisms that are alive.

5462
03:27:05,700 --> 03:27:06,716
- Yeah.

5463
03:27:06,716 --> 03:27:08,490
The way we find meaning might change.

5464
03:27:08,490 --> 03:27:11,010
There is a lot of humans,
probably including myself,

5465
03:27:11,010 --> 03:27:14,610
that finds meaning in
the finiteness of things,

5466
03:27:14,610 --> 03:27:16,560
but that doesn't mean that's
the only source of meaning.

5467
03:27:16,560 --> 03:27:17,400
- Yeah.

5468
03:27:17,400 --> 03:27:19,860
I do think many people will go with that,

5469
03:27:19,860 --> 03:27:21,120
which I think is great.

5470
03:27:21,120 --> 03:27:22,170
I love the idea that people

5471
03:27:22,170 --> 03:27:24,600
can just choose their own adventure.

5472
03:27:24,600 --> 03:27:27,300
You are born as a conscious,
free entity by default.

5473
03:27:27,300 --> 03:27:28,200
I'd like to think.

5474
03:27:28,200 --> 03:27:29,033
- [Lex] Yeah.

5475
03:27:29,033 --> 03:27:33,450
- And you have your
unalienable rights for life.

5476
03:27:33,450 --> 03:27:35,280
- In the pursuit of happiness?

5477
03:27:35,280 --> 03:27:37,920
I don't know if you that, and the nature,

5478
03:27:37,920 --> 03:27:39,417
the landscape of happiness.

5479
03:27:39,417 --> 03:27:41,550
- And you can choose your
own adventure, mostly.

5480
03:27:41,550 --> 03:27:43,617
And that's not fully true but.

5481
03:27:43,617 --> 03:27:45,840
- I'm still am pretty sure I'm an NPC,

5482
03:27:45,840 --> 03:27:49,803
but an NPC can't know it's an NPC.

5483
03:27:51,450 --> 03:27:52,680
There could be different degrees

5484
03:27:52,680 --> 03:27:54,270
and levels of consciousness.

5485
03:27:54,270 --> 03:27:58,680
I don't think there's a more
beautiful way to end it.

5486
03:27:58,680 --> 03:28:00,270
Andrej, you're an incredible person.

5487
03:28:00,270 --> 03:28:02,070
I'm really honored you would talk with me,

5488
03:28:02,070 --> 03:28:04,140
everything you've done for
the machine learning world,

5489
03:28:04,140 --> 03:28:07,410
for the AI world to just inspire people,

5490
03:28:07,410 --> 03:28:09,187
to educate millions of people.

5491
03:28:09,187 --> 03:28:11,910
It's been great and I can't
wait to see what you do next.

5492
03:28:11,910 --> 03:28:12,900
It's been an honor, man.

5493
03:28:12,900 --> 03:28:14,220
Thank you so much for talking today.

5494
03:28:14,220 --> 03:28:15,053
- Awesome.

5495
03:28:15,053 --> 03:28:15,886
Thank you.

5496
03:28:15,886 --> 03:28:17,880
Thanks for listening to this conversation

5497
03:28:17,880 --> 03:28:20,640
with Andrej Karpathy,
to support this podcast

5498
03:28:20,640 --> 03:28:23,640
please check out our
sponsors in the description.

5499
03:28:23,640 --> 03:28:28,567
And now, let me leave you
some words from Samuel Karlin,

5500
03:28:28,567 --> 03:28:32,130
"The purpose of models
is not to fit the data

5501
03:28:32,130 --> 03:28:34,677
but to sharpen the questions."

5502
03:28:35,550 --> 03:28:38,403
Thanks for listening and
hope to see you next time.

